---
layout: default_sparse
title: Keynotes
permalink: /programme/keynotes/
index: 5
---

<div class="row justify-content-around pl-4 pr-4">
    <div class="col-12">
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2025/Marc_Pollefeys.jpg" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://people.inf.ethz.ch/marc.pollefeys/">Marc Pollefeys</a></h4>
                    <span class=""><small>ETH Zurich</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Spatial AI</b></h5>
                    <p class="text-center mb-1"><small></small></p> 
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>In this talk we’ll discuss how to build rich 3D representations of the environment to assist people and robots to perform tasks.  We’ll first discuss how to build visual 3D maps of environments and use those for visual (re)localization, spatial data access and navigation.  We’ll cover recent methods based on geometry, learning and combining both.  One of the questions we will consider is what is best learned and where we should use explicit geometric concepts.  We’ll also discuss how to build rich 3D semantic representations that enable queries and interactions with the scene. Our approach allows open vocabulary queries by leveraging foundation models.  While these models are very powerful in recognizing arbitrary objects, there are some aspects that are still missing to enable robotic interactions.  We’ll also briefly cover some of our work on action recognition which is key in building AI assistants and could also be useful to enable robots to learn from examples.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Marc Pollefeys is a Professor of Computer Science at ETH Zurich and the Director of the Microsoft Spatial AI Lab in Zurich. He is a Fellow of IEEE, ACM, AAIA and ELLIS, as well as a member of the Academia Europaea.  His work received several prizes and awards, including the Marr Prize and several best paper awards.  He obtained his PhD from the KU Leuven in 1999 and was a professor at UNC Chapel Hill before joining ETH Zurich. He is best known for his work in 3D computer vision, having been the first to develop a software pipeline to automatically turn photographs into 3D models, but also works on robotics, graphics and machine learning problems. Other noteworthy projects he worked on are real-time 3D scanning with mobile devices (2013), a real-time pipeline for 3D reconstruction of cities from vehicle mounted-cameras (2007), camera-based self-driving cars and the first fully autonomous vision-based drone (2012). More recently his academic research has focused on combining 3D reconstruction with semantic scene understanding. He served as the program chair for CVPR 2009 and general chair for ECCV 2014 and ICCV 2019 and was the founding president of the European Computer Vision Foundation.</p>
                </div>
            </div>
        </div>
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2025/Philip_Torr.jpg" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://eng.ox.ac.uk/people/philip-torr/">Philip Torr</a></h4>
                    <span class=""><small>University of Oxford</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>To be confirmed.</b></h5>
                    <p class="text-center mb-1"><small></small></p> 
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>To be confirmed.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Professor Philip Torr did his PhD (DPhil) at the Robotics Research Group of the University of Oxford under Professor David Murray of the Active Vision Group. He worked for another three years at Oxford as a research fellow, and still maintains close contact as visiting fellow there. He left Oxford to work for six years as a research scientist for Microsoft Research, first in Redmond, USA, in the Vision Technology Group, then in Cambridge founding the vision side of the Machine Learning and Perception Group. He then became a Professor in Computer Vision and Machine Learning at Oxford Brookes University. In 2013, Philip returned to Oxford as full professor where he has established the Torr Vision group. He won several awards including the Marr prize (the highest honour in vision) in 1998. He is a Royal Society Wolfson Research Merit Award Holder. Recently, together with members of his group, he has won several other awards including an honorary mention at the NIPS 2007 conference for the paper 'P. Kumar, V. Kolmorgorov, and P.H.S. Torr, An Analysis of Convex Relaxations for MAP Estimation', in NIPS 21, Neural Information Processing Conference, and (oral) Best Paper at Conference for 'O. Woodford, P.H.S. Torr, I. Reid, and A.W. Fitzgibbon, Global Stereo Reconstruction under Second Order Smoothness Priors', in Proceedings IEEE Conference of Computer Vision and Pattern Recognition, 2008 . More recently he has been awarded best science paper at BMVC 2010 and ECCV 2010. He was involved in the algorithm design for Boujou released by 2D3. Boujou has won a clutch of industry awards, including Computer Graphics World Innovation Award, IABM Peter Wayne Award, and CATS Award for Innovation, and a technical EMMY. He then worked closely with this Oxford based company as well as other companies such as Sony on the Wonderbook project. He has been involved in numerous spin-outs as founder or advisor including: FiveAI, Onfido, Oxsight, Eigent, DreamTech, Visionary Machines, CamelAI, as well as working closely with big tech companies like Google, Meta, Apple, Microsoft, and Sony. He was elected Fellow of the Royal Academy of Engineering (FREng) in 2019, and Fellow of the Royal Society (FRS) in 2021 for contributions to computer vision. In 2021 he was made Turing AI world leading researcher fellow.</p>
                </div>
            </div>
        </div>
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2025/Angela_Dai.jpg" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://www.3dunderstanding.org/">Angela Dai</a></h4>
                    <span class=""><small>Technical University of Munich</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Can Transformers Speak Geometry?</b></h5>
                    <p class="text-center mb-1"><small></small></p> 
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>What if generating a 3D mesh were as natural as predicting the next word in a sentence? Autoregressive modeling has rapidly become a unifying learning paradigm across data modalities, across language to images, and now offers a compelling approach for 3D geometry. This talk explores how transformer-based autoregressive models enable mesh generation by representing meshes as sequences. Framing mesh generation as a next-token prediction problem enables new ways to handle the compact, irregular structure of human-designed 3D assets, directly compatible with downstream graphics and vision applications. We explore sequence formulation and data representation, and address practical challenges in scaling to high-resolution meshes and interactive synthesis. This will enable more accessible and democratized 3D content creation, paving the way for interactive design, rapid prototyping, and simulation-ready assets, and unlocking new possibilities for both creative and computational exploration of 3D geometry.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Angela Dai is an Associate Professor at the Technical University of Munich where she leads the 3D AI Lab. Angela's research focuses on understanding how real-world 3D scenes around us can be modeled and semantically understood. Previously, she received her PhD in computer science from Stanford in 2018, advised by Pat Hanrahan, and her BSE in computer science from Princeton in 2013. Her research has been recognized through an ECVA Young Researcher Award, ERC Starting Grant, Eurographics Young Researcher Award, German Pattern Recognition Award, Google Research Scholar Award, and an ACM SIGGRAPH Outstanding Doctoral Dissertation Honorable Mention.</p>
                </div>
            </div>
        </div>
    </div>
</div>

{% comment %} 

<div class="row justify-content-around pl-4 pr-4">
    <div class="col-12">
        <!-- 4th keynote speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Laura_Sevilla.webp" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://laurasevilla.me">Laura Sevilla</a></h4>
                    <span class=""><small>University of Edinburgh</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Frontiers of Video Understanding</b></h5>
                    <p class="text-center mb-1"><small></small></p> 
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>Video Understanding is a fundamental skill of intelligent systems. From autonomous robots to virtual assistants, understanding the world in motion is necessary to be able to move and interact with it. The last few years have seen amazing improvements in Video Understanding research. Still there is a remarkable gap between the almost uncanny performance of models in other modalities such as language and still images, and the performance of video. In this talk I will discuss what I believe are the current barriers for video, including efficiency, a tricky relationship with language and finding the right tasks. For each of these topics I will discuss both my recent work on them, as well as what I believe are interesting directions that I hope can be inspiring for the community.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Laura Sevilla is a Reader (Associate Professor) at the University of Edinburgh where she funded and leads the Video Understanding Lab. Her work over the years has advanced the state-of-the-art in many aspects of video understanding, from optical flow, object tracking, video object segmentation, low-shot action classification, action detection, affordance estimation, video captioning and more. She has been the recipient of a Google Faculty Award in 2020 and Google Scholar Award in 2022, as well as other funding from companies such as Meta. She has served as Program Chair for BMVC in 2021 and as Area Chair for ECCV, ICCV, CVPR and AAAI. She's also done work on outreach through the series of Computer Vision for Global Challenges. She's also been elected as an ELLIS Scholar.</p>
                </div>
            </div>
        </div>
        <!-- 1st keynote speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Mubarak_Shah.png" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a></h4>
                    <span class=""><small>University of Central Florida</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Privacy Preservation and Bias Mitigation in Human Action Recognition</b></h5>
                    <p class="text-center mb-1"><small></small></p><br>
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>Advances in action recognition have enabled a wide range of real-world applications, e.g. elderly person monitoring systems, autonomous vehicles, sports analysis. As these techniques are being used in the real world two important issues have emerged: privacy and bias. Most of these video understanding applications involve extensive computation, for which a user needs to share the video data to the cloud computation server, where the user also ends up sharing the private visual information like gender, skin color, clothing, background objects etc. Therefore, there is a pressing need for solutions to privacy preserving action recognition. Beyond privacy protection, bias in video understanding can lead to unfair and incorrect decision making. Action recognition models may predict specific actions based on gender stereotypes, such as associating a perceived female subject with hands near her face as applying makeup or brushing hair, even with nothing in hand, or they may suffer from background bias (i.e., inferring actions based on background cues) and foreground bias (i.e., relying on subject appearance). In this talk, I will present our recent work on Privacy Preservation and Bias Mitigation in human action recognition.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Dr. Mubarak Shah, the UCF Trustee Chair Professor, is the founding director of Center for Research in Computer Visions at University of Central Florida (UCF). Dr. Shah is a fellow of ACM, IEEE, AAAS, NAI, IAPR, AAIA and SPIE.  He has published extensively on topics related to human activity and action recognition, visual tracking, geo localization, visual crowd analysis, object detection and categorization, shape from shading, etc. He has served as ACM and IEEE Distinguished Visitor Program speaker. He is a recipient of 2022 PAMI Mark Everingham Prize for pioneering human action recognition datasets; 2019 ACM SIGMM Technical Achievement award; 2020 ACM SIGMM Test of Time Honorable Mention Award for his paper “Visual attention detection in video sequences using spatiotemporal cues”; 2020 International Conference on Pattern Recognition (ICPR) Best Scientific Paper Award; an honorable mention for the ICCV 2005 Where Am I? Challenge Problem; 2013 NGA Best Research Poster Presentation; 2nd place in Grand Challenge at the ACM Multimedia 2013 conference; and runner up for the best paper award in ACM Multimedia Conference in 2005 and 2010. At UCF he has received Pegasus Professor Award; University Distinguished Research Award; Faculty Excellence in Mentoring Doctoral Students; Faculty Excellence in Mentoring Postdoctoral Scholars, Scholarship of Teaching and Learning award; Teaching Incentive Program award; and Research Incentive Award.</p>
                </div>
            </div>
        </div><br>
        <!-- 2nd keynote speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Margarita_Chli.png" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://scholar.google.ch/citations?user=C0UhwEIAAAAJ&hl=en">Margarita Chli</a></h4>
                    <span class=""><small>University of Cyprus and ETH Zurich</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Vision-based robotic perception: are we there yet?</b></h5>
                    <p class="text-center mb-1"><small></small></p>
                    <p class="pb-1 mb-1 text-justify">As vision plays a key role in how we interpret a situation, developing vision-based perception for robots promises to be a big step towards robotic navigation and intelligence, with a tremendous impact on automating robot navigation. This talk will discuss our recent progress in this area at the Vision for Robotics Lab of the University of Cyprus and ETH Zurich (<a href="http://www.v4rl.com">http://www.v4rl.com</a>), and some of the biggest challenges we are faced with.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Margarita Chli is a Professor of Robotic Vision and the director of the Vision for Robotics Lab, at the University of Cyprus and ETH Zurich. Her work has contributed to the first vision-based autonomous flight of a small drone and the first demonstration of collaborative monocular SLAM for a small swarm of drones. Margarita has given invited keynotes at the World Economic Forum in Davos, TEDx, and ICRA, and she was featured in Robohub's 2016 list of "25 women in Robotics you need to know about". In 2023 she won the ERC Consolidator Grant, one of the most prestigious grants in Europe for blue-sky research, to grow her team at the University of Cyprus to research advanced robotic perception.</p>
                </div>
            </div>
        </div><br>
        <!-- 3rd keynote speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Federico_Tombari.png" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://federicotombari.github.io/">Federico Tombari</a></h4>
                    <span class=""><small>Google</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>The 3D Revolution: Neural Representations and Diffusion Models to Understand and Synthesise the 3D World</b></h5>
                    <p class="text-center mb-1"><small></small></p><br>
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>3D Computer Vision has recently witnessed a surge of interest from the ML and CV research community, due to the progress that recently introduced concepts such as neural representations, foundational models, and diffusion models enabled for many traditional 3D Computer Vision tasks. In this talk, we will focus in particular on the capability of understanding and synthesising 3D scenes and objects, which is a key component of applications in the space of Augmented/Mixed Reality and Robotics. We will look at three tasks in 3D Computer Vision that are fundamental components for these applications while being highly influenced by the aforementioned concepts: novel view synthesis, 3D semantic segmentation and 3D asset generation. For each of these three tasks, we will first understand some important practical limitations of current approaches. We will then walk through some solutions I recently explored with my team and designed to overcome such limitations, which include robust novel view synthesis, open set 3D scene segmentation and realistic 3D asset generation.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Federico Tombari is Senior Staff Research Scientist and Manager at Google where he leads an applied research team in computer vision and machine learning across North America and Europe. He is also a Lecturer (PrivatDozent) at the Technical University of Munich (TUM). He has 250+ peer-reviewed publications in CV/ML and applications to robotics, autonomous driving, healthcare and augmented reality. He got his PhD from the University of Bologna and his Venia Legendi (Habilitation) from Technical University of Munich (TUM). In 2018-19 he was co-founder and managing director of a startup on 3D perception for AR and robotics, then acquired by Google. He regularly serves as Area Chair and Associate Editor for international conferences and journals (IJRR, RA-L, IROS20/21/22, ICRA20/22, 3DV19/20/21/22/24, ECCV22/24, CVPR23/24, NeurIPS23 among others). He was the recipient of two Google Faculty Research Awards, one Amazon Research Award, 5 Outstanding Reviewer Awards (3x CVPR, ICCV21, NeuriIps21), among others. He has been a research partner of private and academic institutions including Google, Toyota, BMW, Audi, Amazon, Univ. Stanford, ETH and MIT.</p>
                </div>
            </div>
        </div>
    </div>
</div>

---

<div class="row justify-content-around pl-4 pr-4">
    <div class="col-12">
        <!-- DC speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/Sohag_Portrait.jpg" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://technovativesolutions.co.uk/">Salauddin Sohag</a></h4>
                    <span class=""><small>Technovative Solutions LTD (TVS)</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Industrial Keynote</b></h5>
                    <p class="text-center mb-1"><small></small></p> 
                    <p class="pb-1 mb-1 text-justify"><b>Abstract: </b>In this keynote, Sohag will chart the company’s journey, recounting major innovations, achievements, and the challenges it has navigated to reach its current position. Sohag will also connect Technovative Solutions' advancements to emerging industry trends, illustrating how the company is strategically positioned to address current and future market demands through cutting-edge solutions and agile methodologies.</p>
                    <p class="pb-2 text-justify"><b>Bio: </b>Salauddin Sohag is an experienced product leader with a passion for innovation and a proven track record of success. As the Head of Product at Technovative Solutions, he leads a talented team in bringing cutting-edge products to market, from conception and development to launch and beyond.
                    With over 10 years of experience in product development, Sohag is an expert in Agile methodologies, including SAFe and Design Thinking. He leverages his deep understanding of these principles, along with Lean methodologies, to drive impactful business outcomes and deliver exceptional customer experiences.
                    Sohag's career is marked by significant achievements, including leading e-commerce integration solutions at Vertex Inc. and transforming customer journeys with innovative mobile solutions at Lloyds Banking Group. He is also a strong advocate for continuous improvement and cross-functional collaboration, driving the company’s mission to innovate and excel in the realm of product management.</p>
                </div>
            </div>
        </div>
        <!-- DC speaker -->
        <div class="row pt-2 pb-2 align-items-center">
            <div class="col-12 col-md-4 col-lg-3"><a class="anchor"></a>
                <div class="text-center">
                    <img src="../../imgs_2024/mostafa.jpg" class="rounded-circle img-fluid" style="max-width: 125px;">
                    <h4 class="pt-2"><a href="https://technovativesolutions.co.uk/">Md. Mostafa Kamal Sarker</a></h4>
                    <span class=""><small>Technovative Solutions LTD (TVS)</small></span>
                </div>
            </div>
            <div class="col-12 col-md-8 col-lg-9">
                <div class="">
                    <h5 class="pt-1 text-center"><b>Doctoral Consortium Speaker</b></h5>
                    <p class="text-center mb-1"><small></small></p> 
                    <p class="pb-1 mb-1 text-justify"><b>Bio: </b>Dr Sarker is the Lead AI Research Scientist at Technovative Solutions LTD (TVS) and a Visiting Fellow at the University of Oxford. He's an expert in artificial intelligence, computer vision, and deep learning. His research has significantly impacted clinical AI, biomedical image analysis, and digital healthcare, evident in his 40+ peer-reviewed publications. At BMVC2024, he'll share his valuable insights and guide aspiring researchers on transitioning from academia to industry and discuss the exciting opportunities this path offers.</p>
                </div>
            </div>
        </div>
    </div>
</div>

{% endcomment %} 