number,forum,title,abstract,num Reviewers,num submitted Reviewers,missing Reviewers,min rating,max rating,average rating,min confidence,max confidence,average confidence,num Area Chairs assigned,num submitted Area Chairs,Meta Review,decision,Senior Area Chairs
12,https://openreview.net/forum?id=rf2VNhRNuG,Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians,"Part segmentation and motion estimation are two fundamental problems for articulated object modeling. In this paper, we present a method to solve these two problems jointly from a sequence of observed point clouds of a single articulated object. The main challenge in our problem setting is that the point clouds are not assumed to be generated by a fixed set of moving points. Instead, each point cloud in the sequence could be an arbitrary sampling of the object surface at that particular time step. Such scenarios occur when the object undergoes major occlusions, or if the dataset is collected using measurements from multiple sensors asynchronously. In these scenarios, methods that rely on tracking point correspondences are not appropriate. We present an alternative approach by representing the object as a collection of simple building blocks modeled as 3D Gaussians. With our representation, part segmentation is achieved by assigning the observed points to the Gaussians. Moreover, the transformation of each point across time can be obtained by following the poses of the assigned Gaussian. Experiments show that our method outperforms existing methods that solely rely on finding point correspondences. Additionally, we extend existing datasets to emulate real-world scenarios by considering viewpoint occlusions. We demonstrate that our method is more robust to missing points as compared to existing approaches on these challenging datasets, even when some parts are completely occluded in some time-steps.  Notably, our part segmentation outperforms the state-of-the-art method by 13% on occluded point clouds. Project page: https://giles200619.github.io/gsart_website/",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
18,https://openreview.net/forum?id=nMspfce4cf,Volumetric Temporal Texture for Smoke Stylization using Dynamic Radiance Fields,"The exemplar-based stylization of dynamic 3D volume sequences remains challenging in computer graphics. The difficulty lies not only in maintaining the styling features concerning a reference image across multiviews and time frames, but also in preserving visually plausible motion as the original smoke simulation, within a reasonable time and computational resources. In this work, we introduce Volumetric Neural Cellular Automata (VNCA), a novel method that synthesizes volumetric temporal textures for smoke stylizations in real-time. Our method formats radiance fields atop the self-emerging Neural Cellular Automata (NCA) and generates a dynamic sequence of stylized color and density volumes. The synthesized temporal textures align with the input sequence's overall motion patterns while matching the style of a reference image. We use flow-guided supervision to align the texture's temporal motion with that of the perceived smoke sequence, reducing the training time by over an order of magnitude. We demonstrate that VNCA can be easily adapted for mesh stylization, akin to solid texture modeling, extending its application beyond dynamic volume simulations.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
22,https://openreview.net/forum?id=0HOyXrj1Ze,Learning from Synthetic Data for Visual Grounding,"We introduce SynGround, a novel approach to learn representations for visual grounding using a combination of real data along with synthetic images, synthetic referring expressions, and corresponding bounding boxes. We explore various strategies to best generate additional image-text pairs and image-text-box triplets using pretrained models under different settings and varying degrees of reliance on real data. Through comparative analyses with synthetic, real, and web-crawled data, we identify factors that contribute to performance differences. We find that SynGround can improve the localization capabilities of a vision-and-language model and offers the potential for arbitrarily large-scale data generation. Particularly, data generated with SynGround improves the pointing game accuracy of a pretrained ALBEF model by 4.81% and improves BLIP by 17.11% absolute percentage points on average across RefCOCO+ and Flickr30k benchmarks.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
24,https://openreview.net/forum?id=AlLiSA4wXD,MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection,"Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form of a bird's eye view (BEV) from multi-view images.
In MVPD, end-to-end trainable deep learning methods have progressed greatly.
However, they often struggle to detect pedestrians with consistently small or large scales in views or with vastly different scales between views.
This is because they do not exploit multi-scale image features to generate the BEV feature and detect pedestrians.
To overcome this problem, we propose a novel MVPD method, called Multi-Scale Multi-View Detection (MSMVD).
MSMVD generates multi-scale BEV features by projecting multi-scale image features extracted from individual views into the BEV space, scale-by-scale.
Each of these BEV features inherits the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with consistently small or large scales in views.
Then, MSMVD combines information at different scales of multiple views by processing the multi-scale BEV features using a feature pyramid network.
This improves the detection of pedestrians with vastly different scales between views.
Extensive experiments demonstrate that exploiting multi-scale image features via multi-scale BEV features greatly improves the detection performance, and MSMVD outperforms the previous highest MODA by $4.5$ points on the GMVD dataset.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
26,https://openreview.net/forum?id=tYlxdMGBW8,Guiding a diffusion model with itself using sliding windows,"Guidance is a widely used technique for diffusion models to enhance sample quality. Technically, guidance is realised by using an auxiliary model that generalises more broadly than the primary model. Using a 2D toy example, we first show that it is highly beneficial when the auxiliary model exhibits similar but stronger generalisation errors than the primary model. Based on this insight, we introduce masked sliding window guidance (M-SWG), a novel, training-free method. M-SWG upweights long-range spatial dependencies by guiding the primary model with itself by selectively restricting its receptive field. M-SWG requires neither access to model weights from previous iterations, additional training, nor class conditioning. M-SWG achieves a superior Inception score (IS) compared to previous state-of-the-art training-free approaches, without introducing sample oversaturation. In conjunction with existing guidance methods, M-SWG reaches state-of-the-art Frechet DINOv2 distance on ImageNet using EDM2-XXL and DiT-XL. The code will be released.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Zhixiang Chen
28,https://openreview.net/forum?id=RKRZTfNiBU,Gen4Gen: Generative Data Pipeline for Generative Multi-Concept Composition,"In this paper, we identify two major gaps in personalizing text-to-image diffusion models, i.e., placing personalized concepts into generated image: 1) Creating a high-quality multi-concept personalized dataset with detailed and aligned text descriptions is challenging. 2) There lacks  comprehensive metrics to evaluate multiple personalized concepts in an image. To overcome these challenges, we propose Gen4Gen, a novel generative data pipeline for creating a benchmark dataset (MyCanvas) that combines personalized concepts into complex compositions aligning with detailed text descriptions, aiming to benchmark and improve multi-concept personalization. In addition, we introduce comprehensive metrics (CP-CLIP / TI-CLIP) for evaluating the performance of multi-concept personalization models more effectively. Finally, we provide a simple yet effective baseline built on top of several personalization methods with empirical prompting strategies for future researchers to evaluate on MyCanvas benchmark. By improving data quality, we can significantly increase the multi-concept image generation quality without changing the model architecture or training algorithms, and we show our work can be simply plug in to personalization approaches. We suggest that leveraging strong foundation models for dataset generation could benefit various computer vision tasks. The code and benchmark dataset will be publicly released.",7,3,~Runpei_Dong1|~Sayan_Kumar_Chaki1|~Haonan_Wang6|~Mushui_Liu1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
29,https://openreview.net/forum?id=yzkXpORheb,SPARTAN: Spatiotemporal Pose-Aware Retrieval for Text-guided Autonomous Navigation,"Generative AI—particularly video diffusion models—offers a scalable alternative to traditional data collection for autonomous navigation. However, current models are not optimized for navigation-specific tasks, often resulting in inaccuracies in physics modeling and scene dynamics. We present SPARTAN (Spatiotemporal Pose-Aware Retrieval for Text-guided Autonomous Navigation), an open-source framework that delivers significant advances to video diffusion components. At its core, SPARTAN features a novel spatiotemporal encoder that converts per-frame camera pose data into continuous spatiotemporal feature embeddings, enhancing representation and modeling efficiency. We also propose a camera pose-conditioned training pipeline and loss function that tightly integrate spatiotemporal features with text annotations to support more accurate retrieval and generation. In addition, we present DrivingScenePTX, a comprehensive driving video dataset that includes both frame-wise camera poses and rich textual scene descriptions. We benchmark SPARTAN against state-of-the-art contrastive language-image pretraining (CLIP) models using standard retrieval tasks, and introduce a novel evaluation method inspired by visual simultaneous localization and mapping (vSLAM) to assess performance in cross-domain trajectory retrieval. Our results demonstrate SPARTAN’s superior ability to retrieve driving videos with high spatial and temporal accuracy, offering a critical step forward in adapting generative AI for autonomous navigation.",5,3,~Zakaria_rguibi1|~Himangi_Mittal1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
37,https://openreview.net/forum?id=790UTKs9hX,Cross-Modal Scene Semantic Alignment for Image Complexity Assessment,"Image complexity assessment (ICA) is a challenging task in perceptual evaluation due to the subjective nature of human perception and the inherent semantic diversity in real-world images. Existing ICA methods predominantly rely on hand-crafted or shallow convolutional neural network-based features of a single visual modality, which are insufficient to fully capture the perceived representations closely related to image complexity. Recently, cross-modal scene semantic information has been shown to play a crucial role in various computer vision tasks, particularly those involving perceptual understanding. However, the exploration of cross-modal scene semantic information in the context of ICA remains unaddressed. Therefore, in this paper, we propose a novel ICA method called Cross-Modal Scene Semantic Alignment (CM-SSA), which leverages scene semantic alignment from a cross-modal perspective to enhance ICA performance, enabling complexity predictions to be more consistent with subjective human perception. Specifically, the proposed CM-SSA consists of a complexity regression branch and a scene semantic alignment branch. The complexity regression branch estimates image complexity levels under the guidance of the scene semantic alignment branch, while the scene semantic alignment branch is used to align images with corresponding text prompts that convey rich scene semantic information by pair-wise learning. Extensive experiments on several ICA datasets demonstrate that the proposed CM-SSA significantly outperforms state-of-the-art approaches. Codes will be publicly available.",5,4,~Sven_Wachsmuth2,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
38,https://openreview.net/forum?id=Te5Wg1jkpl,GC-Font: Few-Shot Font Generation via Global Contextual Feature Modelling,"Few-shot font generation aims to create new fonts with a limited number of glyph references. It can be used to greatly reduce the labour cost of manual font design. However, due to the variety and complexity of font styles, the results generated by existing methods often suffer from visible defects, such as stroke errors, blurriness or distorted shapes. To address these issues, we propose GC-Font, a novel framework which integrates a Global Contextual Feature Modelling (GCFM) module. Specifically, this module is inserted between the content encoder and decoder, where it fuses convolution and attention mechanisms to process intermediate feature maps and injects enhanced global contextual features into the decoder. Moreover, we apply adaptive convolutions to the low-level feature maps from the content encoder to strengthen contextual correlations. In addition, a skeleton consistency loss and an edge consistency loss are also designed to improve geometric alignment. Extensive experiments reveal that our GC-Font outperforms the state-of-the-art methods in both qualitative and quantitative evaluations, demonstrating its effectiveness on diverse font styles. Our source code will be released soon.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
39,https://openreview.net/forum?id=NV4XKQW2Tv,DocAttentionRect: Attention-Guided Document Image Rectification,"In recent years, document image rectification has seen substantial advancements. Nonetheless, current leading algorithms are primarily effective for images with clearly defined document boundaries and some degree of distortion. These algorithms often struggle when presented with images containing text in only a specific area or with incomplete boundaries, leading to subpar rectification results. This limitation is particularly problematic in situations where only sections of a document need processing. Although there are methods that attempt to address these issues, they frequently encounter difficulties when dealing with a combination of intricate distortions and diverse document layouts. To address this gap, our paper introduces a novel approach for document image rectification that specifically targets images with partial or missing document boundaries. Recently, attention-based neural networks have proven highly effective in enhancing the accuracy and efficiency of document rectification. By utilizing attention mechanisms, these networks can focus on relevant parts of an image, thereby improving the rectification outcomes. Our paper presents 'DocAttentionRect', an innovative attention-based rectification network that incorporates attention modules alongside parallel convolution layers to address complex document image rectification challenges. Our proposed architecture captures extensive dependencies and key textual and structural features throughout the rectification process. DocAttentionRect is capable of handling all document types, regardless of the visibility of their boundaries. Extensive experiments conducted on the DocUNet, DIR300, and UVDoc datasets demonstrate the superior performance and effectiveness of our proposed architecture.",6,5,zhixiang.chen@sheffield.ac.uk,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
43,https://openreview.net/forum?id=DY6jWum4Oh,Split Matching for Inductive Zero-shot Semantic Segmentation,"Zero-shot Semantic Segmentation (ZSS) targets the segmentation of unseen classes, i.e., classes not annotated during training. While fine-tuned vision-language models show promise, they often overfit to seen classes due to the lack of supervision. Query-based methods offer strong potential by enabling object localization without explicit labels, but conventional approaches assume full supervision and thus tend to misclassify unseen classes as background in ZSS settings. To address this issue, we propose Split Matching (SM), a novel assignment strategy that decouples Hungarian matching into two components: one for seen classes in annotated regions and another for latent classes in unannotated regions (referred to as unseen candidates). Specifically, we split the queries into seen and candidate queries, enabling each to be optimized independently according to its available supervision. To discover unseen candidates, we cluster CLIP dense features to generate pseudo masks and extract region-level embeddings using CLS tokens. Matching is then conducted separately for the two groups based on both class and mask similarity. Additionally, we introduce a Multi-scale Feature Enhancement (MFE) module that refines decoder features through residual multi-scale aggregation, improving the model’s ability to capture spatial details across resolutions. Besides, we also introduce a Random Query (RQ) strategy to further enhance the performance after training. Our method is the first to introduce decoupled Hungarian matching under the inductive ZSS setting, and achieves 0.8% and 1.1% higher hIoU on two ZSS benchmarks.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
55,https://openreview.net/forum?id=ONcVfk8Yqe,Piezoelectric Acoustic Sensing for Sitting Pose Classification,"Recognizing human poses, particularly under seated conditions, has become increasingly important due to the widespread adoption of remote work and teleconferencing. In this study, we present the first attempt to classify sitting poses using active acoustic sensing based on contact-type piezoelectric devices attached to a chair. Our framework analyzes multi-channel acoustic responses of known sweep signals transmitted through the chair and the seated body and captured by a piezoelectric microphone array. To enhance classification performance, we introduce two learning techniques tailored to this setting. Specifically, we introduce ChannelSwap (CS), a data augmentation method that leverages the geometric symmetry of the sensing system, and Symmetric Consistency Enhancement (SCE), a learning strategy designed to compensate for real-world symmetry deviations. Experiments on real sound data demonstrate that our method improves classification accuracy by 3.6\% compared to standard baselines, validating the feasibility and effectiveness of piezoelectric acoustic sensing for sitting pose classification.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
56,https://openreview.net/forum?id=IIygL1vdWx,Pointly-Supervised Weak-Shot Semantic Segmentation via Dual Mapping Transfer,"Despite the existing large-scale datasets with full annotations, semantic segmentation still suffers from high annotation cost when extending to novel classes. One learning paradigm to relieve such annotation burden is weak-shot semantic segmentation, which jointly uses full annotations for base classes and weak annotations (image-level labels) for novel classes. In this work, we follow the weak-shot learning paradigm, but use more informative point-level annotations for novel classes. Our method is built upon query-based segmentation network, which learns a query-to-mask mapping. With annotated points (keypoints), we additionally learn a keypoint-to-mask mapping. Both mappings are transferable from base classes to novel classes. Moreover, the latter mapping could help the former mapping when learning to segment novel classes. Besides, we design a confident memory bank to alleviate the ambiguity of novel classes. Extensive experiments on COCO-Stuff-10K and ADE20K datasets show the superiority of our method.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
59,https://openreview.net/forum?id=gXFNLsnaDw,CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance,"Murals, as invaluable cultural artifacts, face continuous deterioration from environmental factors and human activities. Digital restoration of murals faces unique challenges due to their complex degradation patterns and the critical need to preserve artistic authenticity. Existing learning-based methods struggle with maintaining consistent mask guidance throughout their networks, leading to insufficient focus on damaged regions and compromised restoration quality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network that addresses these limitations through comprehensive mask guidance and multi-scale feature extraction. Our framework introduces two key component: (1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask sensitivity across resolution scales through dedicated channel-wise feature selection and mask-guided feature fusion; and (2) the Co-Feature Aggregator (CFA), operating at both the highest and lowest resolutions to extract complementary features for capturing fine textures and global structures in degraded regions. Experimental results on benchmark datasets demonstrate that CMAMRNet outperforms state-of-the-art methods, effectively preserving both structural integrity and artistic details in restored murals.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
66,https://openreview.net/forum?id=n4w6qhuRSR,ST-GDance: Long-Term and Collision-Free Group Choreography from Music,"Group dance generation from music has broad applications in film, gaming, and animation production. However, it requires synchronizing multiple dancers while maintaining spatial coordination. As the number of dancers and sequence length increase, this task faces higher computational complexity and a greater risk of motion collisions. Existing methods often struggle to model dense spatial-temporal interactions, leading to scalability issues and multi-dancer collisions. To address these challenges, we propose ST-GDance, a novel framework that decouples spatial and temporal dependencies to optimize long-term and collision-free group choreography. We employ lightweight graph convolutions for distance-aware spatial modeling and accelerated sparse attention for efficient temporal modeling. This design significantly reduces computational costs while ensuring smooth and collision-free interactions. Experiments on the AIOZ-GDance dataset demonstrate that ST-GDance outperforms state-of-the-art baselines, particularly in generating long and coherent group dance sequences. The code will be released upon paper acceptance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Edmond S. L. Ho
69,https://openreview.net/forum?id=U7lEKveycG,TACTFL: Temporal Contrastive Training for Multi-modal Federated Learning with Similarity-guided Model Aggregation,"Real-world federated learning faces two key challenges: limited access to labelled data and the presence of heterogeneous multi-modal inputs. This paper proposes TACTFL, a unified framework for semi-supervised multi-modal federated learning. TACTFL introduces a novel temporal contrastive training scheme that learns modality-agnostic representations from unlabelled client data by leveraging temporal alignment across modalities. However, as clients perform self-supervised training on heterogeneous data, local models may diverge semantically. To mitigate this, TACTFL incorporates a similarity-guided model aggregation strategy that dynamically weights client models based on their representational consistency, promoting global alignment. Extensive experiments across diverse benchmarks and modalities, including video, audio, and wearable sensors, demonstrate that TACTFL achieves state-of-the-art performance. For instance, on the UCF101 dataset with only 10\% labelled data, TACTFL attains 68.48\% top-1 accuracy, significantly outperforming the FedOpt baseline of 35.35\%. Code will be released upon publication.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
77,https://openreview.net/forum?id=NF4wr4u9rb,Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance,"Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering.
Our results highlight the effectiveness of leveraging *dual image prompting* with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks.",7,4,~Salehe_Erfanian_Ebadi1|~S._Alireza_Golestaneh2|~Rainer_Stiefelhagen1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
81,https://openreview.net/forum?id=uvOAJO1VaV,SSNeRF: Sparse View Semi-supervised Neural Radiance Fields with Augmentation,"Sparse-view NeRF is challenging because limited input images lead to an under-constrained optimization problem for volume rendering. Existing methods address this issue by relying on supplementary information, such as depth maps. However, generating this supplementary information accurately remains problematic and often leads to NeRF producing images with undesired artifacts.
To address artifacts and enhance robustness, we propose SSNeRF, a semi-supervised sparse-view NeRF framework based on a teacher-student paradigm. In this framework, the student NeRF is challenged by severe sparse-view degradation while being guided by rectified high-confidence pseudo-labels from the teacher NeRF.
Our sparse-view-specific degradations include injecting designed noise into volume rendering weights, perturbing vulnerable layers, and simulating sparse-view blurriness. These techniques together force the student NeRF to recognize degradation caused by sparse-view training data, ultimately helping the model effectively handle the noise and incomplete information inherent in sparse views.
Extensive experiments demonstrate the effectiveness of our SSNeRF in generating novel views with less sparse-view degradation. We will release code upon acceptance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
83,https://openreview.net/forum?id=cG85S9RNPE,RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze,"Single-image dehazing under dense and non-uniform haze conditions remains challenging due to severe information degradation and spatial heterogeneity. Traditional diffusion-based dehazing methods struggle with insufficient generation conditioning and lack of adaptability to spatially varying haze distributions, which leads to suboptimal restoration. To address these limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing Diffusion Model for robust visibility enhancement in complex haze scenarios. RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST) strategy, which leverages physical priors to reformulate the diffusion Markov chain by generation target transitions, mitigating the issue of insufficient conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising timesteps employing a transmission map cross-attention mechanism, adeptly managing non-uniform haze distributions. Extensive experiments across four real-world datasets demonstrate that RPD-Diff achieves state-of-the-art performance in challenging dense and non-uniform haze scenarios, delivering high-quality, haze-free images with superior detail clarity and color fidelity.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
84,https://openreview.net/forum?id=b2LyC20XCz,Continual Vision-and-Language Navigation,"Developing Vision-and-Language Navigation (VLN) agents typically assumes a \textit{train-once-deploy-once} strategy, which is unrealistic as deployed agents continually encounter novel environments. To address this, we propose the Continual Vision-and-Language Navigation (CVLN) paradigm, where agents learn and adapt incrementally across multiple \textit{scene domains}. CVLN includes two setups: Initial-instruction based CVLN for instruction-following, and Dialogue-based CVLN for dialogue-guided navigation. We also introduce two simple yet effective baselines for sequential decision-making: Perplexity Replay (PerpR), which replays difficult episodes, and Episodic Self-Replay (ESR), which stores and revisits action logits during training. Experiments show that existing continual learning methods fall short for CVLN, while PerpR and ESR achieve better performance by efficiently utilizing replay memory.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
88,https://openreview.net/forum?id=u6jgHVTs4O,CRCE: Coreference-Retention Concept Erasure in Text-to-Image Diffusion Models,"Text-to-Image diffusion models can produce undesirable content that necessitates concept erasure. However, existing methods struggle with under-erasure, leaving residual traces of targeted concepts, or over-erasure, mistakenly eliminating unrelated but visually similar concepts. To address these limitations, we introduce CRCE, a novel concept erasure framework that leverages Large Language Models to identify both semantically related concepts that should be erased alongside the target and distinct concepts that should be preserved. By explicitly modelling coreferential and retained concepts semantically, CRCE enables more precise concept removal, without unintended erasure. Experiments demonstrate that CRCE outperforms existing methods on diverse erasure tasks, including real-world object, person identities, and abstract intellectual property characteristics.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
102,https://openreview.net/forum?id=bubW26EDEG,STAIN: Smooth Tile-Aware Instance Normalisation for Virtual Staining,"We propose STAIN, a method for eliminating tiling artefacts when virtual staining models are applied tile-wise to very high resolution whole slide images (WSIs). Cancer diagnosis depends on examination of tissue samples under a microscope. The samples are naturally transparent, so chemical staining agents are applied to enhance their visibility - a costly and time consuming process. While virtual staining models have been developed to simulate the staining process, they must be applied tile-wise due to the very high resolution of scanned whole slide images. The tiling process introduces artefacts, such as colour and contrast differences between tiles and distorted objects at tile boundaries. To address the tiling artefact problem we combine a pixel-wise instance normalisation layer, with an appropriate padding technique, ensuring smooth colour and contrast transitions between tiles and preserving the structure of objects crossing tile boundaries. The resulting WSI is visually consistent and artefact free. Furthermore, we propose a new method for detecting tiling artefacts using gradient information at the seams between tiles. Experiments on three common virtual staining datasets show that STAIN significantly reduces artefacts compared with existing methods.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
113,https://openreview.net/forum?id=kZoKwxhrcL,Prompt-Based Exemplar Super-Compression and Regeneration for Class-Incremental Learning,"Replay-based methods in class-incremental learning (CIL) have attained remarkable success. Despite their effectiveness, the inherent memory restriction results in saving a limited number of exemplars with poor diversity. In this paper, we introduce PESCR, a novel approach that substantially increases the quantity and enhances the diversity of exemplars based on a pre-trained general-purpose diffusion model, without fine-tuning it on target datasets or storing it in the memory buffer. Images are compressed into visual and textual prompts, which are saved instead of the original images, decreasing memory consumption by a factor of 24. In subsequent phases, diverse exemplars are regenerated by the diffusion model. We further propose partial compression and diffusion-based data augmentation to minimize the domain gap between generated exemplars and real images. PESCR significantly improves CIL performance across multiple benchmarks, e.g., 3.2% above the previous state-of-the-art on ImageNet-100. The code is available at https://github.com/KerryDRX/PESCR.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
117,https://openreview.net/forum?id=Zle6UUX5bm,RAPrivacy: a Readable Anonymizer for Privacy Preserving Action Recognition,"The growing awareness and demand for privacy protection have become increasingly prominent in the field of action recognition, which involves processing image sequences that capture human motion. Consequently, ensuring both accuracy and protecting sensitive information presents a significant challenge. Existing methods often face difficulties in balancing privacy preservation with effective action recognition, often resulting in images that are not perceptible to the human eye. To address these, we introduce RAPrivacy, a novel model that leverages adversarial learning in conjunction with style transfer to generate images that maintain privacy while remaining interpretable for human observers in the context of action recognition. For example, such images enable medical personnel to remotely monitor a patient’s physical condition without revealing the patient's entire identity. Empirical evaluations on the VP-UCF101 and VP-HMDB51 datasets demonstrate the effectiveness of RAPrivacy, achieving competitive performance in action recognition while significantly enhancing privacy protection. Notably, to the best of our knowledge, this is the first study in the domain of privacy-preserving action recognition that ensures human eye readability.",5,4,~Amir_Nakib1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
121,https://openreview.net/forum?id=eBibVicz45,BOTM: Echocardiography Segmentation via Bi-directional Optimal Token Matching,"Existed echocardiography segmentation methods often suffer from anatomical inconsistency challenge caused by shape variation, partial observation and region ambiguity with similar intensity across 2D echocardiographic sequences, resulting in false positive segmentation with anatomical defeated structures in challenging low signal-to-noise ratio conditions. To provide a strong anatomical guarantee across different echocardiographic frames, we propose a novel segmentation framework named __BOTM__ (**B**i-directional **O**ptimal **T**oken **M**atchig) that performs echocardiography segmentation and optimal anatomy transportation simultaneously. Given paired echocardiographic images, BOTM learns to match two sets of discrete image tokens by finding optimal correspondences from a novel anatomical transportation perspective. We further extend the token matching into a bi-directional cross-transport attention proxy to regulate the preserved anatomical consistency within the cardiac cyclic deformation in temporal domain. Extensive experimental results show that BOTM can generate stable and accurate segmentation outcomes (e.g. -1.917 HD on CAMUS2H LV, +1.9% Dice on TED), and provide a better matching interpretation with anatomical consistency guarantee.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
122,https://openreview.net/forum?id=kgsknep6MS,Distribution-guided Generative Replay with Semantic Prompts for Class-Incremental Chest X-ray Diagnosis,"Existing class-incremental learning (CIL) methods perform poorly for the diagnosis of medical images. Some CIL approaches require storing exemplars, which raises privacy and storage concerns. Others rely on unconditioned generative replay, compromising discriminative power. To overcome these issues, we propose a new CIL framework for chest x-ray (CXR) diagnosis that combines prompt tuning with distribution-guided generative replay at the feature level. The prompts are initialized with semantically rich embeddings and refined through training. A variational autoencoder captures the feature distribution in latent space, enabling past knowledge retention without storing raw data. To balance stability and plasticity, parts of the network are frozen after the initial phase while others adapt to new classes. In each session, the model learns new classes using real data and replays the synthetic features of old ones to reduce forgetting. A classification module picks the closest class prompt using cosine similarity. We evaluate our method on public CXR datasets. It outperforms prior CIL methods in accuracy and retention. We achieve up to 9% improvement in average accuracy compared to SOTA methods.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
124,https://openreview.net/forum?id=jJHvqkKzTv,Mask2Act: Predictive Multi-Object Tracking as Video Pre-Training for Robot Manipulation,"Imitation learning from demonstrations is a promising paradigm in robot learning. However, it requires a large amount of robot demonstration data, which is laborious to collect. To address this challenge, recent methods leverage action-free video data to extract skill knowledge and generate latent plans, which contain the predicted future states and guide policy learning. In this paper, we introduce Mask2Act, a novel framework that leverages action-free videos to train a predictive multi-object tracking model to predict future masks of any task-relevant objects in the video as the latent plans. The transition of object masks enriches the latent plans with accurate task-relevant motion knowledge and eliminates distracting information. The latent plans are subsequently utilized to guide policy learning with limited training data. Experiments across 50 manipulation tasks in 2 simulated environments show that our method significantly outperforms other video pre-training methods. Furthermore, Mask2Act inherently guides the model to focus on task-relevant knowledge rather than irrelevant background and distractor information, thus demonstrating superior capability in extracting skill knowledge from cross-task videos and generalizing to new tasks and environments.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
126,https://openreview.net/forum?id=drivjuKp7E,CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval,"The recent growth of large foundation models that can easily generate pseudo labels for huge quantity of unlabeled data makes unsupervised Zero-Shot Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with noisy pseudo labels generated by large foundation models such as CLIP. To this end, we propose CLAIR to refine the noisy pseudo labels with a confidence score from the similarity between the pseudo label text and image features. Furthermore, we design inter-instance and inter-cluster contrastive losses to encode semantic features that align distinct image domains into a shared class-aware latent space, and an inter-domain contrastive loss to alleviate domain discrepancies. We also learn a cross-domain mapping function in closed-form using only CLIP text embeddings to project image features from one domain to another, thereby further aligning the image features for retrieval. Finally, we enhance the zero-shot generalization ability of our CLAIR to handle novel categories by introducing an extra set of learnable prompts. Extensive experiments are carried out using TUBerlin, Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR consistently shows superior performance compared to existing state-of-the-art methods. Our code will be open-sourced upon paper acceptance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
128,https://openreview.net/forum?id=BCvFUNbz9v,Uncertainty Diffusion: Parameter-Efficient Depth Refinement via Uncertainty-Guided Diffusion Models,"We present Uncertainty Diffusion, a model-agnostic framework for refining monocular depth maps by integrating pixel-wise uncertainty into a diffusion-based process. Our method adaptively focuses refinement on regions with low prediction confidence by leveraging an uncertainty-guided sampling mechanism, enabling targeted and effective depth enhancement. For domain adaptation, only a lightweight refinement network is fine-tuned while keeping the base model fixed, resulting in a parameter-efficient adaptation strategy. Extensive experiments on NYU Depth V2, DIODE, and SUN RGB-D demonstrate consistent improvements across diverse baseline models, including recent large-scale approaches. Notably, our framework achieves up to a 61\% reduction in log10 error for domain adaptation, all without retraining the base models. These results highlight the practicality and versatility of Uncertainty Diffusion for robust monocular depth estimation in varied environments.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
131,https://openreview.net/forum?id=VtHiWEQk3g,Dual-Stream Attention with Multi-Modal Queries for Object Detection in Transportation Applications,"Transformer-based object detectors often struggle with occlusions, fine-grained localization, and computational inefficiency caused by fixed queries and dense attention. We propose DAMM, Dual-stream Attention with Multi-Modal queries, a novel framework introducing both query adaptation and structured cross-attention for improved accuracy and efficiency. DAMM capitalizes on three types of queries: appearance-based queries from vision-language models, positional queries using polygonal embeddings, and random learned queries for broader scene coverage. Furthermore, a dual-stream cross-attention module separately refines semantic and spatial features, boosting localization precision in cluttered scenes. We evaluated DAMM on four challenging benchmarks and DAMM achieves state-of-the-art performance in average precision (AP) and recall, demonstrating the effectiveness of multi-modal query adaptation and dual-stream attention.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
133,https://openreview.net/forum?id=pboUdQZ5CX,M$^2$StyleGS: Multi-Modality 3D Style Transfer with Gaussian Splatting,"Conventional 3D style transfer methods rely on a fixed reference image to apply artistic patterns to 3D scenes. However, in practical applications such as virtual or augmented reality, users often prefer more flexible inputs, including textual descriptions and diverse imagery. In this work, we introduce a novel real-time styling technique M$^2$StyleGS to generate a sequence of precisely color-mapped views. It utilizes 3D Gaussian Splatting (3DGS) as a 3D presentation and multi-modality knowledge refined by CLIP as a reference style. M$^2$StyleGS resolves the abnormal transformation issue by employing a precise feature alignment, namely ``subdivisive flow"", it strengthens the projection of the mapped CLIP text-visual combination feature to the VGG style feature. In addition, we introduce observation loss, which assists in the stylized scene better matching the reference style during the generation, and suppression loss, which suppresses the offset of reference color information throughout the decoding process. By integrating these approaches, M$^2$StyleGS can employ text or images as references to generate a set of style-enhanced novel views. Our experiments show that M$^2$StyleGS achieves better visual quality and surpasses the previous work by up to 32.92\% in terms of consistency.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
136,https://openreview.net/forum?id=q2hoCGpGjb,Lang4D: Weakly Supervised Learning of 4D Language Splatting,"Techniques for obtaining the abstract meaning of scenes as language embeddings at arbitrary positions in time-varying 3D spaces, i.e., in 4D spaces, are expected to enable applications such as content search and prediction in digital archives and digital twins represented as 4D media. In this study, to acquire such a ""4D language field,"" we focus on two key challenges when applying existing methods that are designed for static scenes: (1) expanding the previous techniques to 4D space and (2) leveraging video-level language embeddings obtained from vision-language video foundation models. To address these challenges simultaneously, we propose ""Lang4D,"" a novel method that models the temporal changes of language embeddings in 3D space using video foundation models in a data-driven manner. Lang4D employs a video foundation model that inputs virtual viewpoint images, where the recognition accuracy is stable, to obtain a common language embedding across all times and pixels for each video. Subsequently, it weakly supervises the learning of fluctuations in the language embeddings projected onto virtual viewpoints. In our experiments, we constructed a dataset specifically to evaluate the novel 4D language grounding and segmentation task. We verified the effectiveness of the proposed method in addressing these two challenges through quantitative evaluations and ablation studies.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
138,https://openreview.net/forum?id=fvuAWrJk25,SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet,"ControlNet has enabled detailed spatial control in text-to-image diffusion models by incorporating additional visual conditions such as depth or edge maps. However, its effectiveness heavily depends on the availability of visual conditions that are precisely aligned with the generation goal specified by text prompt—a requirement that often fails in practice, especially for uncommon or imaginative scenes. For example, generating an image of a cat cooking in a specific pose may be infeasible due to the lack of suitable visual conditions. In contrast, structurally similar cues can often be found in more common settings—for instance, poses of humans cooking are widely available and can serve as rough visual guides. Unfortunately, existing ControlNet models struggle to use such loosely aligned visual conditions, often resulting in low text fidelity or visual artifacts. To address this limitation, we propose SemanticControl, a training-free method for effectively leveraging misaligned but semantically relevant visual conditions. Our approach adaptively suppresses the influence of the visual condition where it conflicts with the prompt, while strengthening guidance from the text. The key idea is to first run an auxiliary denoising process using a surrogate prompt aligned with the visual condition (e.g., “a human playing guitar” for a human pose condition) to extract informative attention masks, and then utilize these masks during the denoising of the actual target prompt (e.g., “cat playing guitar”). Experimental results demonstrate that our method improves performance under loosely aligned conditions across various conditions, including depth maps, edge maps, and human skeletons, outperforming existing baselines. Our code will be publicly available upon publication.",5,3,~Salehe_Erfanian_Ebadi1|~Anil_Batra1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
147,https://openreview.net/forum?id=m2hrPJJe8W,Spatial-Frequency Domain Aggregation for Visual Place Recognition,"Visual place recognition (VPR) aims to identify an image by matching its global representation with those of reference images. Existing methods primarily aggregate features in the spatial domain, usually ignoring critical global or structural information embedded in the frequency domain. To address this limitation, we propose Dual-Domain Aggregation for Visual Place Recognition (DDA-VPR), which jointly integrates spatial and frequency domain features to enhance global representation aggregation. The core motivation is that the frequency domain inherently captures global and structural patterns, offering complementary and discriminative cues that are difficult to capture in the spatial domain. To bridge the domain gap between spatial and frequency features, a triple fusion strategy is introduced to facilitate cross-domain interaction and fuse spatial and frequency domain features into a unified and robust global representation. Additionally, to refine feature abstraction in both domains, a multi-scale contextual attention module is designed to leverage multi-scale information and preserve critical details during downsampling. Consequently, DDA-VPR generates more discriminative representations than methods that aggregate features solely in the spatial domain. Extensive experiments on challenging benchmarks demonstrate the superior performance of DDA-VPR, validating the effectiveness of dual-domain aggregation in VPR.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
150,https://openreview.net/forum?id=xS0TSBZvKB,WTNet: A Weather Transfer Network for Domain-Adaptive All-In-One Adverse Weather Image Restoration,"All-in-one adverse weather image restoration has attracted increasing attention due to its ability to recover high-quality images in a unified model. However, existing methods often suffer from significant performance drops due to the domain gap between training and testing images. Moreover, they typically yield only balanced, rather than optimal, performance across different weather conditions, especially when compared to models trained in a weather-specific manner. To solve this problem, we propose a novel Weather Transfer Network (WTNet), which fine-tunes all-in-one models to enhance their performance during testing. As paired degraded-clean images are unavailable during testing, WTNet transfers degraded patterns from unseen target domains to the source-domain clean images, thereby constructing domain-adaptive fine-tuning sets for effective domain adaptation. Additionally, by leveraging these fine-tuning sets, all-in-one models can be dynamically adapted to weather-specific or mixed weather models based on the degradation patterns observed during testing. Experimental results demonstrate that WTNet can significantly enhance state-of-the-art all-in-one models on benchmark real-world image deraining, desnowing, and dehazing datasets.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
154,https://openreview.net/forum?id=QYpBt4pIqZ,Seed-to-Seed: Unpaired Image Translation in Diffusion Seed Space,"We introduce Seed-to-Seed Translation (StS), a novel approach that combines GANs and diffusion models (DMs) for unpaired Image-to-Image Translation. Our approach is aimed at global translations of complex automotive scenes, where close adherence to the structure and semantics of the source image is essential. We demonstrate that the semantic information encoded in the space of inverted seeds of a pretrained DM, dubbed as the seed-space, can be used for discriminative tasks, and leverage this information to perform image-to-image translation. Our method involves training an sts-GAN, an unpaired seed-to-seed translation model, based on CycleGAN. The translated seeds are used as the starting point for the DM’s sampling process, while structure preservation is ensured using a ControlNet. We demonstrate the effectiveness of our approach for structure-preserving translation of complex automotive scenes, showcasing superior performance compared to existing GAN-based and diffusion-based methods. In addition to advancing the SoTA in automotive scene translations, our approach offers a fresh perspective on leveraging the semantic information encoded within the seed-space of pretrained DMs for effective image editing and manipulation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
159,https://openreview.net/forum?id=hG3sgNwHLk,Identity-Motion Trade-offs in Text-to-Video Generation,"Text-to-video diffusion models have shown remarkable progress in generating coherent video clips from textual descriptions. However, the interplay between motion, structure, and identity representations in these models remains under-explored. Here, we investigate how self-attention query ($Q$) features simultaneously govern motion, structure, and identity and examine the challenges arising when these representations interact. Our analysis reveals that Q affects not only layout, but that during denoising Q also has a strong effect on subject identity, making it hard to transfer motion without the side-effect of transferring identity. Understanding this dual role enabled us to control query feature injection (Q injection) and demonstrate two applications: (1) a zero-shot motion transfer method that is 10$\times$ more efficient than existing approaches, and (2) a training-free technique for consistent multi-shot video generation, where characters maintain identity across multiple video shots while Q injection enhances motion fidelity.",5,4,~Xingli_Yang1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
165,https://openreview.net/forum?id=Mfzol6CwP7,Four eyes see more than two: Dataset Distillation with Mixture-of-Experts,"The ever-growing size of datasets in deep learning presents a significant challenge in terms of training efficiency and computational cost. Dataset distillation (DD) has emerged as a promising approach to address this challenge by generating compact synthetic datasets that retain the essential information of the original data. However, existing DD methods often suffer from performance degradation when transferring distilled datasets across different network architectures (i.e. the model utilizing distilled dataset for further training is different from the one used in dataset distillation). To overcome this limitation, we propose a novel mixture-of-experts framework for dataset distillation. Our goal focuses on promoting diversity within the distilled dataset by distributing the distillation tasks to multiple expert models. Each expert specializes in distilling a distinct subset of the dataset, encouraging them to capture different aspects of the original data distribution. To further enhance diversity, we introduce a distance correlation minimization strategy to encourage the experts to learn distinct representations. Moreover, during the testing stage (where the distilled dataset is used for training a new model), the mixup-based fusion strategy is applied to better leverage the complementary information captured by each expert. Through extensive experiments, we demonstrate that our framework effectively mitigates the issue of cross-architecture performance degradation in dataset distillation, particularly in low-data regimes, leading to more efficient and versatile deep learning models while being trained upon the distilled dataset.",7,4,~Tae-Min_Choi1|~Muhammad_Abdullah_Adnan1|~Haonan_Wang6,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
168,https://openreview.net/forum?id=7N6QAVqSsA,PSScreen: Partially Supervised Multiple Retinal Disease Screening,"Leveraging multiple partially labeled datasets to train a model for multiple retinal disease screening reduces the reliance on fully annotated datasets, but remains challenging due to significant domain shifts across training datasets from various medical sites, and the label absent issue for partial classes. To solve these challenges, we propose PSScreen, a novel Partially Supervised multiple retinal disease Screening model. Our PSScreen consists of two streams and one learns deterministic features and the other learns probabilistic features via instance normalization integrated with uncertainty injection. Then, we leverage the textual guidance to decouple two types of features into disease-wise features and align them via feature distillation to boost the domain generalization ability. Meanwhile, we employ pseudo label consistency regularization between the two streams to address the label absence issue and introduce a self-distillation to transfer the task-relevant semantics about the known classes from the deterministic to the probabilistic stream to further enhance the detection performances. Experiments show that our PSScreen significantly enhances the detection performances on six retinal diseases and the normal state averagely and achieves state-of-the-art results on both in-domain and out-of-domain datasets. Codes are available at https://anonymous.4open.science/r/PSScreen-CA13.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
173,https://openreview.net/forum?id=lVFJFDqZLV,CellMamba: Adaptive Mamba for Accurate and Efficient Cell Detection,"Cell detection in pathological images presents unique challenges due to densely packed objects, subtle inter-class differences, and severe background clutter. In this paper, we propose CellMamba, a lightweight and accurate one-stage detector tailored for fine-grained biomedical instance detection. Built upon a VSSD backbone, CellMamba integrates CellMamba Blocks, which couple either NC-Mamba or Multi-Head Self-Attention (MSA) with a novel Triple-Mapping Adaptive Coupling (TMAC) module. TMAC enhances spatial discriminability by splitting channels into two branches, each equipped with dual idiosyncratic and one consensus attention map, adaptively fused to preserve local sensitivity and global consistency. Furthermore, we design an Adaptive Mamba Head that fuses multi-scale features via learnable weights for robust detection under varying object sizes. Extensive experiments on two public datasets—CoNSeP and CytoDArk0—demonstrate that CellMamba outperforms both CNN-based, Transformer-based, and Mamba-based baselines in accuracy, while significantly reducing model size and inference latency. Our results validate CellMamba as an efficient and effective solution for high-resolution cell detection.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
181,https://openreview.net/forum?id=RWyuBFm1z0,LuKAN: A Kolmogorov-Arnold Network Framework for 3D Human Motion Prediction,"The goal of 3D human motion prediction is to forecast future 3D poses of the human body based on historical motion data. Existing methods often face limitations in achieving a balance between prediction accuracy and computational efficiency. In this paper, we present LuKAN, an effective model based on Kolmogorov-Arnold Networks (KANs) with Lucas polynomial activations. Our model first applies the discrete wavelet transform to encode temporal information in the input motion sequence. Then, a spatial projection layer is used to capture inter-joint dependencies, ensuring structural consistency of the human body. At the core of LuKAN is the Temporal Dependency Learner, which employs a KAN layer parameterized by Lucas polynomials for efficient function approximation. These polynomials provide computational efficiency and an enhanced capability to handle oscillatory behaviors. Finally, the inverse discrete wavelet transform reconstructs motion sequences in the time domain, generating temporally coherent predictions. Extensive experiments on three benchmark datasets demonstrate the competitive performance of our model compared to strong baselines, as evidenced by both quantitative and qualitative evaluations. Moreover, its compact architecture coupled with the linear recurrence of Lucas polynomials, ensures computational efficiency.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
182,https://openreview.net/forum?id=8T2g72mULl,eXtended Multimodal Composite Association Score (xMCAS): A Gender Inclusive Approach to Measurement of Bias in Text-To-Image Diffusion Models,Text-To-Image Diffusion Models such as DALL-E and Stable Diffusion have become extremely capable of generating images from text prompts. They have also been shown to exhibit stereotypical gender bias. Previous research has identified and measured the prevalence of gender bias in a binary sense. We introduce eXtended Multimodal Composite Association Score (xMCAS): a novel and easy-to-interpret metric capable of detecting and measuring gender bias adopting a more inclusive concept of gender. Our analysis using this metric revealed the presence of stereotypical concepts of non-binary people in both DALL-E 2 and Stable Diffusion.,3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
184,https://openreview.net/forum?id=D5u47wujSA,Graph Similarity Learning of Floor Plans,"Architectural floor plans visually depict building layouts and are often represented as graphs to capture their underlying spatial relationships. 
Efficient comparison of these graphs is critical for applications like search, clustering, and data visualization. 
Graph matching networks -- the most successful methods to compare graphs, rely on costly intermediate cross-graph node-level interactions, therefore being slow in inference time. 
We introduce a more efficient approach that postpones the cross-graph node-level interactions to the end of the joint embedding architecture.
We do so by using a differentiable path-based graph kernel as a distance function on the final learned node-level embeddings.
Compared to graph matching networks, our method is qualitatively comparable or better while significantly increasing the speed.
Code is open.",5,3,~Giuseppe_Mangioni1|~Simon_Weber1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
195,https://openreview.net/forum?id=QB6pSewUYB,Proto-FG3D: Prototype-based Interpretable Fine-Grained 3D Shape Classification,"Deep learning-based multi-view coarse-grained 3D shape classification has achieved remarkable success over the past decade, leveraging the powerful feature learning capabilities of CNN-based and ViT-based backbones. However, as a challenging research area critical for detailed shape understanding, fine-grained 3D classification remains understudied due to the limited discriminative information captured during multi-view feature aggregation, particularly for subtle inter-class variations, class imbalance, and inherent interpretability limitations of parametric model. To address these problems, we propose the first prototype-based framework named Proto-FG3D for fine-grained 3D shape classification, achieving a paradigm shift from parametric softmax to non-parametric prototype learning. Firstly, Proto-FG3D establishes joint multi-view and multi-category representation learning via Prototype Association. Secondly, prototypes are refined via Online Clustering, improving both the robustness of multi-view feature allocation and inter-subclass balance. Finally, prototype-guided supervised learning is established to enhance fine-grained discrimination via prototype-view correlation analysis and enables ad-hoc interpretability through transparent case-based reasoning. Experiments on FG3D and ModelNet40 show Proto-FG3D surpasses state-of-the-art methods in accuracy, transparent predictions, and ad-hoc interpretability with visualizations, challenging conventional fine-grained 3D recognition approaches.",5,3,~Hao_Huang3|~Riccardo_Renzulli1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
211,https://openreview.net/forum?id=Fykj9CshfZ,Efficient Image Restoration via Latent Consistency Flow Matching,"Recent advances in generative image restoration (IR) have demonstrated impressive results. However, these methods are hindered by their substantial size and computational demands, rendering them unsuitable for deployment on edge devices. This work introduces ELIR, an Efficient Latent Image Restoration method. ELIR addresses the distortion-perception trade-off within the latent space and produces high-quality images using a latent consistency flow-based model. In addition, ELIR introduces an efficient and lightweight architecture. Consequently, ELIR is 4x smaller and faster than state-of-the-art diffusion and flow-based approaches for blind face restoration, enabling a deployment on resource-constrained devices. Comprehensive evaluations of various image restoration tasks and datasets show that ELIR achieves competitive performance compared to state-of-the-art methods, effectively balancing distortion and perceptual quality metrics while significantly reducing model size and computational cost.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
218,https://openreview.net/forum?id=qXRLHXOAoa,Bridging Visual-Textual Modalities: Weakly Supervised Histopathology Segmentation,"In weakly supervised histopathology tissue segmentation, Class Activation Maps (CAMs) are commonly used to generate pseudo-masks. However, CAMs typically highlight only the most discriminative regions, leading to inaccurate tissue boundaries. Existing visual-based refinement strategies often exacerbate information loss, while text-based methods suffer from high inter-class similarity and a semantic gap between pixel-level features and text labels. To overcome these limitations, we propose a two-stage segmentation framework that jointly models a bidirectional shared latent space between visual and textual modalities to enhance pseudo-mask quality. In the segmentation (second) stage, we incorporate complex tissue textual descriptions as external discriminative knowledge to compensate for insufficient supervision. We further develop a multi-stage modality fusion strategy based on learnable query tokens and Fourier transforms. Experiments conducted on the LUAD-HistoSeg and BCSS-WSSS datasets demonstrate that our method surpasses state-of-the-art weakly supervised tissue segmentation approaches.",7,3,~Shams_Ur_Rahman1|~Hoang_Vu_Nguyen2|~Woojung_Han1|~Debanjan_Goswami1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
220,https://openreview.net/forum?id=6RwMpBlZVf,LiDAR MOT-DETR: A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking,"Multi-object tracking from LiDAR point clouds presents unique challenges due to the sparse and irregular nature of the data, compounded by the need for temporal coherence across frames. Traditional tracking systems often rely on hand-crafted features and motion models, which can struggle to maintain consistent object identities in crowded or fast-moving scenes. We present a lidar-based two-staged DETR inspired transformer; a smoother and tracker. The smoother stage refines lidar object detections, from any off-the-shelf detector, across a moving temporal window. The tracker stage uses a DETR-based attention block to maintain tracks across time by associating tracked objects with the refined detections using the point cloud as context.
The model is trained on the datasets nuScenes and KITTI in both online and offline (forward peeking) modes demonstrating strong performance across metrics such as ID-switch and multiple object tracking accuracy (MOTA). The numerical results indicate that the online mode outperforms the lidar-only baseline and SOTA models on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475, while the offline mode provides an additional 3 pp aMOTP.",5,4,~Shuai_Li19,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
228,https://openreview.net/forum?id=Jbuqt7Bh95,Dual-Expert Collaborative Network for Fake News Detection with External Knowledge Integration,"The pervasive dissemination of misinformation has imposed a profound negative impact on societal structures, making multi-modal fake news detection an urgent research priority. Traditional approaches mainly focus on evaluating the authenticity and consistency of the original news modalities, often neglecting the contextual understanding necessary to avoid cognitive biases in complex scenarios. In this paper, we propose a Dual-Expert Collaborative Network (DECNet) for fake news detection, a novel approach that integrates original news content with external knowledge. Specifically, we leverage multi-scale images to extract visual features and design a Content-Driven Expert (CDE) that performs hierarchical fusion with textual data, enhancing semantic alignment and uncovering deeper insights of news content. Simultaneously, inspired by the reasoning capabilities of Large Vision-Language Models (LVLMs), we propose a Knowledge-Augmentation Expert (KAE) that leverages generated explanations alongside the inherent multi-modal information of the news to capture subtle cues indicative of fake news. To facilitate interaction between these two branches, a gating network is designed for seamless collaboration. Extensive experiments on two widely-employed datasets demonstrate that our method outperforms current state-of-the-art approaches.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
232,https://openreview.net/forum?id=QjPSRro6rI,Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering,"We propose a simple yet effective approach to tackle the under-explored problem of unsupervised video continual learning, in a realistic scenario where neither task boundaries nor labels are provided for learning a succession of tasks without forgetting. Videos represent a complex and rich spatio-temporal media information, widely used in many applications, but which have not been sufficiently explored in unsupervised continual learning. Prior studies have only focused on supervised continual learning, relying on the knowledge of labels and task boundaries, which is costly and not practical in real scenarios. To address this gap, in this paper we study unsupervised video continual learning, which poses more challenges than the image domain concerning processing and memory requirements. We introduce a general benchmark experimental protocol for unsupervised continual learning of video, by learning a mixture of video data categories during each task. Our proposed approach relies on the Kernel Density Estimation (KDE) of deep embedded video features extracted by unsupervised video transformer networks, enabling the unsupervised structuring of learned information. We introduce a novelty detection criterion that leverages the network confidence when creating new clusters from incoming data. The novelty detection criterion dynamically enables the expansion of a set of memory clusters, derived form the KDE data representation, aiming to capture new knowledge when learning a succession of tasks. We leverage the use of transfer learning from the previous tasks as an initial state for the knowledge transfer to the current learning task. We found that the proposed methodology substantially enhances its performance when successively learning many tasks. We perform in-depth evaluations on three standard video action recognition datasets, including UCF101, HMDB51, and Something-to-Something V2, without using any labels or class boundaries.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
238,https://openreview.net/forum?id=1rtTRQNbfG,Capture and Reconstruct 3D Clothed Human from Images,"The reconstruction of multi-layer 3D garments typically requires expensive multi-view capture setups and specialized 3D editing efforts. To create like-life clothed human avatars with ease, we propose a method for reconstructing multi-layer clothed humans using a new data modality, Image Layers, which captures a subject wearing different layers of clothing with a single RGB camera. To achieve physically plausible 3D clothing reconstruction from multiple 2D layered images, a unified 3D coordinate is necessary to model these garments in a layered manner. We reconstruct and register garment meshes on top of a shared human body in the canonical pose. Complementing this layered representation, we introduce a collision-aware optimization process to address inter-penetrations and represent the garments to implicit neural fields for refinement. Our method is template-free and category-agnostic, which allows for the reconstruction of 3D garments in diverse clothing styles. Through our experiments, we show that our method efficiently reconstructs simulation-compatible 3D clothing and achieves competitive performance compared to methods trained with category-specific data.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
239,https://openreview.net/forum?id=t52W20USpH,REACT: Real-time Efficiency and Accuracy Compromise for Tradeoffs in Scene Graph Generation,"Scene Graph Generation (SGG) is a task that encodes visual relationships between objects in images as graph structures. SGG shows significant promise as a foundational component for downstream tasks, such as reasoning for embodied agents. To enable real-time applications, SGG must address the trade-off between performance and inference speed. However, current methods tend to focus on one of the following: (1) improving relation prediction accuracy, (2) enhancing object detection accuracy, or (3) reducing latency, without aiming to balance all three objectives simultaneously.
To address this limitation, we propose a novel architecture, inference method, and relation prediction model. Our proposed solution, the REACT model, achieves the highest inference speed among existing SGG models, improving object detection accuracy without sacrificing relation prediction performance. Compared to state-of-the-art approaches, REACT is 2.7 times faster and improves object detection accuracy by 58\%. Furthermore, our proposal significantly reduces model size, with an average of 5.5x fewer parameters.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Edmond S. L. Ho
257,https://openreview.net/forum?id=atWPmfdE8a,S$^2$V2V: Training-Free Video-to-Video with Sparse Points and Motion Guidance,"Most existing training-free, motion-controllable video-to-video generation methods typically operate on attention maps or noise estimation for motion guidance. Despite development, they often struggle to maintain consistent temporal coherence across frames and to accurately follow the guided motion. In this paper, we propose S$^2$V2V, a novel training-free paradigm with double sparse guidance, to address the challenge of generating temporally consistent videos with motion guidance. Specifically, we introduce two types of explicit guidance from the reference videos, i.e., i) sparse point guidance, extracted from inter-frame motion correlation patterns at selected sparse key points, and ii) sparsity-based motion guidance, obtained by refining motion correlation patterns in a local region around those sparse points. To further improve temporal consistency, we incorporate a sparse motion consistency loss during the denoising process, encouraging the generated motion representations to align with the reference guidance in the sparse regions. The gradient of this loss in latent space is then used to steer the generation toward precise motion control. Extensive experiments demonstrate that S$^2$V2V sets a new standard for efficient, temporally coherent video generation in various scenarios, particularly in local object motion and box-trajectory-guided motion.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
259,https://openreview.net/forum?id=lZM5akOneR,Revisiting Entropy Minimization for Long-Sequence Continual Test-Time Adaptation,"Deep neural networks (DNNs) have revolutionized a wide range of tasks, yet they remain vulnerable to distributional shifts between training and test data, which is a common occurrence in real-world scenarios. These shifts often unfold continually over time, posing significant challenges for test-time adaptation. A critical issue we identify in this setting is \textbf{noisy label memorization}, where the model gradually overfits to its own erroneous predictions (pseudo-labels), leading to significant performance degradation. This problem is particularly severe in \textbf{long-sequence} continual test-time adaptation, where the model must adapt continuously in non-stationary environments. To address this, we propose a novel methodology that strategically constrains weight updates to prevent noisy labels from degrading the model, resulting in significant performance improvements, particularly in long-sequence continual test-time adaptation (CoTTA) scenarios. Extensive experiments on benchmarks such as CIFAR10-C, CIFAR100-C, ImageNet-C, CCC, and DomainNet demonstrate that our method not only outperforms state-of-the-art CoTTA techniques but also ensures sustained model reliability and robustness over time. Furthermore, our method exhibits greater stability across varying sample sizes and batch sizes, with reduced sensitivity to model selection—all achieved without the need for source data calibration.",5,3,~Sungho_Shin3|~Brejesh_Lall1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
261,https://openreview.net/forum?id=f0IpIa19mJ,ADIR: Adaptive Diffusion for Image Reconstruction,"In recent years, denoising diffusion models have demonstrated outstanding image generation performance. The information on natural images captured by these models is useful for many image reconstruction applications, where the task is to restore a clean image from its degraded observation. In this work, we propose a conditional sampling scheme that exploits the prior learned by diffusion models while retaining agreement with the measurements. We then combine it with a novel approach for adapting pre-trained diffusion denoising networks to their input. We perform the adaptation using images that are ""nearest neighbors"" to the degraded image, retrieved from a diverse dataset using an off-the-shelf visual-language model. To evaluate our method, we test it on two state-of-the-art publicly available diffusion models, Stable Diffusion and Guided Diffusion. We show that our proposed Adaptive Diffusion for Image Reconstruction (ADIR) approach achieves significant improvement in image reconstruction tasks. Our code will be available online upon publication.",5,4,~Jianjia_Wang1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
265,https://openreview.net/forum?id=gB7BFPYA9Q,Enhancing Visual Tracking by Leveraging High-frequency Information within Event Signals,"Traditional object trackers struggle in degraded scenarios, lacking sufficient appearance details of moving targets for precise tracking. Recent trackers have integrated high-frequency event signals to assist tracking. However, they neglect the high-temporal-resolution motion information inherent in events, limiting their performance especially in occlusion and background clutter. To address these challenges, we propose HFTrack, a novel tracker designed to fully leverage the spatio-temporal high-frequency information within event signals, thereby enhancing the tracking performance. Specifically, we introduce a frequency-based feature enhancement module, which enriches the frame feature with high-frequency components from events in frequency space, capturing detailed appearance information of moving targets. Additionally, we propose a spatio-temporal information decoder with an auto-regressive temporal query, integrating both historical motion cues from events and enhanced spatial features for robust target localization. Experimental results demonstrate that our HFTrack significantly outperforms existing trackers, showcasing its strong ability to track the target under challenging conditions.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
270,https://openreview.net/forum?id=EZDLjfq3YT,FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation,"Recognizing and differentiating among both familiar and unfamiliar faces is a critical capability for face recognition systems and a key step toward artificial general intelligence (AGI). Motivated by this ability, this paper introduces $\textit{generalized face discovery}$ (GFD), a novel open-world face recognition task that unifies traditional face identification with $\textit{generalized category discovery}$ (GCD). GFD requires recognizing both labeled and unlabeled known identities (IDs) while simultaneously discovering new, previously unseen IDs. Unlike typical GCD settings, GFD poses unique challenges due to the high cardinality and fine-grained nature of face IDs, rendering existing GCD approaches ineffective. To tackle this problem, we propose $\textit{FaceGCD}$, a method that dynamically constructs instance-specific feature extractors using lightweight, layer-wise prefixes. These prefixes are generated on the fly by a $\textit{HyperNetwork}$, which adaptively outputs a set of prefix generators conditioned on each input image. This dynamic design enables FaceGCD to capture subtle identity-specific cues without relying on high-capacity static models. Extensive experiments demonstrate that FaceGCD significantly outperforms existing GCD methods and a strong face recognition baseline, $\textit{ArcFace}$, achieving state-of-the-art results on the GFD task and advancing toward open-world face recognition.",5,3,~João_C._Neves1|~Deen_Dayal_Mohan2,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
272,https://openreview.net/forum?id=wvVQT7T8wm,B-RIGHT: Benchmark Re-evaluation for Integrity in Generalized Human-Object Interaction Testing,"Human-object interaction (HOI) detection aims to understand complex relationships between humans and objects, but existing benchmarks like HICO-DET suffer from severe class imbalance and inconsistent train-test splits, compromising reliable evaluations. To address these issues, we introduce **B-RIGHT** (**B**enchmark **R**e-evaluation for **I**ntegrity in **G**eneralized **H**uman-Object Interaction **T**esting), a systematically balanced dataset constructed via a novel balancing algorithm and an automated image generation-and-filtering pipeline. By ensuring uniform representation across HOI classes, B-RIGHT significantly reduces performance variance. Our re-evaluation of state-of-the-art methods reveals substantial shifts in model rankings compared to HICO-DET, highlighting previously hidden biases. These results underscore the critical importance of balanced benchmarks for fair and insightful model comparisons. The dataset is
publicly available at [B-RIGHT](https://drive.google.com/drive/folders/1GFZuVfFdkY1z4CIp1PQhiuVtx37ZmedB?usp=sharing]).",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
276,https://openreview.net/forum?id=aK6FkHgLGz,Learning Event-guided Exposure-agnostic Video Frame Interpolation via Adaptive Feature Blending,"Exposure-agnostic video frame interpolation (VFI) is a challenging task that aims to recover sharp, high-frame-rate videos from blurry, low-frame-rate inputs captured under unknown and dynamic exposure conditions. Event cameras are sensors with high temporal resolution, making them especially advantageous for this task. However, existing event-guided methods struggle to produce satisfactory results on severely low-frame-rate blurry videos due to the lack of temporal constraints. In this paper, we introduce a novel event-guided framework for exposure-agnostic VFI, addressing this limitation through two key components: a Target-adaptive Event Sampling (TES) and a Target-adaptive Importance Mapping (TIM). Specifically, TES samples events around the target timestamp and the unknown exposure time to better align them with the corresponding blurry frames. TIM then generates an importance map that considers the temporal proximity and spatial relevance of consecutive features to the target. Guided by this map, our framework adaptively blends consecutive features, allowing temporally aligned features to serve as the primary cues while spatially relevant ones offer complementary support. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of our approach in exposure-agnostic VFI scenarios. Codes will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
277,https://openreview.net/forum?id=rGxKKfsmI4,CFFlow: An Optical Flow Estimation Hinging on Cross-Frequency Attention,"Due to issues in training datasets and motion estimation in optical flow, existing optical flow models overly emphasize surface-level information features, leading to inaccurate optical flow estimations. Through theoretical analysis, we reveal that multi-frame approaches require more attention to interconnections among diverse information than two-frame methods. Consequently, we propose \textbf{CFFlow}, which integrates cross-frequency information into optical flow estimation networks. To address challenges posed by large displacements and small object motion in real-world scenarios, we introduce two specialized modules: Large Displacement Attention (LDA) and Small Object Displacement Attention (SODA), designed to handle distinct motion patterns. CFFlow effectively resolves small target mismatches and large displacement challenges, achieving superior optical flow estimation accuracy. On Sintel and KITTI benchmarks, our CFFlow attains an Average Endpoint Error (AEPE) of \textbf{0.99} (clean pass) and \textbf{1.65} (final pass), with an F1-all error of \textbf{4.21\%}, ranking first among all three-frame and two-frame methods.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
281,https://openreview.net/forum?id=Mr2p4sDiAY,CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections,"In the era of foundation models, CLIP has emerged as a powerful tool for aligning text and visual modalities into a common embedding space. However, the alignment objective used to train CLIP often results in subpar visual features for fine-grained tasks. In contrast, models pretrained in a self-supervised manner, such as DINO, excel at extracting rich visual features due to their specialized training paradigm. Yet, these self-supervised learning (SSL) models require an additional supervised linear probing step, which relies on fully labeled data—often expensive and difficult to obtain at scale. In this paper, we propose a label-free  prompt-tuning method that leverages the rich visual features extracted using DINO SSL model and the broad contextual knowledge of LLMs to enhance CLIP's image classification performance using purely unlabeled images. Our approach unfolds in three key steps: (i) We generate robust textual feature embeddings that more accurately represent object classes by leveraging class-specific descriptions from large language models (LLMs). (ii) The textual embeddings are then used to produce pseudo-labels to train an alignment module that integrates the complementary strengths of LLM description-based textual embeddings and visual features extracted from DINO. (iii) Finally, we prompt-tune CLIP's vision encoder using the trained alignment module. This three-step process allows us to harness the best of visual and textual foundation models, resulting in a powerful and efficient approach that surpasses state-of-the-art (SOTA) label-free classification methods. Notably, our framework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6% over the state-of-the-art LaFTer across 11 diverse image classification datasets. Our code and models will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
283,https://openreview.net/forum?id=DGmMF75P71,AegisRF: Adversarial Perturbations Guided with Sensitivity for Protecting Intellectual Property of Neural Radiance Fields,"As Neural Radiance Fields (NeRFs) have emerged as a powerful tool for 3D scene representation and novel view synthesis, protecting their intellectual property (IP) from unauthorized use is becoming increasingly crucial. In this work, we aim to protect the IP of NeRFs by injecting adversarial perturbations that disrupt their unauthorized applications. However, perturbing the 3D geometry of NeRFs can easily deform the underlying scene structure and thus substantially degrade the rendering quality, which has led existing attempts to avoid geometric perturbations or restrict them to explicit spaces like meshes. To overcome this limitation, we introduce a learnable sensitivity to quantify the spatially varying impact of geometric perturbations on rendering quality. Building upon this, we propose AegisRF, a novel framework that consists of a Perturbation Field, which injects adversarial perturbations into the pre-rendering outputs (color and volume density) of NeRF models to fool an unauthorized downstream target model, and a Sensitivity Field, which learns the sensitivity to adaptively constrain geometric perturbations, preserving rendering quality while disrupting unauthorized use. Our experimental evaluations demonstrate the generalized applicability of AegisRF across diverse downstream tasks and modalities, including multi-view image classification and voxel-based 3D localization, while maintaining high visual fidelity. Codes will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
284,https://openreview.net/forum?id=89FKmljvc8,Multimodal Feature Collaboration and Fusion for Fine-Grained Action Recognition,"Fine-grained action recognition faces challenges of subtle inter-class differences and significant intra-class variations in complex real-world scenarios. Existing methods are often limited by insufficient feature representation capabilities of single modalities or redundant computational costs. In this paper, we propose a Multimodal Feature Collaboration and Fusion framework to achieve efficient recognition through cross-modal guided spatial and temporal modeling. For the RGB modality, skeleton joint coordinates collaboratively localize and crop action regions to suppress background noise, and design a lightweight Spatio-Temporal Attention and Aggregation (STAA) that combines short-term motion excitation and long-term temporal modeling to enhance feature extraction efficiency and accuracy. For the skeleton modality, we propose a Class-aware contrastive learning that dynamically selects high-similarity samples to optimize feature discriminative boundaries. Finally, a score‑weighted fusion strategy integrates the optimized representations from both modalities. Experiments on the FineGym and NTU RGB+D benchmarks demonstrate impressive performance improvements over state‑of‑the‑art methods.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
288,https://openreview.net/forum?id=4FKIWU1d2S,Benchmarking Microsaccade Recognition with Event Cameras: A Novel Dataset and Evaluation,"Microsaccades are small, involuntary eye movements essential for visual perception and neural processing. Traditional microsaccade research often relies on eye trackers and frame-based video analysis. While eye trackers offer high precision, they can be expensive and have limitations in scalability and temporal resolution compared to alternative methods. In contrast, event-based sensing offers a more efficient and precise alternative, as it captures high-resolution spatial and temporal information with minimal latency. This work introduces a pioneering event-based microsaccade dataset, designed to support efforts in studying small eye movement data within cognitive computing. Using Blender, we generate high-fidelity renderings of eye movement scenarios and simulate microsaccades with angular displacements ranging from 0.5° to 2.0°, divided into seven distinct classes. These sequences are simulated into event streams using v2e, preserving the temporal dynamics of real microsaccades. The event streams, with durations as small as 0.25 milliseconds and as large as 2.25 milliseconds, have such high temporal resolution, which prompted us to investigate whether SNNs could effectively detect and classify these movements. We evaluate the proposed dataset using Spiking-VGG11, Spiking-VGG13, and Spiking-VGG16, and further introduce Spiking-VGG16Flow—an optical flow–enhanced variant—implemented with SpikingJelly. Across experiments, these models achieve an average accuracy of approximately 90\%, effectively classifying microsaccades based on angular displacement, irrespective of event count or duration. These results highlight the suitability of SNNs for fine motion classification and establish a benchmark for future research in event-based vision. To facilitate further work in neuromorphic computing and visual neuroscience, both the dataset and trained models will be released publicly.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
289,https://openreview.net/forum?id=qqRB0F4gQ5,CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings,"Accurate 6D pose estimation of complex objects in 3D environments is essential for effective robotic manipulation. Yet, existing benchmarks fall short in evaluating 6D pose estimation methods under realistic industrial conditions as most datasets focus on household objects in domestic settings, while the few available industrial datasets are limited to artificial setups with objects placed on tables. To bridge this gap, we introduce CHIP, the first dataset designed for 6D pose estimation of chairs manipulated by a robotic arm in a real-world industrial environment. CHIP includes seven distinct chairs captured using three different RGBD sensing technologies and presents unique challenges, such as distractor objects with fine-grained differences and severe occlusions caused by the robotic arm and human operators. CHIP comprises 77,811 RGBD images annotated with ground-truth 6D poses automatically derived from the robot’s kinematics, averaging 11,115 annotations per chair. We benchmark CHIP using three zero-shot 6D pose estimation methods, assessing performance across different sensor types, localization priors, and occlusion levels. Results show substantial room for improvement, highlighting the unique challenges posed by the dataset. CHIP will be publicly released.",5,4,~Arpit_Garg1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
291,https://openreview.net/forum?id=OwJCj9mM50,Detection Transformers Under the Knife: A Neuroscience-Inspired Approach to Ablations,"In recent years, Explainable artificial intelligence (XAI) has gained traction as an approach to enhancing model interpretability and transparency, particularly in complex models such as detection transformers. Despite rapid advancements, a substantial research gap remains in understanding the distinct roles of internal components -  knowledge that is essential for improving transparency and efficiency. Inspired by neuroscientific ablation studies, which investigate the functions of brain regions through selective impairment, we systematically analyze the impact of ablating key components in three state-of-the-art detection transformer models: detection transformer (DETR), deformable detection transformer (DDETR), and detection transformer with denoising anchor boxes (DINO). The ablations target query embeddings, encoder and decoder multi-head self-attention (MHSA) as well as decoder multi-head cross-attention (MHCA) layers. We evaluate the consequences of these ablations on the performance metrics generalized intersection over union (gIoU) and F1-score, quantifying effects on both the classification and regression sub-tasks on the COCO dataset. To facilitate reproducibility and future research, we publicly release the DeepDissect library.

Our findings reveal model-specific resilience patterns: while DETR is particularly sensitive to ablations in encoder MHSA and decoder MHCA, DDETR's multi-scale deformable attention enhances robustness, and DINO exhibits the greatest resilience due to its look-forward twice update rule, which helps distributing knowledge across blocks. These insights also expose structural redundancies, particularly in DDETR's and DINO's decoder \ac{mhca} layers, highlighting opportunities for model simplification without sacrificing performance.
    This study advances XAI for detection transformers by clarifying the contributions of internal components to model performance, offering insights to optimize and improve transparency and efficiency in critical applications.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
292,https://openreview.net/forum?id=7C0KRtUWtF,SVAC: Scaling Is All You Need For Referring Video Object Segmentation,"Referring Video Object Segmentation (RVOS) aims to segment target objects in video sequences based on natural language descriptions. While recent advances in Multi-modal Large Language Models (MLLMs) have improved RVOS performance through enhanced text-video understanding, several challenges remain, including insufficient exploitation of MLLMs’ prior knowledge, prohibitive computational and memory costs for long-duration videos, and inadequate handling of complex temporal dynamics. In this work, we propose SVAC, a unified model that improves RVOS by scaling up input frames and segmentation tokens to enhance video-language interaction and segmentation precision. To address the resulting computational challenges, SVAC incorporates the Anchor-Based Spatio-Temporal Compression (ASTC) module to compress visual tokens while preserving essential spatio-temporal structure. Moreover, the Clip-Specific Allocation (CSA) strategy is introduced to better handle dynamic object behaviors across video clips. Experimental results demonstrate that SVAC achieves state-of-the-art performance on multiple RVOS benchmarks with competitive efficiency.",6,3,~Jie_Mei3|~Ruicong_Liu1|~Thanos_Delatolas1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
294,https://openreview.net/forum?id=RiyK7jt1ZV,Task Progressive Curriculum Learning for Robust Visual Question Answering,"Visual Question Answering (VQA) systems are notoriously brittle under distribution
shifts and data scarcity. While previous solutions—such as ensemble methods and data
augmentation—can improve performance in isolation, they fail to generalise well across
in-distribution (IID), out-of-distribution (OOD), and low-data settings simultaneously.
We argue that this limitation stems from the suboptimal training strategies employed.
Specifically, treating all training samples uniformly—without accounting for question
difficulty or semantic structure— leaves the models vulnerable to dataset biases. Thus,
they struggle to generalise beyond the training distribution.
To address this issue, we introduce Task-Progressive Curriculum Learning (TPCL)—
a simple, model-agnostic framework that progressively trains VQA models using a curriculum built by jointly considering question type and difficulty. Specifically, TPCL first
groups questions based on their semantic type (e.g., yes/no, counting) and then orders
them using a novel Optimal Transport-based difficulty measure. Without relying on data
augmentation or explicit debiasing, TPCL improves generalisation across IID, OOD, and
low-data regimes and achieves state-of-the-art performance on VQA-CP v2, VQA-CP
v1, and VQA v2. It outperforms the most competitive robust VQA baselines by over
5% and 7% on VQA-CP v2 and v1, respectively, and boosts backbone performance
by up to 28.5%. The source code is attached as supplementary to the submission for
reproducibility.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
310,https://openreview.net/forum?id=lLoFDZZLhm,PME3D: An Adaptive and Efficient Multi-modal Feature Extraction Plug-in for 3D Object Detection,"3D object detection is fundamental to autonomous driving systems, where efficiently fusing multi-modal sensor data remains a critical challenge. While current approaches predominantly focus on improving detection accuracy through Bird's Eye View representations, they often neglect computational efficiency - a crucial factor for real-world deployment on resource-constrained automotive platforms. To bridge this gap, we propose a lightweight plug-in module that enhances feature fusion efficiency through two key mechanisms: (1) dimensionality inversion of feature extraction outputs, and (2) dynamic selection of camera features for optimal fusion with point cloud data. Our experiments on nuScenes demonstrate that this approach maintains competitive detection performance while significantly reducing computational overhead, offering a practical solution for real-time autonomous driving applications. Code and models will be released to support further research in efficient multi-modal 3D perception.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
313,https://openreview.net/forum?id=eHW2EyQmro,OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware,"Medical applications demand segmentation of large inputs, like prostate MRIs, pathology slices, or videos of surgery. These inputs should ideally be inferred at once to provide the model with proper spatial or temporal context. When segmenting large inputs, the VRAM consumption of the GPU becomes the bottleneck. Architectures like UNets or Vision Transformers scale very poorly in VRAM consumption, resulting in patch- or frame-wise approaches that compromise global consistency and inference speed. The lightweight Neural Cellular Automaton (NCA) is a bio-inspired model that is by construction size-invariant. However, due to its local-only communication rules, it lacks global knowledge. We propose OctreeNCA by generalizing the neighborhood definition using an octree data structure. Our generalized neighborhood definition enables the efficient traversal of global knowledge. Since deep learning frameworks are mainly developed for large multi-layer networks, their implementation does not fully leverage the advantages of NCAs. We implement an NCA inference function in CUDA that further reduces VRAM demands and increases inference speed. Our OctreeNCA segments high-resolution images and videos quickly while occupying 90% less VRAM than a UNet during evaluation. This allows us to segment 184 Megapixel pathology slices or 1-minute surgical videos at once.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
320,https://openreview.net/forum?id=zAeLXfMRLk,Extreme Model Compression with Structured Sparsity at Low Precision,"Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision.
Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE (Structured sparsity at Low PrEcision), a unified framework that is the first to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naïvely combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\sim20\times$ model size reduction while retaining $\sim$99\% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.",5,5,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
328,https://openreview.net/forum?id=oa1IJsB9SU,DepthHMR: Leveraging Depth Around Humans for Multi-Human Mesh Generation,"We present DepthHMR, a novel one-stage framework for multi-human mesh reconstruction from a single RGB image. While recent DETR-style approaches have shown promising results, they primarily rely on image features that lack explicit 3D reasoning, leading to depth ambiguities in scenes with occlusion, scale variation, or distant individuals. To address this, our \textsc{DepthHMR} integrates metric depth cues and explicit 3D positional reasoning into a unified one-stage DETR framework. Central to our approach is a Depth Around Human (DAH) module, which isolates human-centric depth cues from monocular depth maps. Unlike general scene depth estimation, DAH focuses specifically on human subjects, improving depth priors for distant and occluded humans. To enhance depth representation, we adopt a non-uniform depth discretization scheme, allocating denser bins in near-field regions and sparser bins at greater distances. This design enables more precise depth reasoning in human-centric zones. Our depth prediction branch, supervised using DAH-generated pseudo ground-truth, enables the shared backbone to simultaneously learn geometry-aware and appearance-aware features. Building on these depth-informed representations, we propose depth-guided 3D query initialization followed by a depth-aware cross-attention decoder that refines SMPL mesh related attributes for each query representing person in the scene. Our model achieves state-of-the-art mesh reconstruction performance, reducing MVE by 2.8 mm on AGORA and 4.6 mm on 3DPW, while only requiring substantially lower resolution inputs, enhancing both accuracy and efficiency.",6,6,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
330,https://openreview.net/forum?id=wEw1jBzhia,"FaceCrafter: Identity-Conditional Diffusion with Disentangled Control over Facial Pose, Expression, and Emotion","Human facial images encode a rich spectrum of information, encompassing both stable identity-related traits and mutable attributes such as pose, expression, and emotion.
While recent advances in image generation have enabled high-quality identity-conditional face synthesis, precise control over non-identity attributes remains challenging, and disentangling identity from these mutable factors is particularly difficult.
To address these limitations, we propose a novel identity-conditional diffusion model that introduces two lightweight control modules designed to independently manipulate facial pose, expression, and emotion without compromising identity preservation.
These modules are embedded within the cross-attention layers of the base diffusion model, enabling precise attribute control with minimal parameter overhead.
Furthermore, our tailored training strategy, which leverages cross-attention between the identity feature and each non-identity control feature, encourages identity features to remain orthogonal to control signals, enhancing controllability and diversity.
Quantitative and qualitative evaluations, along with perceptual user studies, demonstrate that our method surpasses existing approaches in terms of control accuracy over pose, expression, and emotion, while also improving generative diversity under identity-only conditioning.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
333,https://openreview.net/forum?id=2S7TkcmtWL,Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking,"Multi-object tracking (MOT) methods often rely on Intersection-over-Union (IoU) for association. However, this becomes unreliable when objects are similar or occluded. Also, computing IoU for segmentation masks is computationally expensive. In this work, we use segmentation masks to capture object shapes, but we do not compute segmentation IoU. Instead, we fuse depth and mask features and pass them through a compact encoder trained self-supervised. This encoder produces stable object representations, which we use as an additional similarity cue alongside bounding box IoU and re-identification features for matching. We obtain depth maps from a zero-shot depth estimator and object masks from a promptable visual segmentation model to obtain fine-grained spatial cues. Our MOT method is the first to use the self-supervised encoder to refine segmentation masks without computing masks IoU. MOT can be divided into joint detection-ReID (JDR) and tracking-by-detection (TBD) models. The latter are computationally more efficient. Experiments of our TBD method on challenging benchmarks with non-linear motion, occlusion, and crowded scenes, such as SportsMOT and DanceTrack, show that our method outperforms the TBD state-of-the-art on most metrics, while achieving competitive performance on simpler benchmarks with linear motion, such as MOT17.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
334,https://openreview.net/forum?id=eZGWcr7Mkx,DAOVI: Distortion-Aware Omnidirectional Video Inpainting,"Omnidirectional videos that capture the entire surroundings are employed in a variety of fields such as VR applications and remote sensing.
However, their wide field of view makes it easy for unwanted objects to appear in the videos.
This problem can be addressed by video inpainting, which enables the natural removal of such objects while preserving both spatial and temporal consistency.
Nevertheless, most existing methods assume processing ordinary videos with a narrow field of view and do not tackle the distortion in equirectangular projection of omnidirectional videos.
To address this issue, this paper proposes a novel deep learning model for omnidirectional video inpainting, called Distortion-Aware Omnidirectional Video Inpainting (DAOVI).
DAOVI introduces a module that evaluates temporal motion information in the image space considering geodesic distance, as well as a depth-aware feature propagation module in the feature space that is designed to address the geometric distortions inherent to omnidirectional videos.
The experimental results demonstrate that our proposed method outperforms existing methods both qualitatively and quantitatively.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
338,https://openreview.net/forum?id=C1MeVeKt4f,GLip: A Global-Local Integrated Progressive Framework for Robust Visual Speech Recognition,"Visual speech recognition (VSR), also known as lip reading, is the task of recognizing speech from silent video. Despite significant advancements in VSR over recent decades, most existing methods pay limited attention to real-world visual challenges such as illumination variations, occlusions, blurring, and pose changes. To address these challenges, we propose GLip, a Global-Local Integrated Progressive framework designed for robust VSR. GLip is built upon two key insights: (i) learning an initial \textit{coarse} alignment between visual features across varying conditions and corresponding speech content facilitates the subsequent learning of \textit{precise} visual-to-speech mappings in challenging environments; (ii) under adverse conditions, certain local regions (e.g., non-occluded areas) often exhibit more discriminative cues for lip reading than global features. To this end, GLip introduces a dual-path feature extraction architecture that integrates both global and local features within a two-stage progressive learning framework. In the first stage, the model learns to align both global and local visual features with corresponding acoustic speech units using easily accessible audio-visual data, establishing a coarse yet semantically robust foundation. In the second stage, we introduce a Contextual Enhancement Module (CEM) to dynamically integrate local features with relevant global context across both spatial and temporal dimensions, refining the coarse representations into precise visual-speech mappings. Our framework uniquely exploits discriminative local regions through a progressive learning strategy, demonstrating enhanced robustness against various visual challenges and consistently outperforming existing methods on the LRS2 and LRS3 benchmarks.",5,3,~Saurabh_Kataria1|~Alex_Mackin1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
339,https://openreview.net/forum?id=0dIWMMTTBh,Events Meet Dynamic Mode Decomposition: Capturing the Spatiotemporal Dynamics of Moving Objects,"This paper introduces a novel mechanism for Moving Object Segmentation (MOS) that utilizes Dynamic Mode Decomposition (DMD) applied to event streams from event cameras to reconstruct missing dynamic information between RGB frames. First, we develop an Event-Driven Dynamic Mode Decomposition (ED-DMD) framework capable of capturing intrinsic motion dynamics between consecutive frames. By transforming high-resolution event streams into structured event slices, we convert event data into a compact Event-Driven Descriptor (ED-Dr), facilitating precise recovery of high-speed object movements. We then introduce a Motion-Aware Segmentation Network (MAS-Net) to seamlessly integrate these dynamics with spatial information, comprising a Dynamic Predict Block (DPB) and a Spatial Latent Block (SLB). During training, these blocks collaboratively process the RGB frames and ED-Dr to learn static appearance features and motion dynamics. During inference, the model operates in an event-free mode, requiring only RGB inputs to achieve event-aware performance by leveraging the learned motion priors. We also conduct a comprehensive analysis of the impact of different ED-Dr features, block configurations, and backbone architectures on segmentation accuracy using the DSEC-MOS dataset. Comparative evaluations with state-of-the-art methods demonstrate that our approach consistently surpasses existing baselines in these evaluation metrics, validating the robustness and effectiveness of our framework in capturing spatiotemporal dynamics in complex real-world scenarios.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
342,https://openreview.net/forum?id=6UMcb0iOjF,Flatness-aware Curriculum Learning via Adversarial Difficulty,"Neural networks trained by empirical risk minimization often suffer from overfitting, especially to specific samples or domains, which leads to poor generalization. Curriculum Learning (CL) addresses this issue by selecting training samples based on the difficulty. From the optimization perspective, methods such as Sharpness-Aware Minimization (SAM) improve robustness and generalization by seeking flat minima. However, combining CL with SAM is not straightforward. In flat regions, both the loss values and the gradient norms tend to become uniformly small, which makes it difficult to evaluate sample difficulty and design an effective curriculum. To overcome this problem, we propose the Adversarial Difficulty Measure (ADM), which quantifies adversarial vulnerability by leveraging the robustness that models exhibit in flat minima. Our method incorporates ADM into CL-based training with SAM, where the normalized loss difference between original and adversarial examples is used to adaptively assess sample difficulty. We evaluated our approach on image classification tasks, fine-grained recognition, and domain generalization. The results show that our method preserves the strengths of both CL and SAM while outperforming existing curriculum-based and flatness-aware training strategies.",7,5,~Aman_Gupta1|~Kangkang_Lu1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
346,https://openreview.net/forum?id=K64DoxOOD3,Coarse Attribute Prediction with Task Agnostic Distillation for Real World Clothes Changing ReID,"This work focuses on Clothes Changing Re-IDentification (CC-ReID) for the real world. Existing works perform well with high-quality (HQ) images, but struggle with low-quality (LQ) where we can have artifacts like pixelation, out-of-focus blur, and motion blur. These artifacts introduce noise to not only external biometric attributes (e.g. pose, body shape, etc.) but also corrupt the model’s internal feature representation. Models usually cluster LQ image features together, making it difficult to distinguish between them, leading to incorrect matches. We propose a novel framework Robustness against Low-Quality (RLQ) to improve CC-ReID model on real-world data. RLQ relies on Coarse Attributes Prediction (CAP) and Task Agnostic Distillation (TAD) operating in alternate steps in a novel training mechanism. CAP enriches the model with external fine-grained attributes via coarse predictions, thereby reducing the effect of noisy input. On the other hand, TAD enhances the model’s internal feature representation by bridging the gap between HQ and LQ features, via an external dataset through task-agnostic self-supervision and distillation. Our RLQ is among the few works to successfully learn from external attributes on very low-quality on real-world datasets like LaST, and DeepChange. The code is public on https://github.com/ppriyank/RLQ-CGAL-UBD.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
348,https://openreview.net/forum?id=h7mHnOLMPa,PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing,"Localized subject-driven image editing aims to seamlessly integrate user-specified objects into target scenes. As generative models continue to scale, training becomes increasingly costly in terms of memory and computation, highlighting the need for training-free and scalable editing frameworks.
To this end, we propose PosBridge—an efficient and flexible framework for inserting custom objects. A key component of our method is positional embedding transplant, which guides the diffusion model to faithfully replicate the structural characteristics of reference objects.
Meanwhile, we introduce the Subject Fusion Image Pack (SFIP), which concatenates the reference and background images as input to the FLUX.1-Fill model. During progressive denoising, positional embedding transplant is applied to guide the noise distribution in the target region toward that of the reference object. In this way, SFIP effectively directs the FLUX.1-Fill model to synthesize the target image content at the desired location.
Extensive experiments demonstrate that PosBridge outperforms mainstream baselines in structural consistency, appearance fidelity, and computational efficiency, showcasing its practical value and potential for broad adoption.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
354,https://openreview.net/forum?id=AYVNVYIM5b,Dual-Branch Network via Multiple Illumination-Aware Representation Learning for Steel Surface Defect Classification,"On highly reflective metal surfaces, strong lighting-induced reflections can obscure or distort defect regions, making accurate defect detection challenging. To mitigate this, various classification methods using multi-light source images captured under diverse lighting conditions have been proposed. However, such approaches struggle with two critical issues: (i) failing to clearly separate defect-like reflections from meaningful defect cues, leading to visually ambiguous representations, and (ii) incapable of capturing lighting-dependent variations in defect appearance. In this work, we propose a Dual-branch network that disentangles illumination-invariant and lighting-dependent representations via two dedicated encoders: a view-consistent encoder and a view-specific encoder. The view-consistent encoder extracts defect features that are robust to misleading reflections and invariant to changes in illumination, whereas the view-specific encoder focuses on capturing fine-grained, lighting-dependent visual cues around defect regions. This design enables the model to learn simultaneously generalized and view-adaptive representations of defects. Experimental results demonstrate that our method significantly outperforms existing multi-light source approaches in terms of average classification accuracy.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
357,https://openreview.net/forum?id=jWQWQuzduK,What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos,"Humans usually show exceptional generalisation and discovery ability in the open world, when being shown uncommonly new concepts. Whereas most existing studies in the literature focus on common typical data from closed sets, open-world novel discovery is under-explored in videos.
In this paper, we are interested in asking: \textit{what if atypical unusual videos are exposed in the learning process?}
To this end, we collect a new video dataset consisting of various types of unusual atypical data (e.g. sci-fi, animation, etc.). To study how such atypical data may benefit open-world learning, we feed them into the model training process for representation learning.
Focusing on three key tasks in open-world learning: out-of-distribution (OOD) detection, novel category discovery (NCD), and zero-shot action recognition (ZSAR), we found that even straightforward learning approaches with atypical data consistently improve performance across various settings. Furthermore, we found that increasing the categorical diversity of the atypical samples further boosts OOD detection performance. Additionally, in the NCD task, using a smaller yet more semantically diverse set of atypical samples leads to better performance compared to using a larger but more typical dataset. In the ZSAR setting, the semantic diversity of atypical videos helps the model generalise better to unseen action classes.
These observations in our extensive experimental evaluations reveal the benefits of atypical videos for visual representation learning in the open world, together with the newly proposed dataset, encouraging further studies in this direction.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
358,https://openreview.net/forum?id=oi9E1c6VHp,Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones,"Although appearance-based point-of-gaze (PoG) estimation has improved,  the estimators still struggle to generalize across individuals due to personal differences. Therefore, person-specific calibration is required for accurate PoG estimation. However, calibrated PoG estimators are often sensitive to head pose variations. To address this, we investigate the key factors influencing calibrated estimators and explore pose-robust calibration strategies. Specifically,  we first construct a benchmark, MobilePoG, which includes facial images from 32 individuals focusing on designated points under either fixed or continuously changing head poses. Using this benchmark, we systematically analyze how the diversity of calibration points and head poses influences estimation accuracy. Our experiments show that introducing a wider range of head poses during calibration improves the estimator’s ability to handle pose variation. Building on this insight, we propose a dynamic calibration strategy in which users fixate on calibration points while moving their phones. This strategy naturally introduces head pose variation during a user-friendly and efficient calibration process, ultimately producing a better calibrated PoG estimator that is less sensitive to head pose variations than those using conventional calibration strategies. Our benchmark and codes will be available.",5,4,~SK_Subidh_Ali1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
359,https://openreview.net/forum?id=GfofwajcIY,SteerPose: Simultaneous Extrinsic Camera Calibration and Matching from Articulation,"Can freely moving humans or animals themselves serve as calibration targets for multi-camera systems while simultaneously estimating their correspondences across views?  We humans can solve this problem by mentally rotating the observed 2D poses and aligning them with those in the target views.  Inspired by this cognitive ability, we propose SteerPose, a neural network that performs this rotation of 2D poses into another view. By integrating differentiable matching, SteerPose simultaneously performs extrinsic camera calibration and correspondence search within a single unified framework. We also introduce a novel geometric consistency loss that explicitly ensures that the estimated rotation and correspondences result in a valid translation estimation. Experimental results on diverse in-the-wild datasets of humans and animals validate the effectiveness and robustness of the proposed method. Furthermore, we demonstrate that our method can reconstruct the 3D poses of novel animals in multi-camera setups by leveraging off-the-shelf 2D pose estimators and our class-agnostic model.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
364,https://openreview.net/forum?id=AiHONywhfA,Learning from Silence and Noise for Visual Sound Source Localization,"Visual sound source localization is a fundamental perception task that aims to detect
the location of sounding sources in a video given its audio. Despite recent progress,
we identify two shortcomings in current methods: 1) most approaches perform poorly
in cases with low audio-visual semantic correspondence such as silence, noise, and
offscreen sounds, i.e. in the presence of negative audio; and 2) most prior evaluations are
limited to positive cases, where both datasets and metrics convey scenarios with a single
visible sound source in the scene. To address this, we introduce three key contributions.
First, we propose a new training strategy that incorporates silence and noise, which
improves performance in positive cases, while being more robust against negative sounds.
Our resulting self-supervised model, SSL-SaN, achieves state-of-the-art performance
compared to other self-supervised models, both in sound localization and cross-modal
retrieval. Second, we propose a new metric that quantifies the trade-off between alignment
and separability of auditory and visual features across positive and negative audio-visual
pairs. Third, we present IS3+, an extended and improved version of the IS3 synthetic
dataset with negative audio. Our data, metrics and code are available at BLIND.",5,4,~Shentong_Mo1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
366,https://openreview.net/forum?id=6LqtS3WcRH,HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting,"3D human generation is an important problem with a wide range of applications in computer vision and graphics.
Despite recent progress in generative AI such as diffusion models or rendering methods like Neural Radiance Fields or Gaussian Splatting, controlling the generation of accurate 3D humans from text prompts remains an open challenge.
Current methods struggle with fine detail, accurate rendering of hands and faces, human realism, and controlability over appearance.
The lack of diversity, realism, and annotation in human image data also remains a challenge, hindering the development of a foundational 3D human model.
We present a weakly supervised pipeline that tries to address these challenges.
In the first step, we generate a photorealistic human image dataset with controllable attributes such as appearance, race, gender, etc using a image diffusion model.
Next, we propose an efficient mapping approach from image features to 3D point clouds using a transformer-based architecture.
Finally, we close the loop by training a point-cloud diffusion model that is conditioned on the same text prompts used to generate the original samples. 
We demonstrate orders-of-magnitude speed-ups in 3D human generation compared to the state-of-the-art approaches, along with significantly improved text-prompt alignment, realism, and rendering quality.
We will make the code and dataset available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
367,https://openreview.net/forum?id=YJ7FPBYEwi,RUSplatting: Robust 3D Gaussian Splatting for Sparse-View Underwater Scene Reconstruction,"Reconstructing high-fidelity underwater scenes remains a challenging task due to light absorption, scattering, and limited visibility inherent in aquatic environments. This paper presents an enhanced Gaussian Splatting-based framework that improves both the visual quality and geometric accuracy of deep underwater rendering. We propose decoupled learning for RGB channels, guided by the physics of underwater attenuation, to enable more accurate colour restoration. To address sparse-view limitations and improve view consistency, we introduce a frame interpolation strategy with a novel adaptive weighting scheme. Additionally, we introduce a new loss function aimed at reducing noise while preserving edges, which is essential for deep-sea content. We also release a newly collected dataset, Submerged3D, captured specifically in deep-sea environments. Experimental results demonstrate that our framework consistently outperforms state-of-the-art methods with PSNR gains up to 1.90dB, delivering superior perceptual quality and robustness, and offering promising directions for marine robotics and underwater visual analytics.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
370,https://openreview.net/forum?id=1pwADCXiqA,RGB-Event Fusion for Robust Lane Detection,"Lane detection is crucial for autonomous driving, yet standard RGB cameras struggle under extreme lighting conditions. Event cameras, with their high temporal resolution and low latency, offer a promising solution in such environments, motivating the fusion of RGB and event data to enhance lane detection robustness. To address this challenge, we propose a pipeline incorporating the Multiple Feature Fusion Blocks (MFFB), which leverages the temporal correlation of lane markings by maintaining the current state and providing crucial information for subsequent timesteps. Additionally, we integrate edge features to mitigate the insufficiency of lane edge information caused by insufficient motion in event cameras. Despite the availability of event camera datasets for autonomous driving, no existing dataset simultaneously includes both event and RGB data with annotation for lane detection. To bridge this gap, we introduce DSEC-Lane, the first dataset to provide synchronized RGB frames and event data, capturing autonomous driving scenes under extreme lighting conditions across urban, suburban, and rural environments. Extensive experiments demonstrate that our approach surpasses SOTA methods, highlighting the effectiveness of event-image fusion in challenging lane detection scenarios.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
373,https://openreview.net/forum?id=16VORV0bKd,Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval,"The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural images matching the overall semantics and spatial layout of a free-hand sketch. Unlike prior work focused on architectural augmentations of retrieval models, we emphasize the inherent ambiguity and noise present in real-world sketches. This insight motivates a training objective that is explicitly designed to be robust to sketch variability. We show that with an appropriate combination of pre-training, encoder architecture, and loss formulation, it is possible to achieve state-of-the-art performance without the introduction of additional complexity. Extensive experiments on a challenging FS-COCO and widely-used SketchyCOCO datasets confirm the effectiveness of our approach and underline the critical role of training design in cross-modal retrieval tasks as well as the need to improve the evaluation scenarios of scele-level SBIR.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
376,https://openreview.net/forum?id=yYzjiGQC8R,Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference,"Explainable object recognition using vision-language models such as CLIP involves predicting accurate category labels supported by rationales that justify the decision-making process. Existing methods typically rely on prompt-based conditioning, which suffers from limitations in CLIP's text encoder and provides weak conditioning on explanatory structures. Additionally, prior datasets are often restricted to single, and frequently noisy, rationales that fail to capture the full diversity of discriminative image features. In this work, we introduce a multi-rationale explainable object recognition benchmark comprising datasets in which each image is annotated with multiple ground-truth rationales, along with evaluation metrics designed to offer a more comprehensive representation of the task. To overcome the limitations of previous approaches, we propose a contrastive conditional inference (CCI) framework that explicitly models the probabilistic relationships among image embeddings, category labels, and rationales. Without requiring any training, our framework enables more effective conditioning on rationales to predict accurate object categories. Our approach achieves state-of-the-art results on the multi-rationale explainable object recognition benchmark, including strong zero-shot performance, and sets a new standard for both classification accuracy and rationale quality. Together with the benchmark, this work provides a more complete framework for evaluating future models in explainable object recognition. The code will be made available online upon publication and is currently provided as supplementary material.",6,3,~Mengmeng_Ma1|~Hironobu_Fujiyoshi2|~Jungwoo_Lee1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
381,https://openreview.net/forum?id=i8bqRiuN9J,Permutation-Invariant Polar Harmonic Pooling for Point-based Neural Networks,"Point-based learning methods have recently gained significant attention for point cloud data analysis. These neural networks typically consist of a permutation-equivariant backbone network for feature extraction, a symmetric global pooling operation for feature aggregation, and a permutation-invariant classification or regression head. Most of the prior work has focused on improving the equivariant backbone while the global pooling operation crucial for effective feature summarization has remained relatively underexplored. In this paper, we introduce Polar Harmonic Pooling (PHP), a novel symmetric global pooling method that leverages polar harmonic transforms to aggregate point cloud features. Unlike traditional pooling methods that rely on simple statistical measures, PHP captures global frequency patterns across the point set. This approach yields a more expressive and compact representation by encoding both spatial structure and frequency-domain characteristics in polar coordinates. We evaluate PHP on both classification and regression tasks. Experimental results show that PHP enhances accuracy, reduces the number of parameters in the invariant classification or regression head, and helps reducing overfitting when compared to standard statistical pooling methods.",5,4,~Natacha_Kuete_Meli1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
391,https://openreview.net/forum?id=AFeGoWLZbz,Depth Inconsistency-based spatial-channel attention gate for Mirror Segmentation,"We propose a novel depth-aware mirror and specular object segmentation framework that leverages the contextual inconsistency between two types of depth information: time-of-flight (ToF) measurements and depth predicted by a deep neural network. ToF depth values are often significantly overestimated near mirrors due to indirect light paths, while predicted depth maps tend to be overly smoothed in those regions. We observe that this contextual discrepancy serves as a strong cue for identifying mirror regions. However, direct comparison between these depths in either the image or feature space results in high false positive rates, primarily due to their inherently different value ranges and responses to scene content. To address this issue, we introduce a depth inconsistency-based spatial-channel Attention Gate (discAG), which adaptively highlights informative regions in ToF feature maps based on spatial- and channel-wise extracted depth context inconsistencies. By integrating discAG into a segmentation architecture, our method achieves superior performance compared to state-of-the-art approaches. Experimental results on the RGB-D Mirror Segmentation dataset and our newly constructed Specular Object dataset demonstrate the effectiveness of our approach, particularly in challenging scenarios involving small or frameless mirrors.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
392,https://openreview.net/forum?id=sdRMzyK2r4,UMM: A Unified Multi-Modal Model for Low-Level Vision Tasks with Dual-Driven Prompting,"Current research lacks exploration of the unified multi-modal model  for low-level vision tasks. Although there are some RGB-only unified models, they usually require multiple independent decoders or additional parameters for different tasks, failing to exploit their potential connection and shared knowledge. In this paper, we propose a unified multi-modal model (UMM) that can cope with various low-level vision tasks with all parameters shared and one common decoder. The core of our model is the proposed innovative dual-driven prompting paradigm, which aims to employ multi-modal prompting to enhance the robustness of the model and utilize task prompts to guide the model to extract features related to the specific tasks. Furthermore, we propose a task-aware fusion module (TFM). It guides multi-modal fusion through task prompts, enabling the model to focus on key features of the specific tasks during the fusion process. The experimental results show that our unified model UMM achieves competitive performance on various multi-modal low-level vision tasks, including RGB-T glass detection, RGB-T low-light enhancement, RGB-D salient object detection and RGB-N drivable area detection.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
398,https://openreview.net/forum?id=tY2URF4TIG,Knowledge Distillation via Cross Supervising with Attention for Remote Sensing Object Detection,"Knowledge distillation, which serves as a model compression approach that centers on transferring knowledge from a teacher model to a more compact student model, has been extensively employed to derive lightweight models. However, when applied to remote sensing object detection (RSOD), the unique characteristics of remote sensing images, such as small objects and complex background, substantially impede the student model in effective assimilation of knowledge from its superior teacher model. In this paper, we present a novel self-distillation framework designated as Knowledge Distillation via Cross Supervising with Attention (CSAKD), in which the teacher will offer module-level mentoring and adaptive guidance to eliminate the performance discrepancy between the teacher and the student. Specifically, the teacher will have access to the intermediate features of the student, generate a new set of features, and provide additional Cross Supervising for the student. Meanwhile, we propose an Adaptive Attention Weight Modulation (AAWM) to dynamically adjust the intensity of Cross Supervising, which ensures that the student can receive targeted guidance precisely where it is most needed. In addition, we introduce an Attention-Guided Knowledge Alignment (AGKA), enabling the student to concentrate on the area that the teacher deems crucial instead of learning from the teacher’s features blindly. To verify the effectiveness of the proposed method, we choose YOLOv8 and perform extensive experiments on two publicly available datasets, i.e., DIOR and NWPU VHR-10. The experimental results show that our CSAKD outperforms existing state-of-the-art knowledge distillation approaches in the realm of RSOD.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
399,https://openreview.net/forum?id=9QDHkBlkGf,3D Shape Reconstruction from Autonomous Driving Radars,"This paper presents RFconstruct, a framework that enables 3D shape reconstruction using commercial off-the-shelf (COTS) mmWave radars for self-driving scenarios. RFconstruct overcomes radar limitations of low angular resolution, specularity, and sparsity in radar point clouds through a holistic system design that addresses hardware, data processing, and machine learning challenges. The first step is fusing data captured by two radar devices that image orthogonal planes, then performing odometry-aware temporal fusion to generate denser 3D point clouds. RFconstruct then reconstructs 3D shapes of objects using a customized encoder-decoder model that does not require prior knowledge of the object’s bound box. The shape reconstruction performance of RFconstruct is compared against 3D models extracted from a depth camera equipped with a LiDAR. We show that RFconstruct can accurately generate 3D shapes of cars, bikes, and pedestrians.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
402,https://openreview.net/forum?id=L6NTmBrWCi,Log NeRF: Comparing Spaces for Learning Radiance Fields,"Neural Radiance Fields (NeRF) have achieved remarkable results in novel view synthesis, typically using sRGB images for supervision. However, little attention has been paid to the color space in which the network is learning the radiance field representation. Inspired by the Bi-Illuminant Dichromatic Reflection (BIDR) model, which suggests that a logarithmic transformation simplifies the separation of illumination and reflectance, we hypothesize that log-RGB space enables NeRF to learn a more compact and effective representation of scene appearance.

To test this, we captured approximately 30 videos using a GoPro camera, ensuring linear data recovery through inverse encoding. We trained NeRF models under various color space interpretations—linear, sRGB, GPLog, and log RGB—by converting each network output to a common color space before rendering and loss computation, enforcing representation learning in different color spaces. Quantitative and qualitative evaluations demonstrate that using a log RGB color space consistently improves rendering quality, exhibits greater robustness across scenes, and performs particularly well in low-light conditions while using the same bit-depth input images. Further analysis across different network sizes and NeRF variants confirms the generalization and stability of the log-space advantage.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
404,https://openreview.net/forum?id=fiI5TMn2fD,Exploring Histogram-based Color Constancy,"We present a lightweight fully convolutional network for color constancy (LHCC). The network uses multiple 2-D projections of the 3-D log RGB histogram of an image in order to predict the color correction coefficients.

In developing the network, we explored whether to use linear RGB or log RGB data, the network structure (width and depth), how to handle dark pixels, how to generate the 2-D mappings of the 3-D histogram, and how to normalize or transform the bin counts in order to preserve the fine histogram structure. Our results show that attention to each of these details makes a difference in overall performance.

Our most significant findings are that using log RGB outperforms linear RGB for this task, and that using a log transformation of the bin counts outperforms thresholding, a hyperbolic tangent, and linear normalization.

Our exploration resulted in a fully convolutional network with 0.5M parameters that sets a new performance standard on SimpleCube++ with a surprising $2.39^{\circ}$ angular error on the worst 25\%. It is also competitive on older data sets such as Gehler-Shi and NUS8.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
405,https://openreview.net/forum?id=2FnI6Pmx4J,HVLO-YOLO: An Ultra-Lightweight Detection Model for High-voltage Line Obstacles,"With the expansion of high-voltage power grids and the increase of environmental complexity, obstacle detection on high-voltage lines has become an important task to ensure the safety of power systems. Traditional methods rely on manual feature extraction, which is difficult to deal with complex environments. Although deep learning methods improve the detection accuracy, the demand for computing resources is too high to meet the requirements of real-time and lightweight. To this end, this paper proposed an ultra-lightweight obstacle detection model HVLO-YOLO based on YOLO method. To achieve a better balance between detection accuracy and computational cost, three specialized lightweight modules are introduced: (1) a CSP-Partial Convolution with FourGroup (CP4) module that enhances feature extraction efficiency by selectively applying partial convolution and multi-branch group strategies; (2) a Partial Convolution DownSampler (PDown) module that preserves critical information during spatial resolution reduction through a dual-branch design combining max-pooling and partial convolution; and (3) a Partial Convolution Detection Head (PDetect) module that focuses computational resources on key feature regions through selective lightweight aggregation. These modules collaboratively reduce computational burden, minimize parameter count, and enhance obstacle detection accuracy under complex environments. Extensive experiments conducted on two benchmark datasets demonstrate that HVLO-YOLO achieves competitive detection accuracy while significantly reducing model complexity compared to state-of-the-art models.",6,4,~Guangyu_Li3|~Zhihao_Qu1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
408,https://openreview.net/forum?id=uHwQgPu43j,Multimodal Hate Detection Using Dual-Stream Graph Neural Networks,"Hateful videos present serious risks to online safety and real-world well-being, necessitating effective detection methods. Although multimodal classification approaches integrating information from several modalities outperform unimodal ones, they typically neglect that even minimal hateful content defines a video's category. Specifically, they generally treat all content uniformly, instead of emphasizing the hateful components. Additionally, existing multimodal methods cannot systematically capture essential structured information in videos, which limits the effectiveness of multimodal fusion. To address these two limitations, we propose a novel classification model, the multimodal dual-stream graph neural networks. It constructs an instance graph by separating the given video into several instances to extract instance-level features. Then, a complementary weight graph assigns importance weights to these features, highlighting hateful instances. Importance weights and instance features are combined to generate video labels. Our model employs a graph-based framework to systematically model structured relationships within and across modalities. Extensive experiments on public datasets show that our model is state-of-the-art in hateful video classification and has strong explainability.",6,3,~Fengyun_Li1|~Liang_Wang26|~Xiaoqiang_Yan1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
416,https://openreview.net/forum?id=Dyym8CbgQJ,Beyond Softmax: Dual-Branch Sigmoid Architecture for Accurate Class Activation Maps,"Class Activation Mapping (CAM) and its extensions have become indispensable tools for visualizing the evidence behind deep network predictions. However, by relying on a final softmax classifier, these methods suffer from two fundamental distortions: additive logit shifts that arbitrarily bias importance scores, and sign collapse that conflates excitatory and inhibitory features. We propose a simple, architecture-agnostic dual-branch sigmoid head that decouples localization from classification. Given any pretrained model, we clone its classification head into a parallel branch ending in per-class sigmoid outputs, freeze the original softmax head, and fine-tune only the sigmoid branch with class-balanced binary supervision. At inference, softmax retains recognition accuracy, while class evidence maps are generated from the sigmoid branch -- preserving both magnitude and sign of feature contributions. Our method integrates seamlessly with most CAM variants and incurs negligible overhead. Extensive evaluations on fine-grained tasks (CUB-200-2011, Stanford Cars) and WSOL benchmarks (ImageNet-1K, OpenImages30K) show improved explanation fidelity and consistent Top-1 Localization gains -- without any drop in classification accuracy.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
423,https://openreview.net/forum?id=eM8kvN1giK,JDATT: A Joint Distillation Framework for Atmospheric Turbulence Mitigation and Target Detection,"Atmospheric turbulence (AT) introduces severe degradations—such as rippling, blur, and intensity fluctuations—that hinder both image quality and downstream vision tasks like target detection. While recent deep learning–based approaches have advanced AT mitigation using transformer and Mamba architectures, their high complexity and computational cost make them unsuitable for real-time applications, especially in resource-constrained settings such as remote surveillance. Moreover, the common practice of separating turbulence mitigation and object detection leads to inefficiencies and suboptimal performance. To address these challenges, we propose JDATT—a Joint Distillation framework for Atmospheric Turbulence mitigation and Target detection. JDATT integrates state-of-the-art AT mitigation and detection modules and introduces a unified knowledge distillation strategy that compresses both components while minimizing performance loss. We employ a hybrid distillation scheme: feature-level distillation via Channel-Wise Distillation (CWD) and Masked Generative Distillation (MGD), and output-level distillation via Kullback–Leibler divergence. Experiments on synthetic and real-world turbulence datasets demonstrate that JDATT achieves superior visual restoration and detection accuracy while significantly reducing model size and inference time, making it well-suited for real-time deployment.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
427,https://openreview.net/forum?id=MVEty6d9iL,Interactive Occlusion Boundary Estimation through Exploitation of Synthetic Data,"Occlusion boundaries (OBs) geometrically localize occlusion events in 2D images and provide critical cues for scene understanding. In this paper, we present the first systematic study of Interactive Occlusion Boundary Estimation (IOBE), introducing MS3PE — a novel multi-scribble-guided deep-learning framework that advances IOBE through two key innovations: (1) an intuitive multi-scribble interaction mechanism, and (2) a 3-encoding-path network enhanced with multi-scale strip convolutions. Our MS3PE surpasses adapted baselines from seven state-of-the-art interactive segmentation methods, and demonstrates strong potential for OB benchmark construction.
Besides, to address the scarcity of well-annotated real-world data, we propose using synthetic data for training IOBE models, and developed Mesh2OB, the first automated tool for generating precise ground-truth OBs from 3D scenes with self-occlusions explicitly handled, enabling creation of the OB-FUTURE synthetic benchmark that facilitates generalizable training without domain adaptation. Finally, we introduce OB-LabName — a high-quality real-world benchmark comprising 120 meticulously annotated images that advances evaluation standards in OB research.
Source code and resources are available at [link].",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
444,https://openreview.net/forum?id=SrE1HYK0XG,Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning for Video Question Answering,"While significant advancements have been made in video question answering (VideoQA), the potential benefits of enhancing model generalization through tailored difficulty scheduling have been largely overlooked in existing research. This paper seeks to bridge that gap by incorporating VideoQA into a curriculum learning (CL) framework that progressively trains models from simpler to more complex data. Recognizing that conventional self-paced CL methods rely on training loss for difficulty measurement, which might not accurately reflect the intricacies of video-question pairs, we introduce the concept of uncertainty-aware CL. Here, uncertainty serves as the guiding principle for dynamically adjusting the difficulty. Furthermore, we address the challenge posed by uncertainty by presenting a probabilistic modeling approach for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation graph, where the hidden representations are treated as stochastic variables. This yields two distinct types of uncertainty: one related to the inherent uncertainty in the data and another pertaining to the model's confidence. In practice, we seamlessly integrate the VideoQA model into our framework and conduct comprehensive experiments. The findings affirm that our approach not only achieves enhanced performance but also effectively quantifies uncertainty in the context of VideoQA.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
445,https://openreview.net/forum?id=YwCezwu7MT,Frequency-Temporal Feature Integration for Compressed Video Action Recognition,"Compressed domain action recognition aims to identify human actions by directly leveraging I-frames and P-frames extracted from partially decoded compressed videos. Existing works usually adopted Transformer-based architectures, such as ViT, to perform temporal motion modeling. However, these frameworks tended to overlook high-frequency components (e.g., edge textures and local motion boundaries), which compromised their ability to construct precise spatiotemporal semantic representations. To tackle this issue, we present a Frequency-Temporal feature integration framework for compressed video action recognition, which effectively combines high-frequency edge information with low-frequency global context from both temporal and frequency perspectives. At the feature extraction stage, we design a Frequency-Aware and Temporal-Spatial Embedding (FTE) module to mitigate performance degradation caused by the ViT framework’s insensitivity to high-frequency cues. During feature fusion and prediction, we introduce Frequency-Temporal Interaction Attention (FTIA), which facilitates hierarchical integration between temporal dynamics and high-frequency features, enhancing sensitivity to motion-related regions. Extensive experiments on Kinetics-400, UCF-101, and HMDB-51 demonstrate that FreqTNet achieves state-of-the-art performance in the compressed domain while maintaining computational efficiency.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
453,https://openreview.net/forum?id=wnbGL3lTge,HalfMix Augmentation and Regularized Dual-Path Learning for Cross-Domain Gaze Estimation,"Cross-domain gaze estimation presents a persistent challenge in computer vision, as models often experience significant performance degradation when applied to unseen target domains with different characteristics (e.g., subjects, illumination, camera setups). To address this, we first introduce HalfMix Augmentation, a novel data augmentation technique specifically designed for gaze estimation. HalfMix mitigates issues common in conventional mix-based augmentations like MixUp and CutMix by preserving crucial eye regions without overlap or occlusion. Secondly, we propose a Regularized Dual-Path Learning strategy to effectively capitalize on the rich, dual-gaze information inherent in HalfMix-generated samples. This strategy employs a Dual-Path Architecture where a shared encoder feeds into two distinct prediction pathways. To foster diverse and robust feature learning, we incorporate two key regularization components: Diversity-Promoting Regularization (DPR) and Dual-Gaze Feature Alignment (DGFA). Extensive experiments on several benchmark datasets demonstrate that our integrated approach significantly improves cross-domain gaze estimation performance, outperforming existing methods by learning more robust and generalizable gaze representations that are less sensitive to domain shifts.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
457,https://openreview.net/forum?id=CurImKZ74g,A Novel Local Focusing Mechanism for Deepfake Detection Generalization,"The rapid advancement of deepfake generation techniques has intensified the need for robust and generalizable detection methods. Existing approaches based on reconstruction learning typically leverage deep convolutional networks to extract differential features. However, these methods show poor generalization across object categories (e.g., from faces to cars) and generation domains (e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep CNNs. First, models trained on a specific category tend to overfit to semantic feature distributions, making them less transferable to other categories, especially as network depth increases. Second, Global Average Pooling (GAP) compresses critical local forgery cues into a single vector, thus discarding discriminative patterns vital for real-fake classification.

To address these issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends to discriminative local features for differentiating fake from real images. LFM integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP) module to select the $K$ most informative local patterns. To mitigate potential overfitting introduced by Top-K pooling, we introduce two regularization techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which enhance the model's robustness. LFM achieves a 3.7\% improvement in accuracy and a 2.8\% increase in average precision over the state-of-the-art Neighboring Pixel Relationships (NPR) method, while maintaining exceptional efficiency at 1789 FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for cross-domain deepfake detection.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
458,https://openreview.net/forum?id=73xx12118A,Intra-Modal Divergence-Weighted Distillation for Vision-Language Models,"Large vision-language models like CLIP offer strong zero-shot capabilities but are computationally demanding. Knowledge distillation is crucial for creating efficient student models; however, effectively transferring the teacher's nuanced understanding of within-modality relationships, especially among negative examples, remains challenging. We introduce a novel distillation method focused on capturing the teacher's intra-modal relational knowledge. Our approach employs Kullback-Leibler divergence to measure the disagreement between student and teacher pairwise similarity distributions within each modality. This disagreement score then dynamically weights the distillation loss, compelling the student to prioritize learning from samples exhibiting the most significant relational discrepancies. This strategy encourages closer alignment of the student's internal representation space with the teacher's. Experiments demonstrate our method produces performant and efficient student models by effectively transferring this vital relational information. The source code will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
461,https://openreview.net/forum?id=1RG8QrQzXi,FSF3A: Federated Spatial Feature Alignment and Adaptive Aggregation for Heterogeneous Brain Tumor Segmentation,"Deep learning has the potential to identify complex patterns in brain tumors, but privacy concerns hinder centralized approaches. Federated learning (FL) addresses privacy concerns by enabling collaborative model training without exchanging local client data. However, managing data heterogeneity in FL is challenging due to the varying distributions and data scales at the local clients. To address these problems, we propose FSF3A, a novel federated framework with two approaches: \textbf{(i)} \textit{client localized structural loss (CLSL)}, a local training loss function that can handle varying data distributions by effectively finding a trade-off between spatial localization of local data, using modified dice loss, and the similarity between model representations to correct the local training of individual clients through our localized contrastive penalty and \textbf{(ii)} \textit{adaptive scale-aware weighted aggregation (ASWA)} method which facilitates weighted aggregation at the global model, prioritizing clients based on the data scales to ensure fairness in representation. The efficacy of FSF3A is demonstrated through comprehensive experiments on UCSF PDGM, UPenn-GBM, and MSD datasets for brain tumor segmentation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
478,https://openreview.net/forum?id=nKdKqNYlTe,Bézier Curve-Based Stroke Extraction for Handwritten Characters,"Accurate stroke extraction from handwritten character images is essential for realistic handwriting generation and stroke-based character recognition. Traditional pixel-based segmentation methods often produce fragmented strokes, failing to preserve accurate topological structures. To solve this, we propose a novel Bézier curve-based stroke extraction method. Our approach employs deep learning-based image registration to initially align handwritten images with vector-based reference fonts, followed by precise Bézier curve fitting to the skeleton of the character image. Evaluations on a Japanese handwritten character dataset show that characters reconstructed from extracted strokes achieve high OCR recognition accuracy (95.58%), indicating accurate representation of the original handwriting structures. Additionally, geometric similarity evaluated via feature-point matching confirms precise stroke extraction, though accuracy slightly decreases for complex characters with many strokes. Moreover, our Bézier-based representation naturally supports interpolation between character images by linearly interpolating corresponding control points, enabling the synthesis of diverse handwritten samples. Such synthesized variations hold promise for augmenting OCR training datasets as well as providing a succinct representation of a sample space for stroke-based character image generation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
479,https://openreview.net/forum?id=aq3XUOPmqD,CleverDistiller: Simple and Spatially Consistent Cross-modal Distillation,"Vision foundation models have revolutionized 2D camera-based perception by ex- tracting generalized features for downstream tasks. Recent work applies self-supervised cross-modal knowledge distillation (KD) to transfer these capabilities to 3D LiDAR mod- els, but often relies on complex losses, pseudo-semantic maps, or limits KD to seman- tic segmentation. We introduce CleverDistiller, a self-supervised, cross-modal 2D-to-3D KD framework with simple yet effective design choices. Our method uses a direct feature similarity loss and an MLP projection head to capture complex semantic dependencies without relying on pseudo-semantic maps or explicit semantic supervision. Additionally, we enhance the learned knowledge with a self-supervised occupancy prediction task, further improving 3D spatial reasoning. Experiments on autonomous driving benchmarks show that CleverDistiller achieves state-of-the-art performance in both semantic segmentation and 3D object detection, with up to 10% mIoU improvement, particularly when fine tuning with limited data. Additionally, models pretrained with our approch shows extreme robustness towards weather and sensor corruption as well as great domain generalization capabilities.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
486,https://openreview.net/forum?id=mFMFgVaykj,FreqSelect: Frequency-Aware fMRI-to-Image Reconstruction,"Reconstructing natural images from fMRI data remains a core challenge in natural decoding due to the mismatch between the richness of visual stimuli and the noisy, low-resolution nature of fMRI signals. While recent two-stage models, combining deep VAEs with diffusion models, have advanced this task, they treat all spatial-frequency components of the input equally. This uniform treatment forces the model to extract meaningful features and suppress irrelevant noise simultaneously, limiting its effectiveness. We introduce FreqSelect, a lightweight, adaptive module that selectively filters spatial-frequency bands before encoding. By dynamically emphasizing frequencies that are most predictive of brain activity and suppressing those that are uninformative, FreqSelect acts as a content-aware gate between image features and neural data. It integrates seamlessly into standard VDVAE-diffusion pipelines and requires no additional supervision. Evaluated on the Natural Scenes dataset, FreqSelect consistently improves reconstruction quality across both low-level (e.g., SSIM) and high-level (e.g., perceptual similarity) metrics. Beyond performance gains, the learned frequency-selection patterns offer interpretable insights into how different visual frequencies are represented in the brain. Our method generalizes across subjects and scenes, and holds promise for extension to other neuroimaging modalities such as EEG and MEG, offering a principled approach to enhancing both decoding accuracy and neuroscientific interpretability.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
493,https://openreview.net/forum?id=lFzBY8w4y7,Audio-Visual Separation with Hierarchical Fusion and Representation Alignment,"Self-supervised audio-visual source separation leverages natural correlations between audio and vision modalities to separate mixed audio signals. 
In this work, we first systematically analyze the performance of existing  multimodal fusion methods for audio-visual separation task, demonstrating that the performance of different fusion strategies is closely linked to the characteristics of the sound—middle fusion is better suited for handling short, transient sounds, while late fusion is more effective for capturing sustained and harmonically rich sounds.
We thus propose a hierarchical fusion strategy that effectively integrates both fusion stages.
In addition, training can be made easier by incorporating high-quality external audio representations, rather than relying solely on the audio branch to learn them independently. 
To explore this, we propose a representation alignment approach that aligns the latent features of the audio encoder with embeddings extracted from pre-trained audio models.
Extensive experiments on MUSIC, MUSIC-21 and VGGSound datasets demonstrate that our approach achieves state-of-the-art results, surpassing existing methods under the self-supervised setting.
We further analyze the impact of representation alignment on audio features, showing that it reduces modality gap between the audio and visual modalities.",5,3,~Shentong_Mo1|~Himangi_Mittal1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
495,https://openreview.net/forum?id=b3sQnyQbgz,An Explorative Study on Abstract Images and Visual Representations Learned from Them,"Imagine living in a world composed solely of primitive shapes—could you still recognise familiar objects? Recent studies have shown that abstract images—constructed by primitive shapes—can indeed convey visual semantic information to deep learning models. However, representations obtained from such images often fall short compared to those derived from traditional raster images. In this paper, we study the reasons behind this performance gap and investigate how much high-level semantic content can be captured at different abstraction levels. To this end, we introduce the Hierarchical Abstraction Image Dataset (HAID), a novel data collection that comprises abstract images generated from common raster image datasets at multiple levels of abstraction. We then train and evaluate conventional vision systems on HAID across various tasks of classification, segmentation, and object detection, providing a comprehensive study between rasterised and abstract image representations. We also discussed if the abstract image can be considered as a potentially effective format to provide visual semantic information and contribute to the vision tasks. Code and models will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
499,https://openreview.net/forum?id=rWNwpcm46A,"One target to align them all: LiDAR, RGB and event cameras extrinsic calibration for Autonomous Driving","We present a novel multi-modal extrinsic calibration framework designed to simultaneously estimate the relative poses between RGB cameras, LiDARs, and Event cameras. Core of our approach is a novel 3D calibration target, specifically designed and constructed to be concurrently perceived by all three sensing modalities. The target encodes features in planes, ChArUco, and active LED patterns, each tailored to the unique characteristics of LiDARs, RGB cameras, and event cameras respectively. This unique design enables a one-shot, joint extrinsic calibration process, in contrast to existing approaches that typically rely on separate, pairwise calibrations.
Our  calibration pipeline is suited for complex vision systems, and we demonstrate the effectiveness of our approach in the context of autonomous driving, where precise multi-sensor alignment is critical. We validate the benefit of our approach through an extensive experimental evaluation on a custom built dataset, recorded with an advanced autonomous driving sensor setup,  confirming the accuracy and robustness of our method.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
500,https://openreview.net/forum?id=RZc4goreK0,UFD-KD: Unified Frequency Decoupled Knowledge Distillation,"In this paper, we question whether the feature distillation has the same property as logits distillation, that its learning mainly focuses on one or several principal components, easily missing minor but informative features. In principle, the distribution of feature maps is unlikely to the logits intuitively, whose distribution is naturally imbalanced. However, we surprisingly find that the feature distribution becomes extremely imbalanced after applying the discrete cosine transform (DCT), which shows attributes similar to logit distillation. Inspired by this, we propose Unified Frequency Decoupled Knowledge Distillation (UFD-KD), which is designed to address the negative effect of the imbalance of feature distribution. Specifically, UFD-KD applies DCT to spatial and channel dimensions respectively, attending to decouple the features based on the properties of frequency domain. For each dimension, we design a parameterized weight strategy to emphasize the minor and sparse features which used to be out of notice because of the principal features. Considering the orthogonality of the two dimensions, we assign them different weights to balance the overall feature alignment. In the validation experiments on Cifar-100, 11 teacher-student model pairs demonstrates excellent performance, achieving 3.1\% accuracy gains for ResNet50→ResNet18. Furthermore, in large-scale validation experiments on ImageNet-1K, UFD maintained competitive performance improvement (71.98\% in Swin-T→ResNet18). Additionally, we validate the task transfer capability on ADE20K, achieving 36.89\% mIoU for DeepLabv3- ResNet101→DeepLabv3-ResNet18.",5,5,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
502,https://openreview.net/forum?id=dj6wJGGo8J,DefectGPT: Towards Multi-Class Defect Detection with Limited Electrical Samples,"Traditional deep learning (DL) approaches for electrical defect detection typically require large quantities of labeled data. However, in real-world scenarios, defect samples are significantly rarer than normal samples, making data collection both challenging and resource-intensive. Meanwhile, the wide variability in anomaly types and the unpredictable nature of defect locations further complicate the labeling process. In this paper, we present DefectGPT, a novel approach for electrical defect detection and classification by leveraging multimodal large language models (MLLMs). Specifically, DefectGPT generates defect descriptions to establish a rich semantic context for subsequent reasoning and classification. To effectively utilize limited training samples and enhance classification accuracy, we introduce a novel in-context learning technique, termed hypothesis-first learning (HFL), which facilitates the DefectGPT to generate an initial hypothesis before refining its knowledge. This approach enables DefectGPT to improve defect classification progressively through self-reflection, enhancing its effectiveness and generalization with limited electrical samples. We further conduct a theoretical analysis to gain deeper insights into the underlying learning mechanism of DefectGPT. Experimental evaluations on two distinct electrical datasets demonstrate the effectiveness and generalizability of our approach, highlighting its potential to reduce data dependency while maintaining high classification accuracy.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
507,https://openreview.net/forum?id=4uPAdnv4SN,Robust and Label-efficient Deep Waste Detection,"Effective waste sorting is critical for sustainable recycling, yet AI research in this domain continues to lag behind commercial systems due to limited datasets and reliance on legacy object detectors. In this work, we advance AI-driven waste detection by establishing strong baselines and introducing an ensemble-based semi-supervised learning framework. We first benchmark state-of-the-art Open-Vocabulary Object Detection (OVOD) models on the real-world ZeroWaste dataset, demonstrating that while class-only prompts perform poorly, LLM-optimized prompts significantly enhance zero-shot accuracy. Next, to address domain-specific limitations, we fine-tune modern transformer-based detectors, achieving a new baseline of 51.6 mAP. We then propose a soft pseudo-labeling strategy that fuses ensemble predictions using spatial and consensus-aware weighting, enabling robust semi-supervised training. Applied to the unlabeled ZeroWaste-s subset, our pseudo-annotations achieve performance gains that surpass fully supervised training, underscoring the effectiveness of scalable annotation pipelines. Our work contributes to the research community by establishing rigorous baselines, introducing a robust ensemble-based pseudo-labeling pipeline, generating high-quality annotations for the unlabeled ZeroWaste-s subset, and systematically evaluating OVOD models under real-world waste sorting conditions.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
509,https://openreview.net/forum?id=RjxLI6KmYX,Dynamic Convolution and Graph-Coupled Attention for Cross-Subject EEG-Vision Decoding,"Electroencephalography (EEG) offers a non-invasive route to visual object decoding, but practical deployment is hampered by the signals’ non-stationarity, low signal-to-noise ratio and pronounced inter-subject variability. Existing models employ fixed convolutional filters and therefore generalize poorly across subjects. We introduce \textit{ECHO-Net}—an adaptive, hierarchically organised network that assembles dynamic convolutional kernels, conditioning them on each incoming EEG trial to capture transient neural dynamics. A cross-modal contrastive objective aligns the resulting representations with CLIP image embeddings, while a channel–filter attention mechanism emphasises task-relevant electrodes and time–frequency bands. To regularise spatial structure, an embedded EEG-GAT module propagates information over a fully connected electrode graph, producing more consistent cross-subject features. Evaluated on the 200-way THINGS-EEG benchmark, our method attains 18.5\% top-1 and 44.1\% top-5 cross-subject accuracy—surpassing the strongest prior approach by 2.9\% top-1.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
511,https://openreview.net/forum?id=0ouf2DN8jF,LieMorph: Transformer-based Image Registration Using Flows on Lie Groups,"We propose a data-driven approach for deformable image registration that extends the TransMorph architecture with a module based on the flow equation formulated on matrix fields. This allows us to precisely control components of the deformation that should be ignored by the regularization and provides a powerful plug-in replacement for the deformation field generator in learning-based methods. Combined with the TransMorph architecture, our method performs favorably on the IXI and OASIS datasets.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Zhixiang Chen
516,https://openreview.net/forum?id=K3t3SXJSq7,Making Rotation Averaging Fast and Robust with Anisotropic Coordinate Descent,"Anisotropic rotation averaging has recently been explored as a natural extension of respective isotropic methods. In the anisotropic formulation, uncertainties of the estimated relative rotations---obtained via standard two-view optimization---are propagated to the optimization of absolute rotations. The resulting semidefinite relaxations are able to recover global minima but scale poorly with the problem size. Local methods are fast and also admit robust estimation but are sensitive to initialization. They usually employ minimum spanning trees and therefore suffer from drift accumulation and can get trapped in poor local minima. In this paper, we attempt to bridge the gap between optimality, robustness and efficiency of anisotropic rotation averaging. We analyze a family of block coordinate descent methods initially proposed to optimize the standard chordal distances, and derive a much simpler formulation and an anisotropic extension obtaining a fast general solver. We integrate this solver into the extended anisotropic large-scale robust rotation averaging pipeline. The resulting algorithm achieves state-of-the-art performance on public structure-from-motion datasets.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
517,https://openreview.net/forum?id=4ADbrLNOrg,Catching the Unknown with Limited Data: Bi-Directional Prompt Tuning in CLIP for Few-Shot Open-Set Adaptation,"Domain-adaptive few-shot open-set (DAFSOS) learning aims to transfer knowledge from a label-rich source domain to a target domain where only a few labeled samples are available. Both domains contain disjoint sets of classes, and at inference time, models must accurately classify unlabeled samples from novel target-domain classes using limited support examples, while also recognizing outliers. Existing approaches often rely on meta-learning with sophisticated loss functions to obtain domain-invariant, discriminative features, yet overlook semantic information. 
We propose a novel prompt-learning framework for CLIP, named BIMAP-CLIP (Bi-directional Multi-modal Attribute-enriched Prompting for CLIP), tailored to the DAFSOS setting. Its core innovation is a bi-directional, multi-modal prompt-learning mechanism that shares learnable tokens across both image and text encoders, enhancing visual–semantic alignment. We also introduce a hybrid prompt initialization strategy that integrates manual and learnable prompts to make the prompts resilient to domains and incorporates fine-grained class attributes to tackle the scarcity of training samples. In parallel, a thresholding-based method on prompt-image similarity scores is employed for outlier detection. Across multiple benchmarks, BIMAP-CLIP consistently outperforms existing counterparts.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
528,https://openreview.net/forum?id=BtiuRB9WvX,"OmniSegNet: Towards Scalable, Efficient & Universal Medical Image Segmentation","Medical image segmentation spans diverse modalities, including MRI (Magnetic Resonance Imaging), CT (Computed Tomography), OCT (Optical Coherence Tomography), and USG (Ultrasound Sonography), each with unique spatial and contextual characteristics. This heterogeneity demands architectures that balance global context with fine-grained anatomical details, a challenge for standard models like U-Net and Transformer-based approaches, which either lack generalization or struggle with subtle features. Privacy constraints further restrict the use of large models like the Segment Anything Model (SAM) for cloud-based inference, complicating edge deployment and increasing development complexity. To address these challenges, we propose OmniSegNet, a unified, scalable encoder-decoder architecture that integrates SE-enhanced residual blocks for efficient local-global context capture and ASPP for multi-scale feature aggregation. Compound scaling of depth ($\alpha$) and width ($\beta$) supports flexible model variants ranging from 1.5M to 10M parameters, identified through latency-aware neural architecture search for real-time deployment on devices like Raspberry Pi and Arduino Nano. To generalize without task-specific tuning, we introduce OmniKD, a unified distillation framework that transfers knowledge from fine-tuned SAM models via logits, intermediate features, attention maps, relational structures, and contextual similarities, eliminating the need for handcrafted loss functions. OmniSegNet achieves up to $10\times$ parameter reduction while improving Dice scores by 10--12\% across diverse medical imaging benchmarks, including ISIC, CHAOS, and MRBrainS18, offering a scalable, efficient, and privacy-preserving solution for real-world medical segmentation.",4,4,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
540,https://openreview.net/forum?id=VckRTsYZyX,OptSplat: Recurrent Optimization for Generalizable Reconstruction and Novel View Renderings,"We propose an efficient feed-forward model for novel view synthesis and 3D reconstruction based on Gaussian Splatting, featuring a scalable architecture that reliably predicts multi-view depth maps and 3D Gaussian primitives from as few as two input views. Existing multi-view depth estimation techniques typically depend on processing plane-swept cost volumes, which generate probability distributions over a discrete set of candidate depths. This approach limits scalability, especially when finer depth sampling or higher spatial resolution is required. To address this, we design an optimization-inspired architecture $\textit{OptSplat}$, that employs recurrent iterative updates to refine depth maps and pixel-aligned Gaussian primitives based on previous predictions. Our model leverages a unified update operator that iteratively indexes global cost volumes, progressively improving predictions in the joint space of depth and Gaussian parameters. Comprehensive evaluations across the real world datasets of $\textit{RealEstate10K}$, $\textit{ACID}$ and $\textit{DL3DV}$ shows that our model demonstrates strong cross-dataset generalization and competitive rendering quality for novel views compared to the existing works with plane swept cost volumes, while at the same time offering a significant boost in reconstruction and rendering speed, especially for high-resolution inputs.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
543,https://openreview.net/forum?id=pKVy5vYv8k,Temporally Compressed 3D Gaussian Splatting for Dynamic Scenes,"Recent advancements in high-fidelity dynamic scene reconstruction have leveraged dynamic 3D Gaussians and 4D Gaussian Splatting for realistic scene representation. However, to make these methods viable for real-time applications such as AR/VR, gaming, and rendering on low-power devices, substantial reductions in memory usage and improvements in rendering efficiency are required. While many state-of-the-art methods prioritize lightweight implementations, they struggle in handling scenes with complex motions or long sequences. In this work, we introduce Temporally Compressed 3D Gaussian Splatting (TC3DGS), a novel technique designed specifically to effectively compress dynamic 3D Gaussian representations. TC3DGS selectively prunes Gaussians based on their temporal relevance and employs gradient-aware mixed-precision quantization to dynamically compress Gaussian parameters. In addition, TC3DGS exploits an adapted version of the Ramer-Douglas-Peucker algorithm to further reduce storage by interpolating Gaussian trajectories across frames. Our experiments on multiple datasets demonstrate that TC3DGS achieves up to 67$\times$ compression with minimal or no degradation in visual quality.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
546,https://openreview.net/forum?id=B1no1S5V0D,Multi-Method Ensemble for Out-of-Distribution Detection,"Detecting out-of-distribution (OOD) samples is essential for neural networks operating in open-world settings, particularly in safety-critical applications. Existing methods have improved OOD detection by leveraging two main techniques: feature truncation, which increases the separation between in-distribution (ID) and OOD samples, and scoring functions, which assign scores to distinguish between ID and OOD data. However, most approaches either focus on a single family of techniques or evaluate their effectiveness on a specific type of OOD dataset, overlooking the potential of combining multiple existing solutions. Motivated by this observation, we theoretically and empirically demonstrate that state-of-the-art feature truncation and scoring functions can be effectively combined. Moreover, we show that aggregating multiple scoring functions enhances robustness against various types of OOD samples. Based on these insights, we propose the Multi-Method Ensemble (MME) score, which unifies state-of-the-art OOD detectors into a single, more effective scoring function. Extensive experiments on both large-scale and small-scale benchmarks, covering near-OOD and far-OOD scenarios, show that MME significantly outperforms recent state-of-the-art methods across all benchmarks. Notably, using the BiT model, our method achieves an average FPR95 of 27.57\% on the challenging ImageNet-1K benchmark, improving performance by 6\% over the best existing baseline.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
548,https://openreview.net/forum?id=xnO31AkND9,Pandora: Articulated 3D Scene Graphs from Egocentric Vision,"Robotic mapping systems typically approach building metric-semantic scene representations from the robot's own sensors and cameras. However, these ""first person"" maps inherit the robot's own limitations due to its embodiment or skillset, which may leave many aspects of the environment unexplored. For example, the robot might not be able to open drawers or access wall cabinets. In this sense, the map representation is not as complete, and requires a more capable robot to fill in the gaps. We narrow these blind spots in current methods by leveraging egocentric data captured as a human naturally explores a scene wearing Project Aria glasses, giving a way to directly transfer knowledge about articulation from the human to any deployable robot. We demonstrate that, by using simple heuristics, we can leverage egocentric data to recover models of articulate object parts, with quality comparable to those of state-of-the-art methods based on other input modalities. We also show how to integrate these models into 3D scene graph representations, leading to a better understanding of object dynamics and object-container relationships. We finally demonstrate that these articulated 3D scene graphs enhance a robot's ability to perform mobile manipulation tasks, showcasing an application where a Boston Dynamics Spot is tasked with retrieving concealed target items, given only the 3D scene graph as input.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
558,https://openreview.net/forum?id=xP5cFhkZYd,EZIGen: Enhancing zero-shot personalized image generation with precise subject encoding and decoupled guidance,"Zero-shot personalized image generation models aim to produce images that align with both a given text prompt and subject image, requiring the model to incorporate both sources of guidance. Existing methods often struggle to capture fine-grained subject details and frequently prioritize one form of guidance over the other, resulting in suboptimal subject encoding and imbalanced generation. In this study, we uncover key insights into overcoming such drawbacks, notably that 1) the choice of the subject image encoder critically influences subject identity preservation and training efficiency, and 2) the text and subject guidance should take effect at different denoising stages. Building on these insights, we introduce a new approach, EZIGen, that employs two main components: leveraging a fixed pre-trained Diffusion UNet itself as subject encoder, following a process that balances the two guidances by separating their dominance stage and revisiting certain time steps to bootstrap subject transfer quality. Through these two components, EZIGen, initially built upon SD2.1-base, achieved state-of-the-art performances on multiple personalized generation benchmarks with a unified model, while using 100 times less training data. Moreover, by further migrating our design to SDXL, EZIGen is proven to be a versatile model-agnostic solution for personalized generation.",7,3,~Runpei_Dong1|~Zhe_Wang9|~Zhao_Wang3|~Changmiao_Wang1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
566,https://openreview.net/forum?id=lky7HOF2UL,OpenHuman4D: Open-Vocabulary 4D Human Parsing,"Understanding dynamic 3D human representation has become increasingly critical in virtual and extended reality applications. However, existing human part segmentation methods are constrained by reliance on closed-set datasets and prolonged inference times, which significantly restrict their applicability. In this paper, we introduce the first 4D human parsing framework that simultaneously addresses these challenges by reducing the inference time and introducing open-vocabulary capabilities. Building upon state-of-the-art open-vocabulary 3D human parsing techniques, our approach extends the support to 4D human-centric video with three key innovations: 1) We adopt mask-based video object tracking to efficiently establish spatial and temporal correspondences, avoiding the necessity of segmenting all frames. 2) A novel Mask Validation module is designed to manage new target identification and mitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating memory-conditioned attention and logits equalization for robust embedding fusion. 
Extensive experiments demonstrate the effectiveness and flexibility of the proposed method on 4D human-centric parsing tasks, achieving up to 93.3\% acceleration compared to the previous state-of-the-art method, which was limited to parsing fixed classes.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
567,https://openreview.net/forum?id=ze19SPf5oO,Zero-Shot Anomaly Detection with Dual-Branch Prompt Selection,"Zero‑shot anomaly detection (ZSAD) enables identifying and localizing defects in unseen categories by relying solely on generalizable features rather than requiring any labeled examples of anomalies. However, existing ZSAD methods, whether using fixed or learned prompts, struggle under domain shifts because their training data are derived from limited training domains and fail to generalize to new distributions. In this paper, we introduce PILOT, a framework that addresses these challenges through: (1) a novel dual-branch prompt selection mechanism that dynamically combines learnable prompts with structured semantic attributes, adaptively selecting the most relevant anomaly cues for each input image; and (2) a label-free test-time adaptation strategy that selectively refines only the chosen prompt using high-confidence pseudo-labels from unlabeled test data. Through extensive experiments on 13 industrial and medical benchmarks, PILOT shows that it is state-of-the-art in both anomaly detection and localization.",6,3,~Zakaria_rguibi1|~Yang_He_Xu1|~Mingqi_Gao3,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
577,https://openreview.net/forum?id=FuR3S979XJ,Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency,"Zero-shot denoisers address the dataset dependency of deep-learning-based denoisers, enabling the denoising of unseen single images. Nonetheless, existing zero-shot methods suffer from long training times and rely on the assumption of noise independence and a zero-mean property, limiting their effectiveness in real-world denoising scenarios where noise characteristics are more complicated. This paper proposes an efficient and effective method for real-world denoising, the Zero-Shot denoiser based on Cross-Frequency Consistency (ZSCFC), which enables training and denoising with a single noisy image and does not rely on assumptions about noise distribution. Specifically, image textures exhibit position similarity and content consistency across different frequency bands, while noise does not. Based on this property, we developed cross-frequency consistency loss and an ultralight network to realize image denoising. Experiments on various real-world image datasets demonstrate that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of computational efficiency and denoising performance. The code will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
588,https://openreview.net/forum?id=4DI8BQw2Y6,CoT-SD: Chain-of-Thought Semantic Denoising,"Recent advances in zero-shot image denoising have largely been driven by self-supervised learning and generative models. However, these methods often struggle to preserve fine-grained structural details in real-world noisy images. Inspired by the human ability for slow thinking in problem-solving, which involves breaking down tasks and addressing them step by step, this paper proposes Chain-of-Thought Semantic Denoising (CoT-SD), a novel zero-shot denoising framework that leverages multi-step self-reflective reasoning and CLIP-based semantic alignment to progressively enhance image quality. Unlike conventional single-step denoising approaches, CoT-SD formulates denoising as an iterative reasoning process in which both the encoder and the decoder progressively decompose the problem, refine their understanding, and iteratively generate cleaner images. The CLIP-guided decoder ensures that denoised outputs align with high-level image semantics, enabling deeper image understanding for reflection, thereby improving the quality of the denoised images. Experimental results demonstrate that our CoT-SD outperforms other state-of-the-art (SOTA) dataset-free denoisers. The code will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
593,https://openreview.net/forum?id=y3UNzMC0fo,PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads,"Achieving realistic hair strand synthesis is essential for creating lifelike digital humans, but producing high-fidelity hair strand geometry remains a significant challenge. 
Existing methods require a complex setup for data acquisition, involving multi-view images captured in constrained studio environments. Additionally, these methods have longer hair volume estimation and strand synthesis times, which hinder efficiency.
We introduce PanoHair, a model that estimates head geometry as signed distance fields using knowledge distillation from a pre-trained generative teacher model for head synthesis. 
Our approach enables the prediction of semantic segmentation masks and 3D orientations specifically for the hair region of the estimated geometry. 
Our method is generative and can generate diverse hairstyles with latent space manipulations. For real images, our approach involves an inversion process to infer latent codes and produces visually appealing hair strands, offering a streamlined alternative to complex multi-view data acquisition setups.  
Given the latent code, PanoHair generates a clean manifold mesh for the hair region in under 5 seconds, along with semantic and orientation maps, marking a significant improvement over existing methods, as demonstrated in our experiments.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
594,https://openreview.net/forum?id=jSMRNWH3PP,Video Dataset Condensation with Diffusion Models,"In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance, particularly in the video domain. In this paper, we focus on video dataset distillation. We start from employing a video diffusion model to generate synthetic videos. Since the videos are generated only once, this significantly reduces computational costs. Next, we introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \(10.61\%\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
596,https://openreview.net/forum?id=i1geycWJwc,TopoMortar: A Dataset to Evaluate Topology Accuracy in Image Segmentation,"We present TopoMortar, a brick wall dataset that is the first dataset specifically designed to evaluate topology-focused image segmentation methods, such as topology loss functions. Motivated by the known sensitivity of methods to dataset challenges, such as small training sets, noisy labels, and out-of-distribution test-set images, TopoMortar is created to enable in two ways investigating methods' effectiveness at improving topology accuracy. First, by eliminating dataset challenges that, as we show, impact the effectiveness of topology loss functions.
Second, by allowing to represent different dataset challenges in the same dataset, isolating methods' performance from dataset challenges. TopoMortar includes three types of labels (accurate, pseudo-labels, and noisy labels), two fixed training sets (large and small), and in-distribution and out-of-distribution test-set images. We compared eight loss functions on TopoMortar, and we found that clDice achieved the most topologically accurate segmentations, and that the relative advantageousness of the other loss functions depends on the experimental setting. Additionally, we show that data augmentation and self-distillation can elevate Cross entropy Dice loss to surpass most topology loss functions, and that those simple methods can enhance topology loss functions as well. TopoMortar and our code can be found at 
https://anonymous.4open.science/r/TopoMortarAnonymous-E880.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
602,https://openreview.net/forum?id=YF8hHJUaiJ,Dynamic Try-On: Taming Video Virtual Try-on with Dynamic Attention Mechanism,"Video try-on stands as a promising area for its tremendous real-world potential. Previous research on video try-on has primarily focused on transferring product clothing images to videos with simple human poses, while performing poorly with complex movements. To better preserve clothing details, those approaches are armed with an additional garment encoder, resulting in higher computational resource consumption. The primary challenges in this domain are twofold: (1) leveraging the garment encoder's capabilities in video try-on while lowering computational requirements; (2) ensuring temporal consistency in the synthesis of human body parts, especially during rapid movements. To tackle these issues, we propose a novel video try-on framework based on Diffusion Transformer(DiT), named \textbf{Dynamic Try-On}. To reduce computational overhead, we adopt a straightforward approach by utilizing the DiT backbone itself as the garment encoder and employing a dynamic feature fusion module to store and integrate garment features. To ensure temporal consistency of human body parts, we introduce a limb-aware dynamic attention module that enforces the DiT backbone to focus on the regions of human limbs during the denoising process. Extensive experiments demonstrate the superiority of Dynamic Try-On in generating stable and smooth try-on results, even for videos featuring complicated human postures.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
603,https://openreview.net/forum?id=pTTLWzxO8M,Dual-Stream Adapters for Open-Set Segmentation in Driving Scenes,"The task of segmenting novel categories in road scenes, often referred to as anomaly segmentation, has been recently addressed with great success by using mask-based architectures, but their efficacy is dependent on fine-tuning large transformer backbones. In this work, we design a specialized adapter for this task, which makes it possible to leverage even large backbones without re-training them. The key feature of our adapter is the separation of the adapted features in two streams, one specialized on the known categories (in-distribution) and the other that captures the characteristics of out-of-distribution categories. The out-of-distribution features adaptation is supervised by using synthetic negative data generated by a normalizing flow process. This dual-stream architecture allows to better disentangle features for known and unknown categories, preserving in-distribution performance while enabling direct and more accurate anomaly segmentation with fewer false positives. Experiments show that dual-stream adapters outperform previous methods while reducing training parameters by 38\%.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
610,https://openreview.net/forum?id=PTxUtywcuF,"HERO-VQL: Hierarchical, Egocentric and Robust Visual Query Localization","In this work, we tackle the egocentric visual query localization (VQL), where a model should localize the query object in a long-form egocentric video. Frequent and abrupt viewpoint changes in egocentric videos cause significant object appearance variations and partial occlusions, making it difficult for existing methods to achieve accurate localization. To tackle these challenges, we introduce Hierarchical, Egocentric and RObust Visual Query Localization (HERO-VQL), a novel method inspired by human cognitive process in object recognition. We propose i) Top-down Attention Guidance (TAG) and ii) Egocentric Augmentation based Consistency Training (EgoACT). Top-down Attention Guidance refines the attention mechanism by leveraging the class token for high-level context and principal component score maps for fine-grained localization. To enhance learning in diverse and challenging matching scenarios, EgoAug enhances query diversity by replacing the query with a randomly selected corresponding object from groundtruth annotations and simulates extreme viewpoint changes by reordering video frames. Additionally, CT loss enforces stable object localization across different augmentation scenarios. Extensive experiments on VQ2D dataset validate that HERO-VQL effectively handles egocentric challenges, significantly outperforming baselines.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
614,https://openreview.net/forum?id=Zubl6RjmQV,PerSense: Training-Free Personalized Instance Segmentation in Dense Images,"The emergence of foundational models has significantly advanced segmentation approaches. However, challenges still remain in dense scenarios, where occlusions, scale variations, and clutter impede precise instance delineation. To address this, we propose PerSense, an end-to-end, training-free, and model-agnostic one-shot framework for personalized instance segmentation in dense images. We start with developing a new baseline capable of automatically generating instance-level point prompts via proposing a novel Instance Detection Module (IDM) that leverages density maps (DMs), encapsulating spatial distribution of objects in an image. To reduce false positives, we design the Point Prompt Selection Module (PPSM), which refines the output of IDM based on an adaptive threshold and spatial gating. Both IDM and PPSM seamlessly integrate into our model-agnostic framework. Furthermore, we introduce a feedback mechanism that enables PerSense to improve the accuracy of DMs by automating the exemplar selection process for DM generation. Finally, to advance research in this relatively underexplored area, we introduce PerSense-D, an evaluation benchmark for instance segmentation in dense images. Our extensive experiments establish PerSense's superiority over SOTA in dense settings.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
626,https://openreview.net/forum?id=Smfqvi8ZMZ,Beyond Gloss: A Hand-Centric Framework for Gloss-Free Sign Language Translation,"Sign Language Translation (SLT) is a challenging task that requires bridging the modality gap between visual and linguistic information while capturing subtle variations in hand shapes and movements. To address these challenges, we introduce \textbf{BeyondGloss}, a novel gloss-free SLT framework that leverages the spatio-temporal reasoning capabilities of Video Large Language Models (VideoLLMs). Since existing VideoLLMs struggle to model long videos in detail, we propose a novel approach to generate fine-grained, temporally-aware textual descriptions of hand motion. A contrastive alignment module aligns these descriptions with video features during pre-training, encouraging the model to focus on hand-centric temporal dynamics and distinguish signs more effectively. To further enrich hand-specific representations, we distill fine-grained features from HaMeR. Additionally, we apply a contrastive loss between sign video representations and target language embeddings to reduce the modality gap in pre-training.  \textbf{BeyondGloss} achieves state-of-the-art performance on the Phoenix14T and CSL-Daily benchmarks, demonstrating the effectiveness of the proposed framework. We will release the code upon acceptance of the paper.",6,5,~Xinyao_Sun2,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
627,https://openreview.net/forum?id=kVRxW3FJRk,TopoDiT-3D: Topology-Aware Diffusion Transformer with Bottleneck Structure for 3D Point Cloud Generation,"Recent advancements in Diffusion Transformer (DiT) models have significantly improved 3D point cloud generation. However, existing methods primarily focus on local feature extraction while overlooking global topological information, such as voids, which are crucial for maintaining shape consistency and capturing complex geometries. To address this limitation, we propose TopoDiT-3D, a Topology-Aware Diffusion Transformer with a bottleneck structure for 3D point cloud generation. Specifically, we design the bottleneck structure utilizing Perceiver Resampler, which not only offers a mode to integrate topological information extracted through persistent homology into feature learning, but also adaptively filters out redundant local features to improve training efficiency. Experimental results demonstrate that TopoDiT-3D outperforms state-of-the-art models in visual quality, diversity, and training efficiency. Furthermore, TopoDiT-3D demonstrates the importance of rich topological information for 3D point cloud generation and its synergy with conventional local feature learning.",5,4,~Abril_Corona-Figueroa1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
628,https://openreview.net/forum?id=K3S4IM7Vrs,MuDG: Taming Multi-modal Diffusion with Gaussian Splatting for Urban Scene Reconstruction,"Recent breakthroughs in radiance fields have significantly advanced 3D scene reconstruction and novel view synthesis (NVS) in autonomous driving. Nevertheless, critical limitations persist: reconstruction-based methods exhibit substantial performance deterioration under large viewpoint deviations from training trajectories, while generation-based techniques struggle with temporal coherence and precise scene controllability. To overcome these challenges, we present MuDG, an innovative framework that integrates Multi-modal Diffusion model with Gaussian Splatting (GS) for Urban Scene Reconstruction. MuDG leverages aggregated LiDAR point clouds with RGB and geometric priors to condition a multi-modal video diffusion model, synthesizing photorealistic RGB, depth, and semantic outputs for novel viewpoints. This synthesis pipeline enables feed-forward NVS without computationally intensive per-scene optimization,providing supervision to refine 3DGS representations for robust rendering under extreme viewpoint changes. Experiments on the Open Waymo Dataset demonstrate that MuDG outperforms existing methods in both reconstruction and synthesis quality.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
630,https://openreview.net/forum?id=lnJKnVbPZa,Incremental Multi-Scene Modeling via Continual Neural Graphics Primitives,"Neural radiance fields (NeRF) have revolutionized photorealistic rendering of novel views for 3D scenes. Despite their growing popularity and efficiency as 3D resources, NeRFs face scalability challenges due to the need for separate models per scene and the cumulative increase in training time for multiple scenes. The potential for incrementally encoding multiple 3D scenes into a single NeRF model remains largely unexplored. To address this, we introduce Continual-Neural Graphics Primitives (C-NGP), a novel continual learning framework that integrates multiple scenes incrementally into a single neural radiance field. Using a generative replay approach, C-NGP adapts to new scenes without requiring access to old data. We demonstrate that C-NGP can accommodate multiple scenes without increasing the parameter count, producing high-quality novel-view renderings on synthetic and real datasets. Notably, C-NGP models all $8$ scenes from the Real-LLFF dataset together, with only a $2.2\%$ drop in PSNR compared to vanilla NeRF, which models each scene independently. Further, C-NGP allows multiple style edits in the same network.",5,3,~Riccardo_Renzulli1|~Patrick_Fox-Roberts1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
632,https://openreview.net/forum?id=l2hfeVIAdD,Segmentation Assisted Incremental Test Time Adaptation in an Open World,"In dynamic environments, unfamiliar objects and distribution shifts are often encountered which challenge the generalization abilities of the deployed trained models. 
This work addresses Incremental Test Time Adaptation (ITTA) of Vision-Language Models (VLMs), tackling scenarios where unseen classes and unseen domains continuously appear during testing. 
Unlike traditional Test Time Adaptation approaches where the test samples come only from a predefined set of classes, our framework allows models to adapt simultaneously to both covariate and label shifts, actively incorporating new classes as they emerge.
Towards this goal, we establish a new benchmark for ITTA, integrating single-image TTA methods for VLMs with active labeling to query an oracle for samples potentially representing unseen classes during test time.
We propose a segmentation assisted active labeling module, termed SegAssist, which is training-free and repurposes the VLM’s segmentation capabilities to refine active sample selection, prioritizing samples likely to belong to unseen classes. Extensive experiments on several benchmark datasets demonstrate the potential of SegAssist to enhance the performance of VLMs in real-world scenarios, where continuous adaptation to emerging data is essential.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
644,https://openreview.net/forum?id=Ae7gFxVQM4,Solving Zero-Shot 3D Visual Grounding as Constraint Satisfaction Problems,"3D visual grounding (3DVG) aims to locate objects in a 3D scene with natural language descriptions. Supervised methods have achieved decent accuracy, but have a closed vocabulary and limited language understanding ability. Zero-shot methods mostly utilize large language models (LLMs) to handle natural language descriptions, yet suffer from slow inference speed. To address these problems, in this work, we propose a zero-shot method that reformulates the 3DVG task as a Constraint Satisfaction Problem (CSP), where the variables and constraints represent objects and their spatial relations, respectively. This allows a global reasoning of all relevant objects, producing grounding results of both the target and anchor objects. Moreover, we demonstrate the flexibility of our framework by handling negation- and counting-based queries with only minor extra coding efforts. Our system, Constraint Satisfaction Visual Grounding (CSVG), has been extensively evaluated on the public datasets ScanRefer and Nr3D datasets using only open-source LLMs. Results show the effectiveness of CSVG and superior grounding accuracy over current state-of-the-art zero-shot 3DVG methods with improvements of $+7.0\%$ (Acc@0.5 score) and $+11.2\%$ on the ScanRefer and Nr3D datasets, respectively. The code of our system is available at https://anonyleaf.github.io/csvg_bmvc.",6,4,~Runpei_Dong1|~Yun_Chang1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
646,https://openreview.net/forum?id=MTKkX3KP2K,Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework,"Deep learning in medical imaging is often limited by scarce and imbalanced annotated data. We propose a unified framework that integrates generative modeling and semi-supervised learning to improve data efficiency for both classification and segmentation. Class-conditional StyleGAN3 models generate synthetic samples to address class imbalance, while iterative pseudo-labeling enables weak supervision for segmentation. Experiments using ResNet-50 and VM-UNet across multiple datasets demonstrate consistent performance gains, highlighting the potential of synthetic data to alleviate annotation bottlenecks in medical image analysis.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
649,https://openreview.net/forum?id=jOQgzICgbN,Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes,"3D object detection plays a crucial role in autonomous systems, yet existing methods are limited by closed-set assumptions and struggle to recognize novel objects and their attributes in real-world scenarios. We propose OVODA, a novel framework enabling both open-vocabulary 3D object and attribute detection with no need to know the novel class anchor size. OVODA uses foundation models to bridge the semantic gap between 3D features and texts while jointly detecting attributes, e.g., spatial relationships, motion states, etc. To facilitate such research direction, we propose OVAD, a new dataset that supplements existing 3D object detection benchmarks with comprehensive attribute annotations. OVODA incorporates several key innovations, including foundation model feature concatenation, prompt tuning strategies, and specialized techniques for attribute detection such as perspective-specified prompts and horizontal flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets show that under the condition of no given anchor sizes of novel classes, OVODA outperforms the state-of-the-art methods in open-vocabulary 3D object detection while successfully recognizing object attributes.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
650,https://openreview.net/forum?id=FBEz7aZ9zr,Prompt-Informed Reinforcement Learning for Visual Coverage Path Planning,"Visual coverage path planning with unmanned aerial vehicles (UAVs) requires agents to strategically coordinate UAV motion and camera control to maximize coverage, minimize redundancy, and maintain battery efficiency. Traditional reinforcement learning (RL) methods rely on environment-specific reward formulations that lack semantic adaptability. This study proposes Prompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates the zero-shot reasoning ability and in-context learning capability of large language models with curiosity-driven RL. PIRL leverages semantic feedback from an LLM, GPT-3.5, to dynamically shape the reward function of the Proximal Policy Optimization (PPO) RL policy guiding the agent in position and camera adjustments for optimal visual coverage. The PIRL agent is trained using OpenAI Gym and evaluated in various environments. Furthermore, the sim-to-real-like ability and zero-shot generalization of the agent are tested by operating the agent in Webots simulator which introduces realistic physical dynamics. Results show that PIRL outperforms multiple learning-based baselines such as PPO with static rewards, PPO with exploratory weight initialization, imitation learning, and an LLM-only controller. Across different environments, PIRL outperforms the best-performing baseline by achieving up to 14\% higher visual coverage in OpenAI Gym and 27\% higher in Webots, up to 25\% higher battery efficiency, and up to 18\% lower redundancy, depending on the environment. The results highlight the effectiveness of LLM-guided reward shaping in complex spatial exploration tasks and suggest a promising direction for integrating natural language priors into RL for robotics.",5,4,~Byeonghwi_Kim1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
656,https://openreview.net/forum?id=uyjn4zO98S,3D Curvix: From Multiview 2D Edges to 3D Curve Segments,"The semantic reconstruction of a scene relies in part on the curvilinear structure inherent in images. The recovery of curvilinear structure is not only key to the representation of objects via ridges and other object curves but is also critical to the reconstruction from texture-poor images which lack a sufficient number of features. Prior methods advocate for the recovery of edges from multiple images which have shown very redundant in the 3D edge representation. This paper proposes 3D Curvix, a paradigm that consolidate redundant edges arising from hypotheses edge formation and multiview reconstruction. 3D Curvix organizes 3D edges by a weighted neighborhood graph which transforms parallel groups of redundant 3D edges into a one-dimensional manifold, followed by a connectivity graph construction which links 3D edges to form 3D curve segments. The result is a collection of clean, accurate 3D curves representing a sequence of dense 3D edges. Comprehensive experiments demonstrated that the proposed 3D Curvix provides remarkable 3D curves that can be used for a variety of real-world applications.",10,3,~Segun_Adebayo1|~Wei-Chao_Chen1|~Zoltan_Kato1|~Ruixuan_Yu1|~Nissim_Maruani1|~Yijun_Xiao2|~Alper_Yilmaz2,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
661,https://openreview.net/forum?id=PbjD6XvuvZ,Leveraging Sparsity for Efficient Inference of High-Resolution Vision Foundation Models,"Resolution scaling enhances the performance of Vision Transformers (ViTs) but incurs substantial computational costs due to the quadratic time complexity of self-attention. Our study reveals that sparsity naturally emerges in the attention maps of pre-trained vision encoders. Building on this observation, we introduce Sparse Vision Encoder (SVE), a post-training optimization framework that exploits sparsity to accelerate inference of vision encoders. SVE selectively applies sparsity in key layers, performs sparsity distillation to adapt models to sparse attention, and incorporates a lightweight predictor to eliminate redundant computations. Furthermore, we leverage cross-layer consistency in sparsity patterns, enabling efficient reuse of sparsity structures. Experiments on DINOv2, CLIP, and SAM2 show that SVE scales effectively for high-resolution encoding, delivering up to 80\% speedup while preserving model performance. This demonstrates that SVE is a scalable and cost-effective solution for high-resolution visual representation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
664,https://openreview.net/forum?id=HdXNFbuAmn,Canonical Makeup Transfer,"Makeup transfer aims at adapting makeup styles from reference images to source non-makeup images, typically portraits. Despite significant progress in this area, challenges persist, particularly in effectively transferring makeup across faces with notable pose or expression variations. In this study, we introduce a canonical makeup transfer (CMT) approach to reduce the disparity between the source and reference style faces, while ensuring semantic consistency in the transformation process. By minimizing this transfer gap, our method facilitates robust and efficient interaction between source and style facial images. Additionally, we establish a comprehensive benchmark for evaluating the performance of state-of-the-art methods across diverse scenarios. Extensive experiments demonstrate that our approach achieves superior results in terms of transfer quality and artifact reduction.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
666,https://openreview.net/forum?id=fF7Abq0Aok,Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization,"Multimodal Large Language Models (MLLMs) emerge as a unified interface to address a multitude of tasks, ranging from NLP to computer vision. Despite showcasing state-of-the-art results in many benchmarks, a long-standing issue is the tendency of MLLMs to hallucinate, that is to generate answers to the user's query that are not reflected in the visual input. In this paper, we address the problem of hallucinations as an alignment problem, seeking to steer the MLLM so that it prefers generating content without hallucinations. In contrast to recent approaches that require complicated pipelines to build synthetic preference data for alignment training, often relying on proprietary models, we capitalize on the well-known CHAIR metric, originally proposed to gauge the degree of hallucinations in image captioning. Given a pair of generated answers, we leverage CHAIR to distinguish winner and loser options (i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf MLLMs via Direct Preference Optimization (DPO). The resulting method, which we refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated answers on several hallucination benchmarks, demonstrating the effectiveness of fine-tuning the MLLM with a CHAIR-based reward.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
668,https://openreview.net/forum?id=Wjqqyp3fq4,MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression,"Deploying transformer-based neural networks on resource-constrained edge devices presents a significant challenge.
This is commonly addressed using techniques like low-rank approximation and mixed-precision quantization.
In this work, we introduce Mixed Low-Rank and Quantization (MLoRQ), a novel method that integrates both techniques. 
MLoRQ employs a two-stage optimization process to determine optimal bit-width and rank assignments for each layer, adhering to predefined memory constraints.
This process includes: 
(i) an intra-layer optimization that identifies potentially optimal compression solutions out of all low-rank and quantization combinations;
(ii) an inter-layer optimization that assigns bit-width precision and rank to each layer while ensuring the memory constraint is met.
An optional final step applies a sequential optimization process using a modified adaptive rounding technique to mitigate compression-induced errors in joint low-rank approximation and quantization.
The method is compatible and can be seamlessly integrated with most existing quantization algorithms.
MLoRQ shows state-of-the-art results with up to 15% performance improvement, evaluated on Vision Transformers for image classification, object detection and instance segmentation tasks.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
675,https://openreview.net/forum?id=EcaLrWPwoE,Jack of many Faces: A Step Towards Facial Expression and Physiological State Analysis with a Single Network,"Facial feature analysis, particularly dynamic facial expression recognition, is essential in computer vision for understanding human emotions, behaviors, and physiological states. However, existing approaches often exhibit limited performance, stemming from inadequate modelling of facial dynamics, noise sensitivity, ambiguous expression semantics, and are generally specific to single-task scenarios. To address these issues, we propose a compact 3D spatio-temporal network capable of handling both expression recognition and physiological state analysis. Our network includes two custom modules: (1) Contrastive Adversarial Efficient Local Channel Attention (ConAdv-ELCA), which extracts and disentangles fine-grained local facial features, and (2) Efficient Global Channel Attention (EGCA), to capture local-global interactions. Unlike prior work, which predominantly evaluates models on similar datasets within single-task domains, our work has demonstrated the ability to generalize across different tasks that are based on facial analysis. Experimental results demonstrate that our model consistently achieves state-of-the-art or near-state-of-the-art performance on blood alcohol concentration estimation, dynamic facial expression recognition, and driver fatigue detection.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
680,https://openreview.net/forum?id=Q9sjSUTJnS,On the Role of Individual Differences in Current Approaches to Computational Image Aesthetics,"Image aesthetic assessment (IAA) evaluates image aesthetics, a task complicated by image diversity and user subjectivity. Current approaches address this in two stages: Generic IAA (GIAA) models estimate mean aesthetic scores, while Personal IAA (PIAA) models adapt GIAA using transfer learning to incorporate user subjectivity. However, a theoretical understanding of transfer learning between GIAA and PIAA, particularly concerning the impact of group composition, group size, aesthetic differences between groups and individuals, and demographic correlations, is lacking. This work establishes a theoretical foundation for IAA, proposing a unified model that encodes individual characteristics in a distributional format for both individual and group assessments. We show that transferring from GIAA to PIAA involves extrapolation, while the reverse involves interpolation, which is generally more effective for machine learning. Experiments with varying group compositions, including sub-sampling by group size and disjoint demographics, reveal significant performance variation even for GIAA, indicating that mean scores do not fully eliminate individual subjectivity. Performance variations and Gini index analysis reveal education as the primary factor influencing aesthetic differences, followed by photography and art experience, with stronger individual subjectivity observed in artworks than in photos. Our model uniquely supports both GIAA and PIAA, enhancing generalization across demographics.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
690,https://openreview.net/forum?id=iU6Fo8yCZc,MonoGSDF: Exploring Monocular Geometric Cues for Gaussian Splatting-Guided Implicit Surface Reconstruction,"Accurate meshing from monocular images remains a key challenge in 3D vision. While state-of-the-art 3D Gaussian Splatting (3DGS) methods excel at synthesizing photorealistic novel views through rasterization-based rendering, their reliance on sparse, explicit primitives severely limits their ability to recover watertight and topologically consistent 3D surfaces. We introduce MonoGSDF, a novel method that couples Gaussian-based primitives with a neural Signed Distance Field (SDF) for high-quality reconstruction. During training, the SDF guides Gaussians' spatial distribution, while at inference, Gaussians serve as priors to reconstruct surfaces, eliminating the need for memory-intensive Marching Cubes. To handle arbitrary-scale scenes, we propose a scaling strategy for robust generalization. A multi-resolution training scheme further refines details and monocular geometric cues from off-the-shelf estimators enhance reconstruction quality. Experiments on real-world datasets show MonoGSDF outperforms prior methods while maintaining efficiency.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Jun Liu
694,https://openreview.net/forum?id=42VbOHiVvH,Occam’s LGS: An Efficient Approach for Language Gaussian Splatting,"Gaussian Splatting is a widely adopted approach for 3D scene representation, offering efficient, high-quality reconstruction and rendering. A key reason for its success is the simplicity of representing scenes with sets of Gaussians, making it interpretable and adaptable. To enhance understanding beyond visual representation, recent approaches extend Gaussian Splatting with semantic vision-language features, enabling open-set tasks. Typically, these language features are aggregated from multiple 2D views, however, existing methods rely on cumbersome techniques, resulting in high computational costs and longer training times.

In this work, we show that the complicated pipelines for language 3D Gaussian Splatting are simply unnecessary. Instead, we follow a probabilistic formulation of Language Gaussian Splatting and apply Occam's razor to the task at hand, leading to a highly efficient weighted multi-view feature aggregation technique. Doing so offers us state-of-the-art results with a speed-up of two orders of magnitude without any compression, allowing for easy scene manipulation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
698,https://openreview.net/forum?id=nwQLyX0gG9,SAMWave: Adapting Segment Anything Model to difficult tasks,"The emergence of large foundation models has propelled significant advances in various domains. The Segment Anything Model (SAM), a leading model for image segmentation, exemplifies these advances, outperforming traditional methods. However, such foundation models often suffer from performance degradation when applied to complex tasks for which they are not trained. 
Existing methods typically employ adapter-based fine-tuning strategies to adapt SAM for tasks and leverage high-frequency features extracted from the Fourier domain. However, Our analysis reveals that these approaches offer limited benefits due to constraints in their feature extraction techniques.
To overcome this, we propose \textbf{\textit{SAMwave}}, a novel and interpretable approach that utilizes the wavelet transform to extract richer, multi-scale high-frequency features from input data. Extending this, we introduce complex-valued adapters capable of capturing complex-valued spatial-frequency information via complex wavelet transforms. By adaptively integrating these wavelet coefficients, SAMwave enables SAM's encoder to capture information more relevant for dense prediction. Empirical evaluations on four challenging low-level vision tasks demonstrate that SAMwave significantly outperforms existing adaptation methods. This superior performance is consistent across both the SAM and SAM2 backbones and holds for both real and complex-valued adapter variants, highlighting the efficiency, flexibility, and interpretability of our proposed method for adapting segment anything models.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Jefersson A Dos Santos
700,https://openreview.net/forum?id=lkbfwhR0ax,Q-Align: Alleviating Attention Leakage in Zero-Shot Appearance Transfer via Query-Query Alignment,"We observe that zero-shot appearance transfer with large-scale image generation models faces a significant challenge: Attention Leakage. This challenge arises when the semantic mapping between two images is captured by the Query-Key alignment. To tackle this issue, we introduce Q-Align, utilizing Query-Query alignment to mitigate attention leakage and improve the semantic alignment in zero-shot appearance transfer. Q-Align incorporates three core contributions: (1) Query-Query alignment, facilitating the sophisticated spatial semantic mapping between two images; (2) Key-Value rearrangement, enhancing feature correspondence through realignment; and (3) Attention refinement using rearranged keys and values to maintain semantic consistency. We validate the effectiveness of Q-Align through extensive experiments and analysis, and Q-Align outperforms state-of-the-art methods in appearance fidelity while maintaining competitive structure preservation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
701,https://openreview.net/forum?id=pygmpsFtbz,Category-level Text-to-Image Retrieval Improved: Bridging the Domain Gap with Diffusion Models and Vision Encoders,"This work explores text-to-image retrieval for queries that specify or describe a semantic category. While vision-and-language models (VLMs) like CLIP offer a straightforward solution, they map text and images to distant regions in the representation space, limiting retrieval performance. To bridge this modality gap, we propose a two-step approach: first, we transform the text query into a visual query using a generative diffusion model; then, we estimate image-to-image similarity with a vision model. Additionally, we introduce an aggregation network that combines multiple generated images into a single vector representation and fuses similarity scores across both query modalities. Our approach leverages advancements in vision encoders, VLMs, and text-to-image generation models. Extensive evaluations show that it consistently outperforms retrieval methods relying solely on text queries.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
704,https://openreview.net/forum?id=80TO3tbQVN,JOG3R: Towards 3D-Consistent Video Generators,"Emergent capabilities of image generators have led to many impactful zero- or few-shot applications. Inspired by this success, we investigate whether video generators similarly exhibit 3D-awareness. Using structure-from-motion as a 3D-aware task, we test if intermediate features of a video generator (OpenSora in our case) can support camera pose estimation. Surprisingly, we only find a weak correlation between the two tasks. Deeper investigation reveals that although the video generator produces plausible video frames, the frames themselves are not truly 3D-consistent. Instead, we propose to jointly train for the two tasks, using photometric generation and 3D aware errors. Specifically, we find that SoTA video generation and camera pose estimation networks share common structures, and propose an architecture that unifies the two. The proposed unified model, named \nameMethod, produces camera pose estimates with competitive quality while producing 3D-consistent videos. In summary, we propose the first unified video generator that is
3D-consistent, generates realistic video frames, and can potentially be repurposed for other 3D-aware tasks.",5,4,~Andon_Tchechmedjiev1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
705,https://openreview.net/forum?id=wS85NIVHUk,ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation,"Estimating model accuracy on unseen, unlabeled datasets is crucial for real-world machine learning applications, especially under distribution shifts that can degrade performance. Existing methods often rely on predicted class probabilities (softmax scores) or data similarity metrics. While softmax-based approaches benefit from representing predictions on the standard simplex, compressing logits into probabilities leads to information loss. Meanwhile, similarity-based methods can be computationally expensive and domain-specific, limiting their broader applicability. In this paper, we introduce ALSA (Anchors in Logit Space for Accuracy estimation), a novel framework that preserves richer information by operating directly in the logit space. Building on theoretical insights and empirical observations, we demonstrate that the aggregation and distribution of logits exhibit a strong correlation with the predictive performance of the model. To exploit this property, ALSA employs an anchor-based modeling strategy: multiple learnable anchors are initialized in logit space, each assigned an influence function that captures subtle variations in the logits. This allows ALSA to provide robust and accurate performance estimates across a wide range of distribution shifts. Extensive experiments on vision, language, and graph benchmarks demonstrate ALSA’s superiority over both softmax- and similarity-based baselines. Notably, ALSA’s robustness under significant distribution shifts highlights its potential as a practical tool for reliable model evaluation. Code will be released upon acceptance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
707,https://openreview.net/forum?id=9K3cVqGuGS,Catch Your Concepts: A Flexible Concept Locator for Interpretable Visual Recognition,"The interpretability of visual recognition models has attracted significant attention, particularly with the advancement of concept-based methods exemplified by Concept Bottleneck Models (CBMs), which decompose categories as a set of representations to human perceivable concepts. While much of the prior research has contributed to extending concept sets to enhance the semantic interpretability of the models, the spatially activated regions of the concepts, or localization interpretability, have been routinely neglected. Based on insights from cognitive psychology, both types of interpretability are crucial for constructing explanations that enhance human understanding and trust. This paper introduces a flexible concept locator, which serves as a simple yet effective plugin for localizing concepts in CBMs during training, and providing both semantic and localization explanations during inference. A set of dynamic masks, applicable to various backbones and gradually shrinking during the training steps, is employed to constrain the activation region of concept neurons. This allows the concepts used for category recognition to be concentratedly expressed in specific spatial areas. Experiments conducted on multiple datasets demonstrate that the proposed locator effectively locates concepts in CBMs, offering additional localization explanations at a relatively low cost.",5,4,~Mengmeng_Ma1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
709,https://openreview.net/forum?id=PNLII7xIWs,TAPM-Net: Trajectory-Aware Perturbation Modeling for Infrared Small Target Detection,"Infrared small target detection (ISTD) remains a long-standing challenge due to weak signal contrast, limited spatial extent, and cluttered backgrounds. Despite performance improvements from convolutional neural networks (CNNs) and Vision Transformers (ViTs), current models lack the mechanism to trace how small targets trigger directional, layer-wise perturbations in the feature space—an essential cue for distinguishing signal from structured noise in infrared scenes. To address this limitation, we propose a Trajectory-Aware Mamba Propagation Network (TAMP-Net) that explicitly models the spatial diffusion behavior of target-induced feature disturbances. The proposed architecture is built upon two novel components: a Perturbation-guided Path Module (PGM) and a Trajectory-Aware State Block (TASB). The TGM constructs perturbation energy fields from multi-level features and extracts gradient-following trajectories that reflect the directionality of local responses. The resulting sequences are fed into the TASB, which is a Mamba-based state-space unit that models dynamic propagation along each path while incorporating velocity-constrained diffusion and semantic-aligned feature fusion from both word- and sentence-level embeddings. Unlike existing attention-based methods, our approach enables anisotropic, context-sensitive state transitions along spatial trajectories, maintaining global coherence at low computational cost. Experiments on NUAA-SIRST and IRSTD-1K demonstrate that TAMP-Net achieves SOTA performance in infrared small target detection.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
716,https://openreview.net/forum?id=mP5s529Ots,Size-aware Contrastive Imitation Learning for Language-conditioned Multi-task Robotic Manipulation,"Translating high-level linguistic instructions into geometrically consistent manipulation actions is a critical challenge in robotics, especially when dealing with objects of diverse geometric properties, such as size and shape. This task demands both linguistic comprehension and precise geometric reasoning from the robot agent. Previous approaches have primarily focused on enhancing visual precision and integrating language embeddings, often overlooking the alignment of subtle geometric features with linguistic representations. In this paper, we propose Size-Aware Contrastive Imitation Learning (SACIL), a novel framework that addresses this gap through two key components: Image-Text Contrast and Current-Goal Contrast. These components ensure the alignment of language embeddings with geometric features and maintain temporal consistency in size-aware reasoning across multi-step tasks. Additionally, we introduce a set of size-aware query tokens to effectively aggregate geometric features. Our experimental results demonstrate that SACIL significantly outperforms state-of-the-art methods, highlighting its potential to enhance size-sensitive reasoning and advance language-conditioned robotic manipulation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,2,1,Weak Accept (Poster),Accept,Zhixiang Chen
717,https://openreview.net/forum?id=c59AaOPs2N,From Open Vocabulary to Open World: Teaching Vision Language Models to Detect Novel Objects,"Traditional object detection methods operate under the closed-set assumption, where models can only detect a fixed number of objects predefined in the training set. Recent works on open vocabulary object detection (OVD) enable the detection of objects defined by an in-principle unbounded vocabulary, which reduces the cost of training models for specific tasks. However, OVD heavily relies on accurate prompts provided by an ``oracle'', which limits their use in critical applications such as driving scene perception. OVD models tend to misclassify near-out-of-distribution (NOOD) objects that have similar features to known classes, and ignore far-out-of-distribution (FOOD) objects. To address these limitations, we propose a framework that enables OVD models to operate in open world settings, by identifying and incrementally learning previously unseen objects. To detect FOOD objects, we propose Open World Embedding Learning (OWEL) and introduce the concept of Pseudo Unknown Embedding which infers the location of unknown classes in a continuous semantic space based on the information of known classes.  We also propose Multi-Scale Contrastive Anchor Learning (MSCAL), which enables the identification of misclassified unknown objects by promoting the intra-class consistency of object embeddings at different scales.  The proposed method achieves state-of-the-art performance on standard open world object detection and autonomous driving benchmarks while maintaining its open vocabulary object detection capability.",5,4,~Shazad_Ashraf1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Jun Liu
718,https://openreview.net/forum?id=j3hOWia47J,PADS: Plug-and-Play 3D Human Pose Analysis via Diffusion Generative Modeling,"Diffusion models have demonstrated impressive capabilities in modeling complex data distributions and are increasingly applied in various generative tasks. In this work, we propose Pose Analysis by Diffusion Synthesis (PADS), a unified generative modeling framework for 3D human pose analysis. PADS first learns a task-agnostic 3D pose prior via unconditional diffusion synthesis and then performs training-free adaptation to a wide range of pose analysis tasks, including 3D pose estimation, denoising, completion, etc., through a posterior sampling scheme. By formulating each task as an inverse problem with a known forward operator, PADS injects task-specific constraints during inference while keeping the pose prior fixed. This plug-and-play framework removes the need for task-specific supervision or retraining, offering flexibility and scalability across diverse conditions. Extensive experiments on different benchmarks showcase the superior performance against both learning-based and optimization-based baselines, demonstrating the effectiveness and generalization capability of our method.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
721,https://openreview.net/forum?id=lDPqsSy5R5,Gromov Wasserstein Optimal Transport for Semantic Correspondences,"Establishing correspondences between image pairs is a long studied task in computer vision. With recent large-scale foundation models showing strong zero-shot performance on downstream tasks including classification and segmentation, there has been interest in using the internal feature maps of these models for the semantic correspondence task. Recent works observe that features from DINOv2 and Stable Diffusion (SD) are complementary, the former producing accurate but sparse correspondences, while the latter produces spatially consistent correspondences. As a result, the current state-of-the-art methods for semantic correspondence involves combining features from both models in an ensemble. While the performance of these methods is impressive, they are computationally expensive, requiring evaluating feature maps for large-scale foundation models. In this work we take a different approach, instead replacing SD features with a superior matching algorithm which is imbued with the desirable spatial consistency property. Specifically, we replace the standard nearest neighbours matching with an optimal transport algorithm that includes a Gromov Wasserstein spatial smoothness prior. We show that we can significantly boost the performance of the DINOv2 baseline, and be competitive and sometimes surpassing state-of-the-art methods using Stable Diffusion features, while being 5--10x more efficient.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
722,https://openreview.net/forum?id=4ZYZ8Xvdht,Unsupervised Multimodal Deepfake Detection Through Explicit Intra-Modal and Cross-Modal Inconsistency Discovery,"Deepfake videos present an increasing threat to society with potentially negative impact on criminal justice, democracy, and personal safety and privacy. Meanwhile, detecting deepfakes, at scale, remains a very challenging task that often requires labeled training data from existing deepfake generation methods. Further, even the most accurate supervised deepfake detection methods do not generalize to deepfakes generated using new generation methods. In this paper, we propose a novel unsupervised method for detecting deepfake videos by explicitly discovering intra-modal and cross-modal inconsistencies between video segments. The fundamental hypothesis behind our proposed method is that motion or identity inconsistencies are inevitable in motion-transfer deepfake videos. We provide the first mathematical and empirical evidence to support this hypothesis, hence justifying the generalizability of our method. Our proposed method outperforms prior state-of-the-art unsupervised deepfake detection methods on the challenging FakeAVCeleb dataset, and also has several additional advantages: it is scalable because it does not require pristine (real) samples for each identity during inference and therefore can apply to arbitrarily many identities; generalizable because it is trained only on real videos and therefore does not rely on a particular deepfake method; reliable because it does not rely on any likelihood estimation in high dimensions; and explainable because it can pinpoint the exact location of modality inconsistencies.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
732,https://openreview.net/forum?id=eH4rc6RDs4,TryOffDiff: Virtual-Try-Off via High-Fidelity Garment Reconstruction using Diffusion Models,"This paper introduces Virtual Try-Off (VTOFF), a novel task generating standardized garment images from single photos of clothed individuals. Unlike Virtual Try-On (VTON), which digitally dresses models, VTOFF extracts canonical garment images, demanding precise reconstruction of shape, texture, and complex patterns, enabling robust evaluation of generative model fidelity. We propose TryOffDiff, adapting Stable Diffusion with SigLIP-based visual conditioning to deliver high-fidelity reconstructions. Experiments on VITON-HD and Dress Code datasets show that TryOffDiff outperforms adapted pose transfer and VTON baselines. We observe that traditional metrics such as SSIM inadequately reflect reconstruction quality, prompting our use of DISTS for reliable assessment. Our findings highlight VTOFF’s potential to improve e-commerce product imagery, advance generative model evaluation, and guide future research on high-fidelity reconstruction. Demo, code, and models are available at: https://rizavelioglu.github.io/tryoffdiff",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
736,https://openreview.net/forum?id=0W2qwBDt70,"EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models","Recent advances in creative AI have enabled the synthesis of high-fidelity images and videos conditioned on language instructions. Building on these developments, text-to-video diffusion models have evolved into embodied world models (EWMs) capable of generating physically plausible scenes from language commands, effectively bridging vision and action in embodied AI applications. This work addresses the critical challenge of evaluating EWMs beyond general perceptual metrics to ensure the generation of physically grounded and action-consistent behaviors. We propose the Embodied World Model Benchmark (\Ours), a dedicated framework designed to evaluate EWMs based on three key aspects: visual scene consistency, motion correctness, and semantic alignment. Our approach leverages a meticulously curated dataset encompassing diverse scenes and motion patterns, alongside a comprehensive multi-dimensional evaluation toolkit, to assess and compare candidate models. The proposed benchmark not only identifies the limitations of existing video generation models in meeting the unique requirements of embodied tasks but also provides valuable insights to guide future advancements in the field. The dataset, evaluation tools will be open-sourced at https://anonymous.4open.science/r/EWMBench-617F.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
737,https://openreview.net/forum?id=vLoeDoRr9y,Self-Intersection-Aware 3D Human Motion Generation Using an Efficient Human Sphere Proxy,"Human motion generation has made tremendous progress in recent years, with state-of-the-art approaches surpassing ground truth data in leading evaluation benchmarks. However, visual inspection of the generated motions paints a different picture. Even state-of-the-art approaches generate motions frequently containing self-intersections, i.e., body parts interpenetrating, which are strong artifacts, severely limiting the perceived motion quality.
We introduce a novel loss, which explicitly penalizes self-intersections, to the training of human motion generation methods. We base our loss on a sphere proxy of human geometry, which allows us to calculate a self-intersection loss 98% faster and uses 83% less memory than comparable methods based on triangular meshes. The loss is agnostic to the specific approach, and we add it to the training of the recent human motion generation methods human motion diffusion model (MDM) and MoMask. Our extensive experiments show a reduction of self-intersections in generated motions of up to 49% while improving other evaluation metrics.
The code will be released after publication.",5,3,~Amit_Kumar5|~Jiangbei_Yue1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
745,https://openreview.net/forum?id=Y0cYTb63xO,Mapping like a Skeptic: Probabilistic BEV Projection for Online HD Mapping,"Constructing high-definition (HD) maps from sensory input requires accurately mapping the image space to the Bird's Eye View (BEV) space. The precision of this mapping directly impacts the quality of the final vectorized HD map. Existing HD mapping approaches outsource the projection to standard mapping techniques, such as attention-based ones. However, these methods struggle with accuracy due to generalization problems, often hallucinating nonexisting road elements. Our key idea is to start with a geometric mapping based on camera parameters and adapt it to the scene to extract relevant information from camera images. To implement this, we propose a novel probabilistic projection mechanism with confidence scores to (i) refine the mapping to better align with the scene and (ii) filter out irrelevant elements that should not influence HD map generation. In addition, we improve temporal processing by using confidence scores to selectively accumulate reliable information over time. Experiments on new splits of the nuScenes and Argoverse2 datasets demonstrate improved performance over state-of-the-art approaches, indicating better generalization. The improvements are particularly pronounced on nuScenes and in the challenging long perception range. We will share the code upon publication.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
751,https://openreview.net/forum?id=TUeuD8zCR3,TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification,"Congenital heart defect (CHD) detection in ultrasound videos is hindered by image noise and probe positioning variability. While automated methods can reduce operator dependence, current machine learning approaches often neglect temporal information, limit themselves to binary classification, and do not account for prediction calibration. We propose Temporal Prompt Alignment (TPA), a method leveraging foundation image-text model and prompt-aware contrastive learning to classify fetal CHD on cardiac ultrasound videos. TPA extracts features from each frame of video subclips using an image encoder, aggregates them with a trainable temporal extractor to capture heart motion, and aligns the video representation with class-specific text prompts via a margin-hinge contrastive loss. To enhance calibration for clinical reliability, we introduce a Conditional Variational Autoencoder Style Modulation (CVAESM) module, which learns a latent style vector to modulate embeddings and quantifies classification uncertainty. Evaluated on a private dataset for CHD detection and on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPA achieves state-of-the-art macro F1 scores of 85.40\% for CHD diagnosis, while also reducing expected calibration error by 5.38\% and adaptive ECE by 6.8\%. On EchoNet-Dynamic’s three-class task, it boosts macro F1 by 4.73\% (from 53.89\% to 58.62\%). The code will be provided upon acceptance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
753,https://openreview.net/forum?id=1DGdmMftLO,Boosting Camera Motion Control for Video Diffusion Transformers,"Despite recent advancements in camera control methods for U-Net based video diffusion models, these methods have been shown to be ineffective for transformer-based diffusion models (DiT). In this paper, we investigate the underlying causes of this issue and propose solutions. Our study reveals that camera control performance depends heavily on the choice of conditioning methods, rather than on camera pose representations, as is commonly believed. To address the persistent motion degradation in DiT, we introduce Camera Motion Guidance (CMG), a classifier-free guidance approach that boosts camera motion by over 400%. Additionally, we present a sparse camera control pipeline that improves training data efficiency and simplifies the process of specifying camera poses for long videos.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
755,https://openreview.net/forum?id=A3XG8Yj8J3,Audio-Guided Visual Editing with Complex Multi-Modal Prompts,"Visual editing with diffusion models has made significant progress but often struggles with complex scenarios that textual guidance alone could not adequately describe, highlighting the need for additional non-text editing prompts. In this work, we introduce a novel audio-guided visual editing framework that can handle complex editing tasks with multiple text and audio prompts without requiring additional training. Existing audio-guided visual editing methods often necessitate training on specific datasets to align audio with text, limiting their generalization to real-world situations. We leverage a pre-trained multi-modal encoder with strong zero-shot capabilities and integrate diverse audio into visual editing tasks, by alleviating the discrepancy between the audio encoder space and the diffusion model's prompt encoder space. Additionally, we propose a novel approach to handle complex scenarios with multiple and multi-modal editing prompts through our separate noise branching and adaptive patch selection. Our comprehensive experiments on diverse editing tasks demonstrate that our framework excels in handling complicated editing scenarios by incorporating rich information from audio, where text-only approaches fail.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
762,https://openreview.net/forum?id=NGrqOSJPqS,Learning a Neural Association Network for Self-supervised Multi-Object Tracking,"This paper introduces a novel framework to learn data association for multi-object tracking in a self-supervised manner. Fully-supervised learning methods are known to achieve excellent tracking performances, but acquiring identity-level annotations is tedious and time-consuming. Motivated by the fact that in real-world scenarios object motion can be usually represented by a Markov process, we present a novel expectation maximization (EM) algorithm that trains a neural network to associate detections for tracking, without requiring prior knowledge of their temporal correspondences. At the core of our method lies a neural Kalman filter, with an observation model conditioned on associations of detections parameterized by a neural network. Given a batch of frames as input, data associations between detections from adjacent frames are predicted by a neural network followed by a Sinkhorn normalization that determines the assignment probabilities of detections to states. Kalman smoothing is then used to obtain the marginal probability of observations given the inferred states, producing a training objective to maximize this marginal probability using gradient descent. The proposed framework is fully differentiable, allowing the underlying neural model to be trained end-to-end. We evaluate our approach on the challenging MOT17, MOT20, and BDD100K datasets and achieve state-of-the-art results in comparison to self-supervised trackers using public detections.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
766,https://openreview.net/forum?id=3fc8tTENkO,Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing,"Text-based editing diffusion models exhibit limited performance when the user's input instruction is ambiguous. To solve this problem, we propose Specify ANd Edit (SANE), a zero-shot inference pipeline for diffusion-based editing systems. We use a large language model (LLM) to decompose the input instruction into specific instructions, i.e. well-defined interventions to apply to the input image to satisfy the user's request. We benefit from the LLM-derived instructions along the original one, thanks to a novel denoising guidance strategy specifically designed for the task. Our experiments with three baselines and on two datasets demonstrate the benefits of SANE in all setups. Moreover, our pipeline improves the interpretability of editing models, and boosts the output diversity. Our code will be public.",5,3,~Marcus_A_Brubaker1|~Zifu_Wan1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
768,https://openreview.net/forum?id=FmclKdFSaW,Interpretable Text-Guided Image Clustering via Iterative Search,"Traditional clustering methods aim to group unlabeled data points based on their similarity to each other. However, clustering, in the absence of additional information, is an ill-posed problem as there may be many different, yet equally valid, ways to partition a dataset. Different users may want to use different criteria to form clusters in the same data, eg shape vs color. Recently introduced text-guided image clustering methods aim to address this ambiguity by allowing users to specify the criteria of interest using natural language instructions. This instruction provides the necessary context and control needed to obtain clusters that are more aligned with the users' intent.
We propose a new text-guided clustering approach named ITGC that uses an iterative discovery process, guided by an unsupervised clustering objective, to generate interpretable visual concepts that better capture the criteria expressed in a user's instructions.
We report superior performance compared to existing methods across a wide variety of image clustering and fine-grained classification benchmarks.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Jefersson A Dos Santos
770,https://openreview.net/forum?id=LQrb3ULcJY,DTFSal: Audio-Visual Dynamic Token Fusion for Video Saliency Prediction,"Audio-visual saliency prediction aims to mimic human visual attention by identifying salient regions in videos through the integration of both visual and auditory information. Although visual-only approaches have significantly advanced, effectively incorporating auditory cues remains challenging due to complex spatio-temporal interactions and high computational demands. To address these challenges, we propose Dynamic Token Fusion Saliency (DFTSal), a novel audio-visual saliency prediction framework designed to balance accuracy with computational efficiency. Our approach features a multi-scale visual encoder equipped with two novel modules: the Learnable Token Enhancement Block (LTEB), which adaptively weights tokens to emphasize crucial saliency cues, and the Dynamic Learnable Token Fusion Block (DLTFB), which employs a shifting operation to reorganize and merge features, effectively capturing long-range dependencies and detailed spatial information. In parallel, an audio branch processes raw audio signals to extract meaningful auditory features. Both visual and audio features are integrated using our Adaptive Multimodal Fusion Block (AMFB), which employs local, global, and adaptive fusion streams for precise cross-modal fusion. The resulting fused features are processed by a hierarchical multi-decoder structure, producing accurate saliency maps. Extensive evaluations on six audio-visual benchmarks demonstrate that DFTSal achieves SOTA performance while maintaining computational efficiency.",6,3,~Sung-Ho_Bae1|~Mustansar_Fiaz1|~Wei_Quan2,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
782,https://openreview.net/forum?id=1fT6vDa4vW,Is Safety Checker Still Safe? A Study on the Covert NSFW Text,"Recently, with the assist of generative models, one can generate highly realistic images with arbitrary content. However, models learn a large volume of inappropriate content from dataset, which leads to the dissemination of inappropriate content. In addition to intuitive nappropriate content in images, we discover for the first time that normal objects can be utilized to subtly hide offensive text in images. To
explore the risks of such content, we propose a generation method, Covert NSFW Text (CNT), which can insert the offensive text into generated images. And these images can effectively cheat the Safety Checker. We establish a novel Covert NSFW Text Dataset (CNTD) based on CNT and release it for further research. In response to this type of content, we propose a detection method, Enhanced Safety Checker (ESC). ESC is training-free and can effectively detect both the covert offensive text and any inappropriate content that Safety Checker can detect. Experimental results demonstrate the effectiveness of CNT and ESC.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
784,https://openreview.net/forum?id=4PU5PqbpuA,UDT : Unsupervised Discovery of Transformations between Fine-Grained Classes in Diffusion Models,"Diffusion models demonstrate excellent performance in image generation and synthesis. Effective and controllable editing with these models requires a deep understanding of their latent spaces, a primary focus of much prior research. However, existing unsupervised exploration methods often remain at attribute-level edits, revealing limitations in complex fine-grained class transformations. To address this, we propose UDT (Unsupervised Discovery of Transformations), a novel framework that, despite its unsupervised nature, supports fine-grained class transformations by structuring the latent space in a hierarchy-aware manner. UDT systematically constructs the latent space by incorporating parent class guidance, ensuring semantic consistency across class transformations while preserving pose. Our hierarchy-informed contrastive learning disentangles class-defining traits, enabling the discovery of diverse and meaningful fine-grained transformation directions. Experiments on dog, cat, and flower datasets demonstrate that UDT's superior performance over existing methods in generating coherent, diverse, and semantically accurate edits. These results suggest UDT is a scalable and effective approach for semantic latent space exploration in diffusion models.",5,4,~Ori_Maoz1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
786,https://openreview.net/forum?id=1Lj0UuWUVI,TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding,"Video Temporal Grounding (VTG) aims to extract relevant video segments based on a given natural language query.  Recently, zero-shot VTG methods have gained attention by leveraging pretrained vision-language models (VLMs) to localize target moments without additional training. However, existing approaches suffer from semantic fragmentation, where temporally continuous frames sharing the same semantics are split across multiple segments. When segments are fragmented, it becomes difficult to predict an accurate target moment that aligns with the text query. Also, they rely on skewed similarity distributions for localization, making it difficult to select the optimal segment. Furthermore, they heavily depend on the use of LLMs which require expensive inferences.To address these limitations, we propose a TAG, a simple yet effective Temporal-Aware approach for zero-shot video temporal Grounding, which incorporates temporal pooling, temporal coherence clustering, and similarity adjustment. Our proposed method effectively captures the temporal context of videos and addresses distorted similarity distributions without training. Our approach achieves state-of-the-art results on Charades-STA and ActivityNet Captions benchmark datasets without rely on LLMs.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
787,https://openreview.net/forum?id=yGDpwnPrs3,"${C}^{3}$-GS: Learning Context-aware, Cross-dimension, Cross-scale Feature for Generalizable Gaussian Splatting","Generalizable Gaussian Splatting aims to synthesize novel views for unseen scenes without per-scene optimization. In particular, recent advancements utilize feed-forward networks to predict per-pixel Gaussian parameters, enabling high-quality synthesis from sparse input views. However, existing approaches fall short in encoding discriminative, multi-view consistent features for Gaussian predictions, which struggle to construct accurate geometry with sparse views. To address this, we propose $\mathbf{C}^{3}$-GS, a framework that enhances feature learning by incorporating context-aware, cross-dimension, and cross-scale constraints. Our design integrates three lightweight modules into a unified rendering pipeline, improving feature fusion and enabling photorealistic synthesis without requiring additional supervision. Extensive experiments on benchmark datasets validate that $\mathbf{C}^{3}$-GS achieves state-of-the-art rendering quality and generalization ability.",6,4,~Joao_F._Henriques1|~Marius_Leordeanu1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
788,https://openreview.net/forum?id=zVAVtPxcPH,Vision Backbone Efficient Selection for Image Classification in Low-Data Regimes,"Transfer learning has become an essential tool in modern computer vision, allowing practitioners to leverage backbones, pretrained on large datasets, to train successful models from limited annotated data. Choosing the right backbone is crucial, especially for small datasets, since final performance depends heavily on the quality of the initial feature representations. While prior work has conducted benchmarks across various datasets to identify universal top-performing backbones, we demonstrate that backbone effectiveness is highly dataset-dependent, especially in low-data scenarios where no single backbone consistently excels. To overcome this limitation, we introduce dataset-specific backbone selection as a new research direction and investigate its practical viability in low-data regimes. Since exhaustive evaluation is computationally impractical for large backbone pools, we formalize Vision Backbone Efficient Selection (VIBES) as the problem of searching for high-performing backbones under computational constraints. We define the solution space, propose several heuristics, and demonstrate VIBES feasibility for low-data image classification by performing experiments on four diverse datasets. Our results show that even simple search strategies can find well-suited backbones within a pool of over $1300$ pretrained models, outperforming generic benchmark recommendations within just ten minutes of search time on a single GPU (NVIDIA RTX A5000)",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
811,https://openreview.net/forum?id=n1hwECDww0,DEAD: Data-Efficient Audiovisual Dubbing using Neural Rendering Priors,"Visual dubbing is the process of generating lip motions of an actor in a video to synchronize with given audio. Visual dubbing allows video-based media to reach global audiences. Recent advances have made progress towards realizing this goal. However, existing person-specific models see only one frame of the actor and, therefore, lack the ability to capture identity in the form of characteristic motion and related idiosyncracies, or they are expensive methods requiring off-putting large data requirements and costly model training. Our key insight is to train a large, multi-person prior network, which can then be adapted to new users. This method allows for $\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors. We show that we achieve state-of-the-art in terms of $\textbf{visual quality}$ and $\textbf{recognizability}$ both quantitatively and qualitatively through two user studies. Our prior learning and adaptation method $\textbf{is able to adapt to small datasets better than baselines}$. Our experiments on real-world, limited data scenarios find that our model is preferred over baseline models.",5,4,~Soon_Yau_Cheong1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
824,https://openreview.net/forum?id=CQYsFRJLXb,A Unified Framework for High-Frame-Rate HDR Video Synthesis,"Capturing high dynamic range (HDR) video at high frame rates is critical for applications such as cinematic production and autonomous systems, yet it remains challenging with conventional cameras. We present the first unified framework that jointly performs HDR reconstruction and temporal interpolation from sequences with alternating exposures. Unlike prior work that reconstructs only middle frames or uses heavy off-the-shelf interpolators, our lightweight network synthesizes HDR video at arbitrary timesteps in real time on mid-range GPUs. To support this task, we introduce a new dataset of exposure-bracketed video sequences with real-world motion. To reduce reliance on ground-truth HDR data, we also propose a novel self-supervised training scheme that delivers competitive results. Experiments show our approach outperforms existing baselines in efficiency while achieving comparable or better visual quality, establishing a new benchmark for practical HDR video synthesis.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
825,https://openreview.net/forum?id=SIZlHfeovt,Modular Embedding Recomposition for Incremental Learning,"The advent of pre-trained Vision-Language Models (VLMs) has significantly transformed Continual Learning (CL), mainly due to their zero-shot classification abilities. Such proficiency makes VLMs well-suited for real-world applications, enabling robust performance on novel unseen classes without requiring adaptation. However, fine-tuning remains essential when downstream tasks deviate significantly from the pre-training domain. Prior CL approaches primarily focus on preserving the zero-shot capabilities of VLMs during incremental fine-tuning on a downstream task. We take a step further by leveraging the data not only to preserve but also to enhance the zero-shot capabilities of VLMs. Our approach, named MoDular Embedding Recomposition (MoDER), introduces a modular framework that trains multiple textual experts, each specialized in a single seen class, and stores them in a foundational hub. At inference time, for each unseen class, we query the hub and compose the retrieved experts to synthesize a refined prototype that improves classification. We show the effectiveness of our method across two popular zero-shot incremental protocols, Class-IL and MTIL, comprising a total of 14 datasets.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
827,https://openreview.net/forum?id=s25P6oHbCk,MonoTracker: Monocular RGB-Only 6D Tracking of Unknown Object,"Estimating the six degrees of freedom (6D) pose of unknown objects using only monocular RGB images is a challenging task, especially when dealing with textureless and small objects. In this paper, we propose a novel pipeline, MonoTracker, for 6D object pose estimation and tracking that operates without any prior depth information. MonoTracker is a model-free, RGB-only, 6D detector that works on unseen objects. It leverages state-of-the-art pre-trained deep learning models, enabling zero-shot 6D pose estimation by jointly optimizing object poses and correcting scale inconsistencies in monocular depth predictions. We validate our method on three public datasets -- YCBInEOAT, HO3D, and BEHAVE -- demonstrating significant improvements over the state of the art. As a downstream application, we also show that the estimated camera poses can be used as input in NeRF pipelines, facilitating novel-view synthesis. Our results highlight the potential of monocular RGB inputs for accurate 6D object tracking and reconstruction in real-world scenarios. The code will be made public.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Zhixiang Chen
828,https://openreview.net/forum?id=xazfdhh6Jy,Leveraging Modality Tags for Enhanced Cross-Modal Video Retrieval,"Video retrieval requires aligning visual content with corresponding natural language descriptions. In this paper, we introduce Modality Auxiliary Concepts for Video Retrieval (MAC-VR), a novel approach that leverages modality-specific tags -- automatically extracted from foundation models -- to enhance video retrieval.
We propose to align modalities in a latent space, along with learning and aligning auxiliary latent concepts derived from the features of a video and its corresponding caption.
We introduce these auxiliary concepts to improve the alignment of visual and textual latent concepts, allowing us to distinguish concepts from one another. 

We conduct extensive experiments on five diverse datasets: MSR-VTT, DiDeMo, TGIF, Charades and YouCook2. The experimental results consistently demonstrate that modality-specific tags improve cross-modal alignment, outperforming current state-of-the-art methods across three datasets and performing comparably or better across others.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
831,https://openreview.net/forum?id=CHZGRDAced,LOGen: Toward LiDAR Object Generation by Point Diffusion,"The generation of LiDAR scans is a growing topic with diverse applications to autonomous driving. However, scan generation remains challenging, especially when compared to the rapid advancement of image and 3D object generation. We consider the task of LiDAR object generation, requiring models to produce 3D objects as viewed by a LiDAR scan. It focuses LiDAR scan generation on a key aspect of scenes, the objects, while also benefiting from advancements in 3D object generative methods. We introduce a novel diffusion-based model to produce LiDAR point clouds of dataset objects, including intensity, and with an extensive control of the generation via conditioning information. Our experiments on nuScenes and KITTI-360 show the  quality of our generations measured with new 3D metrics developed to suit LiDAR objects.",6,4,~Hao_Zhao1|~Zhouhui_Lian1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
832,https://openreview.net/forum?id=bQpWLAxGoX,SIMULDITEX: Single Image Multiscale & Lightweight Diffusion for Texture Modelling,"We propose SIMuLDiTex, a Single Image Multi-scale and Light-weight Diffusion Texture model. While traditional patch-based methods are fast, they often fail to preserve complex texture details and generalize from limited examples. Recent generative models, though effective, suffer from high computational costs and memory requirements. Our approach addresses these challenges by employing a coarse-to-fine strategy that accelerates sampling and maintains high-resolution fidelity without the need for auto-encoders. We introduce a scale conditioning mechanism, enabling the use of a single network with only one million parameters. Experiments demonstrate that SIMuLDiTex outperforms existing methods in speed, quality, and scalability, offering a practical solution for real-time, high-resolution texture generation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
835,https://openreview.net/forum?id=x9g35A9QTo,Cloud-Stereo: A Dataset and Benchmark for Reconstructing Atmospheric Clouds from Stereo Images,"Obtaining accurate measurements of clouds is a critical problem in atmospheric physics, as accurate modeling of cloud properties allows us to better understand and predict climate change.  Stereo camera networks have shown promise in obtaining such measurements, being able to reconstruct detailed cloud fields over multi-km$^2$ domains. However, previous studies on cloud stereo depth estimation have been limited to using traditional (non-learned) matching techniques, due to the absence of suitable training datasets for this challenging domain. In this work, we present a novel dataset (Cloud-Stereo) specifically tailored for cloud depth estimation. The Cloud-Stereo dataset includes: 1) a synthetic dataset for training, comprising 3000 stereo pairs and simulated dense LiDAR depth data, and 2) a high-accuracy real-world dataset consisting of $\approx 120k$ frames acquired from a stereo camera and Doppler Aerosol LiDAR for testing. Using our dataset we benchmark existing learning and non-learning based stereo depth estimation approaches, and demonstrate that fine-tuning on our dataset can lead to significant accuracy improvement for learned methods. We believe this dataset will enable the training of future, more accurate, methods for cloud field reconstruction, enhancing a unique measurement capability for developing and evaluating atmospheric models.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
850,https://openreview.net/forum?id=5V4n1Tu6j3,C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression,"Neural network compression has gained increasing attention in recent years, particularly in computer vision applications, where the need for model reduction is crucial for overcoming deployment constraints. Pruning is a widely used technique that prompts sparsity in model structures, e.g. weights, neurons, and layers, reducing size and inference costs. Structured pruning is especially important as it allows for the removal of entire structures, which further accelerates inference time and reduces memory overhead. However, it can be computationally expensive, requiring iterative retraining and optimization. To overcome this problem, recent methods considered one-shot setting, which applies pruning directly at post-training. Unfortunately, they often lead to a considerable drop in performance. In this paper, we focus on this issue by proposing a novel one-shot pruning framework that relies on explainable deep learning. First, we introduce a causal-aware pruning approach that leverages cause-effect relations between model predictions and structures in a progressive pruning process. It allows us to efficiently reduce the size of the network, ensuring that the removed structures do not deter the performance of the model. Then, through experiments conducted on convolution neural network and vision transformer baselines, pre-trained on classification tasks, we demonstrate that our method consistently achieves substantial reductions in model size, with minimal impact on performance, and without the need for fine-tuning. Overall, our approach outperforms its counterparts, offering the best trade-off.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
851,https://openreview.net/forum?id=xsRY0T4Pgq,Asymmetric Event-Image Stereo with Temporal Feature Gating and Iterative Structure-Detail Refinement,"This paper studies disparity estimation using asymmetric stereo inputs, combining event streams and conventional frame-based images. Past works directly calculate the similarity between features from the two modalities for making predictions, neglecting the inherent domain shift between them. First, event data are both temporally and spatially aware, while a frame is spatially aware. Second, event data are well-suited for capturing scene structure, whereas frame-based images excel at preserving fine-grained local visual details.
To address these challenges, our method introduces two key components:
(1) We enhance frame features by gating them with temporal cues extracted from consecutive frames, which improves their temporal awareness for more accurate similarity computation.
(2) We perform disparity estimation using an iterative refinement scheme that alternates between updating structure and local features. This process starts with reliable structure features and progressively refining the disparity prediction by integrating fine detail.
Extensive experiments on the MVSEC and DSEC datasets show that our method significantly outperforms prior state-of-the-art approaches.",5,4,~Zhigang_Chang1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
852,https://openreview.net/forum?id=4FMRmK6PbV,LoFT: LoRA-fused Training Dataset Generation with Few-shot Guidance,"Despite recent advances in text-to-image generation, using synthetically generated data seldom brings a significant boost in performance for supervised learning. Oftentimes, synthetic datasets do not faithfully recreate the data distribution of real data, i.e., they lack the fidelity or diversity needed for effective downstream model training. While previous work has employed few-shot guidance to address this issue, existing methods still fail to capture and generate features unique to specific real images. In this paper, we introduce a novel dataset generation framework named LoFT, LoRA-Fused Training-data Generation with Few-shot Guidance. Our method fine-tunes LoRA weights on individual real images and fuses them at inference time, producing synthetic images that combine the features of real images for improved diversity and fidelity of generated data. We evaluate the synthetic data produced by LoFT on 10 datasets, using 8 to 64 real images per class as guidance and scaling up to 1000 images per class. Our experiments show that training on LoFT-generated data consistently outperforms other synthetic dataset methods, significantly increasing accuracy as the dataset size increases. Additionally, our analysis demonstrates that LoFT generates datasets with high fidelity and sufficient diversity, which contribute to the performance improvement.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
857,https://openreview.net/forum?id=Fi1tgTdgSJ,Lost in Time: A New Temporal Benchmark for VideoLLMs,"Large language models have demonstrated impressive performance when integrated with vision models even enabling video understanding. However, evaluating video models presents its own unique challenges, for which several benchmarks have been proposed. In this paper, we show that the currently most used video-language benchmarks can be solved without requiring much temporal reasoning. We identified three main issues in existing datasets: (i) static information from single frames is often sufficient to solve the tasks (ii) the text of the questions and candidate answers is overly informative, allowing models to answer correctly without relying on any visual input (iii) world knowledge alone can answer many of the questions, making the benchmarks a test of knowledge replication rather than video reasoning. In addition, we found that open-ended question-answering benchmarks for video understanding suffer from similar issues while the automatic evaluation process with LLMs is unreliable, making it an unsuitable alternative. As a solution, we propose TVBench, a novel open-source video multiple-choice question-answering benchmark, and demonstrate through extensive evaluations that it requires a high level of temporal understanding. Surprisingly, we find that many recent video-language models perform similarly to random performance on TVBench, with only a few models such as Aria, Qwen2-VL, and Tarsier surpassing this baseline.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
859,https://openreview.net/forum?id=snDh9UuDu6,Improving Multimodal Distillation for 3D Semantic Segmentation under Domain Shift,"Semantic segmentation networks trained under full supervision for one type of lidar fail to generalize to unseen lidars without intervention. 
To reduce the performance gap under domain shifts, a recent trend is to leverage vision foundation models (VFMs) providing robust features across domains. In this work, we conduct an exhaustive study to identify recipes for exploiting VFMs in unsupervised domain adaptation for semantic segmentation of lidar point clouds. Building upon unsupervised image-to-lidar knowledge distillation, our study reveals that: (1) the architecture of the lidar backbone is key to maximize the generalization performance on a target domain; (2) it is possible to pretrain a single backbone once and for all, and use it to address many domain shifts; (3) best results are obtained by keeping the pretrained backbone frozen and training an MLP head for semantic segmentation. The resulting pipeline achieves state-of-the-art results in four widely-recognized and challenging settings. The code will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
864,https://openreview.net/forum?id=abQaX53f9f,Geometry-Aware Diffusion Models for Multiview Scene Inpainting,"In this paper, we focus on 3D scene inpainting, where parts of an input image set, captured from different viewpoints, are masked out. The main challenge lies in generating plausible image completions that are geometrically consistent across views. Most recent work addresses this challenge by combining generative models with a 3D radiance field to fuse information across a relatively dense set of viewpoints. However, a major drawback of these methods is that they often produce blurry images due to the fusion of inconsistent cross-view images. To avoid blurry inpaintings, we eschew the use of an explicit or implicit radiance field altogether and instead fuse cross-view information in a learned space. In particular, we introduce a geometry-aware conditional generative model, capable of multi-view consistent inpainting using reference-based geometric and appearance cues. A key advantage of our approach over existing methods is its unique ability to inpaint masked scenes with a limited number of views (i.e., few-view inpainting), whereas previous methods require relatively large image sets for their 3D model fitting step. Empirically, we evaluate and compare our scene-centric inpainting method on two datasets, SPIn-NeRF and NeRFiller, which contain images captured at narrow and wide baselines, respectively, and achieve state-of-the-art 3D inpainting performance on both. Additionally, we demonstrate the efficacy of our approach in the few-view setting compared to prior methods.",6,3,~Prakhar_Kaushik1|~Enric_Meinhardt-Llopis1|~Divya_Kothandaraman1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
865,https://openreview.net/forum?id=hMEgF7sqDU,Estimating Foot Pressure and Stability from Visual Input,"We propose FootFormer, a new cross-modality approach for regressing foot pressure distribution, foot contact, and center of mass from visual input. The proposed network effectively leverages spatiotemporal information from pose sequences and facilitates learning of rich motion-dynamics, enabling the estimation of human stability. We achieve SOTA on three different datasets and tasks, each evaluating the different network outputs. To further validate our approach, we collect a set of multi-view RGB video and foot pressure-paired motions and evaluate the model's accuracy on these unseen movements. The findings indicate that our method can effectively estimate  motion-dynamics and human stability measurements on a diverse range of activities.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
866,https://openreview.net/forum?id=7Mr19ZFGbF,Image Recognition with Vision and Language Embeddings of VLMs,"Vision-language models (VLMs) have enabled strong zero-shot classification through image–text alignment, but their purely visual inference capabilities remain underexplored. In this work, we conduct a comprehensive evaluation of both language-guided and vision-only classification with a diverse set of dual-encoder VLMs, including recent models such as SigLIP 2 and RADIO. 
On the standard ImageNet-1k dataset and its label-corrected variant,
performance is compared in a standardized setup. Key factors affecting accuracy are analysed, including prompt design, class granularity, number of neighbours, and retrieval set size. We show that language and vision offer complementary strengths, with some classes favouring textual prompts and others better handled by visual similarity. To exploit this complementarity, we introduce a simple, learning-free fusion method based on per-class precision that improves classification performance.",5,3,~Jie_Mei3|~Tommaso_Galliena1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
873,https://openreview.net/forum?id=VQy7PNQE2l,Exploring Image Representation with Decoupled Classical Visual Descriptors,"Decomposing visual components for general image tasks remains a challenge in computer vision. Despite the substantial advancements introduced by deep learning approaches, the inherent complexity of these models often obscures how images are internally processed and understood. In contrast, classical visual descriptors (e.g., edge, colour, and intensity distribution) have long been fundamental to image analysis and remain intuitively understandable to humans. Inspired by this, we present VisualSplit, a new learning paradigm designed to explore image decomposition by explicitly leveraging decoupled classical visual descriptors. Specifically, this approach integrates multiple traditional visual descriptors as independent yet informative inputs, extracting meaningful features through a reconstruction-based pretraining process. By explicitly decomposing visual attributes, our method inherently facilitates effective attribute control in various advanced visual tasks, including image generation and editing, extending beyond conventional classification and segmentation, suggesting the effectiveness of this new learning approach towards visual understanding. The code and models will be made publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
875,https://openreview.net/forum?id=NN71uQnRSI,Lost in Translation? Vocabulary Alignment for Source-Free Adaptation in Open-Vocabulary Semantic Segmentation,"We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which augments the pseudo-label generation process by incorporating additional class concepts. 
To ensure efficiency, we utilize Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. Furthermore, we propose a top-k class selection mechanism for the student model, significantly reducing memory requirements while further improving the adaptation performance. Our approach achieves a remarkable +6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting. Our code will be made publicly available upon acceptance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
880,https://openreview.net/forum?id=479CwMavSQ,Distortion-Aware Multi-Object Tracking via Virtual Plane Projection in Overhead Fisheye Cameras,"Despite progress in multi-object tracking (MOT) for perspective cameras, applying these methods to overhead fisheye cameras remains challenging.
We argue that existing MOT approaches often fail due to two key challenges: the linear motion assumption of Kalman filters conflicting with fisheye distortions, and highly variable appearances caused by extremely varying viewpoints and non-uniform geometric distortion.
We propose DAFTrack, a motion-based tracker designed for overhead fisheye cameras, with three main contribtutions. First, we track in a distortion-aware space by projecting detections as torso locations onto a virtual plane. Second, we use the Unscented Kalman Filter (UKF) for nonlinear updates, enabling accurate state estimation. Third, we leverage camera height to enhance distance-based matching, improving robustness across different camera setups.
Experimental results show that DAFTrack outperforms baselines on overhead fisheye MOT datasets, achieving a +6.1 HOTA improvement on CEPDOF and +3.3 HOTA on WEPDTOF.",6,3,~Vivek_Menon1|~Alan_Lukezic1|~Matteo_Dunnhofer1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
887,https://openreview.net/forum?id=LGLrFk5fIL,QWD-GAN: Quality-aware Wavelet-driven GAN for Unsupervised Medical Microscopy Images Denoising,"Image denoising plays a critical role in biomedical and microscopy imaging, especially when acquiring wide-field fluorescence-stained images. This task faces challenges in multiple fronts, including limitations in image acquisition conditions, complex noise types, algorithm adaptability, and clinical application demands. Although many deep learning-based denoising techniques have demonstrated promising results, further improvements are needed in preserving image details, enhancing algorithmic efficiency, and increasing clinical interpretability. We propose an unsupervised image denoising method based on a Generative Adversarial Network (GAN) architecture. The approach introduces a multi-scale adaptive generator based on wavelet transform and a dual-branch discriminator that integrates difference perception feature maps with original features. Experimental results on multiple biomedical microscopy image datasets show that the proposed model achieves state-of-the-art denoising performance, particularly excelling in the preservation of high-frequency information. Furthermore, the dual-branch discriminator is seamlessly compatible with various GAN frameworks. This quality-aware, wavelet-driven GAN denoising model is termed as QWD-GAN.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
896,https://openreview.net/forum?id=orFmzg04II,Llama Learns to Direct: DirectorLLM for Human-Centric Video Generation,"In this paper, we introduce DirectorLLM, a novel video generation model that employs a large language model (LLM) as the ``director'' to simulate human poses within videos. To enhance the authenticity of human motions in text-to-video models, we extend the LLM from a text generator to a video director and human motion simulator. We train the DirectorLLM to generate detailed human poses, to guide video generation, offloading the simulation of human motion from the video generator to an LLM, effectively creating informative outlines for human-centric scenes. These signals are used as conditions by the video renderer, facilitating more realistic and prompt-following video generation. Experiments on automatic evaluation benchmarks and human evaluations show that our model outperforms existing ones in generating videos with higher human motion fidelity, improved prompt faithfulness, and enhanced rendered subject naturalness.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
898,https://openreview.net/forum?id=q4HbVLic5v,FaceCPT: Toward Cross-Modal Facial Representation Learning with Face-Caption Pre-Training,"Facial representation learning (FRL) through weakly-supervised pre-training has shown significant promise across various downstream tasks, highlighting its improved generalizability. However, most existing FRL models excel only in face generation and analysis, and there is a lack of models that address cross-modal tasks. To fill this gap, we pose the following question: Can we learn a universal facial representation by pre-training on web-sourced face-caption pairs for surveillance-related tasks? These tasks include both cross-modal understanding, such as face captioning and text-based face image retrieval, as well as face analysis tasks like face, attribute, expression recognition, and age estimation. In this paper, we take a step toward this objective by introducing FaceCPT, a new framework for learning facial representation using Face Caption Pre-Training. However, domain misalignment and information asymmetry between image-text pairs challenge the model's ability to achieve a meaningful interaction. To overcome this, we utilize contrastive learning along with a semantic attribute-aware loss (SAAL) to improve the semantic associations between face-caption pairs and encourage the model to focus on key semantic attributes, respectively. Experiments show that FaceCPT outperforms existing vision-language pre-training and FRL baselines, achieving state-of-the-art results in task-specific fine-tuning and improved zero-shot transferability across both single-modal and cross-modal tasks, even with low-resolution inputs.",5,4,~Guangyu_Zhong1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
900,https://openreview.net/forum?id=EqZe1ZXH29,Time-Scaling State-Space Models for Dense Video Captioning,"Dense video captioning is a challenging video understanding task which aims to simultaneously segment the video into a sequence of meaningful consecutive events and to generate detailed captions to accurately describe each event. Existing methods often encounter difficulties when working with the long videos associated with dense video captioning, due to the computational complexity and memory limitations. Furthermore, traditional approaches require the entire video as input, in order to produce an answer, which precludes online processing of the video. We address these challenges by time-scaling State-Space Models (SSMs) to even longer sequences than before. Our approach, State-Space Models with Transfer State, combines both the long-sequence and recurrent properties of SSMs and addresses the main limitation of SSMs which are otherwise not able to sustain their state for very long contexts, effectively scaling SSMs further in time. The proposed model is particularly suitable for generating captions on-the-fly, in an online or streaming manner, without having to wait for the full video to be processed, which is more beneficial in practice. When applied to dense video captioning, our approach scales well with video lengths and uses 7x fewer FLOPs.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
902,https://openreview.net/forum?id=OzQPJmV8oG,ITC-RWKV: Interactive Tissue–Cell Modeling with Recurrent Key-Value Aggregation for Histopathological Subtyping,"Accurate interpretation of histopathological images demands integration of information across spatial and semantic scales, from nuclear morphology and cellular textures to global tissue organization and disease-specific patterns. Although recent foundation models in pathology have shown strong capabilities in capturing global tissue context, their omission of cell-level feature modeling remains a key limitation for fine-grained tasks such as cancer subtype classification. To address this, we propose a dual-stream architecture that models the interplay between macroscale tissue features and aggregated cellular representations. To efficiently aggregate information from large cell sets, we propose a receptance-weighted key-value aggregation model, a recurrent transformer that captures inter-cell dependencies with linear complexity. Furthermore, we introduce a bidirectional tissue-cell interaction module to enable mutual attention between localized cellular cues and their surrounding tissue environment. Experiments on two histopathological subtype classification benchmarks show that the proposed method outperforms existing models, demonstrating the critical role of cell-level aggregation and tissue-cell interaction in fine-grained computational pathology.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Fani Deligianni
903,https://openreview.net/forum?id=q9Cgi9jWks,Toward Robust Audio-Visual Synchronization Detection in Egocentric Video with Sparse Synchronization Events,"Audio-Visual Synchronization Detection (AVS) is a core task in multimodal video quality analysis, yet most existing methods are developed and evaluated on domains with limited diversity of sparse events or predominantly dense, repetitive cues—such as talking heads or scripted broadcasts—restricting their generalization to many real-world scenarios. We present the first comprehensive study of AVS in the challenging domain of egocentric video, using the Ego4D dataset as a benchmark. Motivated by the growing use of head-mounted and body-worn cameras in live streaming, augmented reality, law enforcement, and sports, this domain presents unique challenges: sparse, heterogeneous synchronization events, unstable viewpoints, and minimal access to dense anchors like visible faces. Our findings reveal sharp performance drops in existing AVS models on egocentric content. In response, we introduce $\textit{AS-Synchformer}$, a novel streaming AVS model tailored for sparse, unconstrained video. $\textit{AS-Synchformer}$ incorporates three key innovations: (1) a history-aware streaming token selection strategy, (2) a contrastive alignment loss to enforce temporal correspondence for selected streaming tokens, and (3) an Earth Mover’s Distance (EMD) loss to capture ordinal offset structure for the AVS task. These yield substantial gains, including a 3.55% boost in ACC@1 and a 22.3% EMD reduction over strong streaming baselines like APA Synchformer, and a 2.41% ACC@1 gain with a 21.6% EMD reduction over Synchformer in snapshot AVS—setting a new state of the art in both paradigms. Moreover, we investigate the individual impact of full encoder fine-tuning on our model through an ablation study. Our analysis highlights the critical role of encoder fine-tuning in achieving robust AVS under real-world egocentric conditions, representing the first large-scale AVS systems with end-to-end training released.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
914,https://openreview.net/forum?id=bKx8VovTPL,Improving Human Motion Plausibility with Body Momentum,"Many studies decompose human motion into local motion in a frame attached to
the root joint and global motion of the root joint in the world frame, treating them
separately. However, these two components are not independent. Global movement
arises from interactions with the environment, which are, in turn, driven by changes
in the body configuration. Motion models often fail to precisely capture this physical
coupling between local and global dynamics, while deriving global trajectories from joint
torques and external forces is computationally expensive and complex. To address these
challenges, we propose using whole-body linear and angular momentum as a constraint to
link local motion with global movement. Since momentum reflects the aggregate effect
of joint-level dynamics on the body’s movement through space, it provides a physically
grounded way to relate local joint behavior to global displacement. Building on this
insight, we introduce a new loss term that enforces consistency between the generated
momentum profiles and those observed in ground-truth data. We evaluate our loss on
the global motion recovery task. Incorporating our loss reduces foot sliding and jitter,
improves balance, and preserves the accuracy of the recovered motion.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
915,https://openreview.net/forum?id=8sek44Vz1p,DualDistill: A Unified Cross-Modal Knowledge Distillation Framework for Camera-Based BEV Representation,"Cross-modal knowledge distillation has drawn much attention to camera-based bird’s-eye-view (BEV) models, aiming to narrow the performance gap with their LiDAR-based counterparts. However, distilliing knowledge from a LiDAR-based teacher is not easy due to the discrepancy between sensor modalities. In this work, we introduce DualDistill, a unified cross-modal knowledge distillation framework to address this challenge. We propose an attention-guided orthogonal alignment (AOA) to align student features with the teacher's representations while preserving useful information. This alignment is integrated into a multi-scale feature distillation with adaptive region weighting scheme. In addition, we introduce a cross-head response distillation (CRD) to enforce consistency in BEV representations by comparing the predictions of the teacher and the aligned student. We evaluate our method on the nuScenes dataset. Comprehensive experiments show that our method significantly improves camera-based BEV models and outperforms recent cross-modal knowledge distillation techniques.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
922,https://openreview.net/forum?id=RfxHU2rqpx,FSLC: Fast Scoring with Learnable Coreset for Zero-shot Industrial Anomaly Detection,"This paper presents an efficient approach for zero-shot anomaly classification (AC) and segmentation (AS) in industrial applications. While existing zero-shot anomaly detection methods often rely on supplemental prior knowledge or trade computational speed for performance, our method eliminates dependence on external data and accelerates detection. We propose a dynamic coreset strategy that learns directly from test data and repurposes it for anomaly scoring. The coreset is initially constructed from diverse image patches to comprehensively capture potential data patterns across all test samples. Through iterative expansion and score-based filtration, the coreset progressively refines its representation of normal data distributions. This adaptive process enables quantitative evaluation of anomaly severity based on deviations from the learned norms. Experimental validation across multiple benchmarks demonstrates the method’s effectiveness. On MVTec AD, we achieve state-of-the-art average AUROC scores of 93.52\% (AC) and 96.55\% (AS), while maintaining processing speeds of 5 to 7 frames per second. These results highlight the ability of our framework to balance accuracy and efficiency in practical industrial deployments.",5,4,~Dinesh_Singh1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
931,https://openreview.net/forum?id=0qO6zXUzzO,Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation,"The quadratic computational complexity of self-attention in diffusion transformers (DiT) introduces substantial computational costs in high-resolution image generation. While the linear-complexity Mamba model emerges as a potential alternative, direct Mamba training remains empirically challenging. To address this issue, this paper introduces diffusion transformer-to-mamba distillation (T2MD), forming an efficient training pipeline that facilitates the transition from the self-attention-based transformer to the linear complexity state-space model Mamba. We establish a diffusion self-attention and Mamba hybrid model that simultaneously achieves efficiency and global dependencies. With the proposed layer-level teacher forcing and feature-based knowledge distillation, T2MD alleviates the training difficulty and high cost of a state space model from scratch. Starting from the distilled 512 $\times$ 512 resolution base model, we push the generation towards 2048 $\times$ 2048 images via lightweight adaptation and high-resolution fine-tuning. Experiments demonstrate that our training path leads to low overhead but high-quality text-to-image generation. Importantly, our results also justify the feasibility of using sequential and causal Mamba models for generating non-causal visual output, suggesting the potential for future exploration.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Zhixiang Chen
938,https://openreview.net/forum?id=jynhiieAEV,Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment,"Accurate segmentation of the optic disc and cup is critical for the early diagnosis and management of ocular diseases such as glaucoma. However, segmentation models trained on one dataset often suffer significant performance degradation when applied to target data acquired under different imaging protocols or conditions. To address this challenge, we propose Grad-CL, a novel source-free domain adaptation framework that leverages a pre-trained source model and unlabeled target data to robustly adapt segmentation performance without requiring access to the original source data. Our approach combines a gradient-guided pseudolabel refinement module with a cosine similarity–based contrastive learning strategy. In the first stage, salient class-specific features are extracted via a gradient-based mechanism, enabling more accurate uncertainty quantification and robust prototype estimation for refining noisy pseudolabels. In the second stage, a contrastive loss based on cosine similarity is employed to explicitly enforce inter-class separability between the gradient-informed features of the optic cup and disc. Extensive experiments on challenging cross-domain fundus imaging datasets demonstrate that Grad-CL outperforms current state-of-the-art unsupervised and source-free domain adaptation methods, achieving superior segmentation accuracy and improved boundary delineation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
940,https://openreview.net/forum?id=jajVP0jafI,Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation,"Histopathology image segmentation is crucial for disease diagnosis, prognosis, and treatment planning, yet it remains challenging due to complex tissue structures, staining artifacts, and annotation scarcity. Semi-supervised learning has been employed to alleviate the need for extensive labeled data, but existing methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and morphological misclassification. In this work, we introduce Semi-MOE, to the best of our knowledge, the first multi-task Mixture-of-Experts framework for semi-supervised histopathology image segmentation. Our approach leverages three specialized expert networks—a main segmentation expert, a signed distance field regression expert, and a boundary prediction expert—each dedicated to capturing distinct morphological features. Subsequently, the Multi-Gating Pseudo-labeling module dynamically aggregates expert features, enabling a robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate manual tuning while dynamically balancing multiple learning objectives, we propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and CRAG benchmarks show that our method outperforms state-of-the-art approaches in extremely low-label settings, highlighting the potential of MoE-based architectures in advancing semi-supervised segmentation. Code is available and will be released.",5,4,~Maria_J_Ledesma_Carbayo1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
943,https://openreview.net/forum?id=aa6ITjX0P2,HEAL: Learning-Free Source Free Unsupervised Domain Adaptation for Cross-Modality Medical Image Segmentation,"Growing demands for clinical data privacy and storage constraints have spurred advances in Source Free Unsupervised Domain Adaptation (SFUDA). SFUDA addresses the domain shift by adapting models from the source domain to the unseen target domain without accessing source data, even when target-domain labels are unavailable. However, SFUDA faces significant challenges: the absence of source domain data and label supervision in the target domain due to source free and unsupervised settings. To address these issues, we propose HEAL, a novel SFUDA framework that integrates  Hierarchical denoising, Edge-guided selection, size-Aware fusion, and Learning-free characteristic. Large-scale cross-modality experiments demonstrate that our method outperforms existing SFUDA approaches, achieving state-of-the-art (SOTA) performance. The source code is publicly available at: https://github.com/derekshiii/HEAL.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
948,https://openreview.net/forum?id=FiLsvTSwxU,Tracking Meets Large Multimodal Models for Driving Scenario Understanding,"Large Multimodal Models (LMMs) have recently gained prominence in autonomous driving research, showcasing promising capabilities across various emerging benchmarks. LMMs specifically designed for this domain have demonstrated effective perception, planning, and prediction skills. However, many of these methods underutilize 3D spatial and temporal elements, relying mainly on image data. As a result, their effectiveness in dynamic driving environments is limited. We propose to integrate tracking information as an additional input to recover 3D spatial and temporal details that are not effectively captured in the images. We introduce a novel approach for embedding this tracking information into LMMs to enhance their spatiotemporal understanding of driving scenarios. By incorporating 3D tracking data through a track encoder, we enrich visual queries with crucial spatial and temporal cues while avoiding the computational overhead associated with processing lengthy video sequences or extensive 3D inputs. Moreover, we employ a self-supervised approach to pretrain the tracking encoder to provide LMMs with additional contextual information, significantly improving their performance in perception, planning, and prediction tasks for autonomous driving. Experimental results demonstrate the effectiveness of our approach, with a gain of 9.5\% in accuracy, an increase of 7.04 points in the ChatGPT score, and 9.4\% increase in the overall score over baseline models on DriveLM-nuScenes benchmark, along with a 3.7\% final score improvement on DriveLM-CARLA. Our code and models will be made publicly available.",5,4,~Rongchang_Li1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
949,https://openreview.net/forum?id=h8izdAvbgl,Prompt Image to Watch and Hear: Multimodal Prompting for Parameter-Efficient Audio-Visual Learning,"We explore how to endow the static and pre-trained image models to watch and hear from the audio-video domain with a limited set of trainable parameters. To achieve this objective, we propose an Audio-Visual Spatial-Temporal-Fusion Prompting, called AV-STFP, to gradually adapt the shared and frozen image model to learn audio-visual representation by decoupling the adaptation into temporal-aware prompting, spatial-aware prompting, and multimodal fusion prompting. First, temporal-aware prompting introduces a set of temporal prompts into tokens along temporal frames to adapt the shared pre-trained image layers to capture temporal patterns. Next, spatial-aware prompting inserts a set of spatial prompts into the tokens to enable the shared image layers to learn audio contexts and further enhance the visual semantics. Finally, multimodal fusion prompting incorporates a set of fusion prompts between audio and visual tokens to enforce the pre-trained image layers to integrate two modalities. By only tuning the inserted learnable prompts and freezing the pre-trained image backbones, AV-STFP efficiently transfers the well-generalized image knowledge into the audio-visual domain with minimal costs. Extensive experiments on the various audio-visual understanding tasks indicate that AV-STFP achieves competitive or even superior performance to state-of-the-art methods while involving minimal trainable parameters (i.e. 0.6% of model parameters).",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
955,https://openreview.net/forum?id=ev5mXnprnx,Clean Sample Selection and Noisy Sample Rematching for Text-Based Pedestrian Retrieval,"Text-Based Pedestrian Retrieval (TBPR) aims to match pedestrian images with corresponding text descriptions. Although notable advancements have been made in this field, most existing methods assume that training samples are clean. However, the neglect of noisy samples can degrade model performance. To address this issue, we propose an effective framework called CSNR from the perspective of dynamic dataset reconstruction, incorporating three key innovations. The Clean Sample Selection Module (CSSM) exploits the differences in the loss values of clean and noisy samples, filtering out noisy samples to improve the quality of training samples. The Noisy Sample Rematching Module (NSRM) replaces the text descriptions of some noisy samples with better-matching descriptions to create new clean samples, thereby increasing the set of usable clean data. The Weighted topK Contrastive Loss (WKCL) uses more negative samples and prioritizes harder negatives to fully leverage the information from negative samples and ensure the efficiency of model training under noisy description circumstances. Experimental results demonstrate that our approach achieves state-of-the-art performance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
960,https://openreview.net/forum?id=CQEn2bKQgg,A Hybrid Framework Bridging CNN and ViT based on Theory of Evidence for Diabetic Retinopathy Grading,"Diabetic retinopathy (DR) is a leading cause of vision loss among middle-aged and elderly people, which significantly impacts their daily lives and mental health. To improve the efficiency of clinical screening and enable the early detection of DR, a variety of automated DR diagnosis systems have been recently established based on convolutional neural network (CNN) or vision Transformer (ViT). However, due to the own shortages of CNN / ViT, the performance of existing methods using single-type backbone has reached a bottleneck. One potential way for the further improvements is integrating different kinds of backbones, which can fully leverage the respective strengths of them (i.e., the local feature extraction capability of CNN and the global feature capturing ability of ViT). To this end, we propose a novel paradigm to effectively fuse the features extracted by different backbones based on the theory of evidence. Specifically, the proposed evidential fusion paradigm transforms the features from different backbones into supporting evidences via a set of deep evidential networks. With the supporting evidences, the aggregated opinion can be accordingly formed, which can be used to adaptively tune the fusion pattern between different backbones and accordingly boost the performance of our hybrid model. We evaluated our method on two publicly available DR grading datasets. The experimental results demonstrate that our hybrid model not only improves the accuracy of DR grading, compared to the state-of-the-art frameworks, but also provides the excellent interpretability for feature fusion and decision-making. The code will be released once the paper is accepted.",5,4,~Sung-Ho_Bae1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
962,https://openreview.net/forum?id=Dyz738oZnl,LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba,"Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exhibits exceptional scalability and surpasses the performance of DiT across various model scales on ImageNet at 256x256 resolution, all while utilizing substantially fewer GFLOPs and a comparable number of parameters. Compared to state-of-the-art diffusion models on ImageNet 256x256 and 512x512, our largest model presents notable advantages, such as a reduction of up to 62% GFLOPs compared to DiT-XL/2, while achieving superior performance with comparable or fewer parameters.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
976,https://openreview.net/forum?id=Jp7lippZwP,Advancing Utility Pole and Sign Detection Through Deep Learning,"Utility poles are an essential part of the infrastructure used to support power distribution systems and other critical public services. Their regular inspection is crucial to ensure the stability and safety of the electrical grid.

We present a deep learning framework for the automated detection, segmentation and lean angle estimation of wooden utility poles, and classification of attached electrical warning signs, using ground-level imagery. Our system is trained on a custom dataset of 4,570 annotated images extracted from Google Street View, featuring challenging real-world scenes with visually ambiguous wooden poles lacking distinctive features. The proposed model is based on the Detection Transformer (DETR), suitably modified and trained on our custom dataset. The model outperforms standard object detectors (RetinaNet, Faster R-CNN, YOLOv3-Tiny), achieving a mean average precision of 90.43% for pole detection and 88.26% for sign detection.

Extending this model with a segmentation head enables per-instance mask generation, which is then used to estimate pole lean angle. The model accurately estimates lean for 1,367 out of 1,433 test-set poles, with a mean absolute error of 1.01 degrees. Moreover, the custom dataset created in this work is also made publicly available to be used as a benchmark.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
981,https://openreview.net/forum?id=aZUSU8BqvO,Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams,"Event cameras offer various advantages for novel view rendering compared to synchronously operating RGB cameras, and efficient event-based techniques supporting rigid scenes have been recently demonstrated in the literature. 
In the case of non-rigid objects, however, existing approaches additionally require sparse RGB inputs, which can be a substantial practical limitation; it remains unknown if similar models could be learned from event streams only. 
This paper sheds light on this challenging open question and introduces Ev4DGS, i.e.,the first approach for novel view rendering of non-rigidly deforming objects in the explicit observation space (i.e., as RGB or grayscale images) from monocular event streams. 
Our method regresses a deformable 3D Gaussian Splatting representation through 1) a loss relating the outputs of the estimated model with the 2D event observation space, and 2) a coarse 3D deformation model trained from binary masks generated from events. 
We perform experimental comparisons on existing synthetic and newly recorded real datasets with non-rigid objects. 
The results demonstrate the validity of Ev4DGS and its superior performance compared to multiple naive baselines that can be applied in our setting. 
We will release our models and the datasets used in the evaluation for research purposes.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
986,https://openreview.net/forum?id=SMQRcRaEtt,Visible Structure Retrieval for Lightweight Image-Based Relocalisation,"Accurate camera pose estimation from an image observation in a previously mapped environment is commonly done through structure-based methods: by finding correspondences between 2D keypoints on the image and 3D structure points in the map. In order to make this correspondence search tractable in large scenes, existing pipelines either rely on search heuristics, or perform image retrieval to reduce the search space by comparing the current image to a database of past observations. However, these approaches result in elaborate pipelines or storage requirements that grow with the number of past observations. In this work, we propose a new paradigm for making structure-based relocalisation tractable. Instead of relying on image retrieval or search heuristics, we learn a direct mapping from image observations to the visible scene structure in a compact neural network. Given a query image, a forward pass through our novel visible structure retrieval network allows obtaining the subset of 3D structure points in the map that the image views, thus reducing the search space of 2D-3D correspondences. We show that our proposed method enables performing localisation with an accuracy comparable to the state of the art, while requiring lower computational and storage footprint.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
987,https://openreview.net/forum?id=ag4Pwdu4Ks,Spatiotemporal Event Spotting via 3D Heatmaps with Dynamically Shifted Gaussian Kernels,"Spatiotemporal event spotting in ball sports is essential for understanding complex game dynamics, requiring both high temporal precision and accurate spatial localization. However, most existing methods focus primarily on temporal localization, often neglecting the spatial dimensions that are crucial for tactical analysis. In this study, we propose a novel representation, 3D Heatmaps with Dynamically Shifted Gaussian Kernels, specifically designed to enable comprehensive spatiotemporal event spotting. To overcome current limitations, we introduce the Volleyball Nations League (VNL) dataset, which includes detailed annotations for eight key event types, encompassing both temporal and spatial labels. Our approach leverages a modified 3D U-Net architecture that effectively captures spatiotemporal patterns by utilizing our proposed heatmap design. Experimental results show that our method significantly outperforms state-of-the-art techniques in both temporal accuracy and spatial precision on the VNL dataset and a spatially augmented version of the SoccerNet Ball Action Spotting (BAS) dataset. These findings demonstrate the robustness and generalizability of our approach across different ball sports domains.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
988,https://openreview.net/forum?id=gENrcEFhIN,Ink Enhancement for Ancient Bamboo Manuscripts  Using Iterative Restoration-Degradation Adversarial Learning,"Bamboo slips are crucial documentary evidences for investigating ancient Chinese history and civilization. However, due to the lengthy deterioration process for about 2000 years, inscriptions on bamboo slips typically suffer from ink fading problem, which seriously impacts their clarity and legibility. This paper proposes BambInk, which is among the first technical initiatives that explore AI-enabled ink enhancement for bamboo slips with faded inks. BambInk is a new self-supervised learning method that devises an iterative restoration-degradation adversarial learning mechanism to progressively enhance the faded inks on the bamboo slips, yet without requiring any annotated data. In specific, BambInk designs two generators which are ink enhancer and ink degrader, and two discriminators for evaluate the effects of the enhanced and degraded images, respectively. Moreover, in the generators, it introduces a dynamic convolution mechanism  that integrates features captured via three heterogeneous types of attentions; it also adopts a simple yet effective high-low frequency information differentiation mechanism for extracting the fine-details of the ink traces. Experiments conducted on the real-world bamboo slips dataset demonstrate the effectiveness of our method. Code, data and the enhanced results are available at: https://anonymous.4open.science/r/BambInk-46E5.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
989,https://openreview.net/forum?id=ngI2TpyTiR,MedOpenSeg: Open-World Medical Segmentation with Memory-Augmented Transformers,"Open-world segmentation in medical imaging presents unique challenges, as models must generalize to seen and unseen classes while retaining knowledge of previously encountered structures. We propose MedOpenSeg, a Memory-Augmented Transformer framework that dynamically stores and updates class prototypes to enhance segmentation accuracy, improve adaptability to new anatomical structures, and detect novel regions during inference. MedOpenSeg integrates a Swin Transformer 3D backbone with a memory bank module that retrieves class-specific feature embeddings and facilitates prototype-based novelty detection using cosine similarity and Euclidean Distance Sum (EDS). We benchmark MedOpenSeg on multiple datasets against state-of-the-art closed-set segmentation and foundation models, demonstrating its effectiveness in handling open-set medical segmentation. Code is publicly available at https://anonymous.4open.science/r/MedOpenSeg-244C",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
992,https://openreview.net/forum?id=QtpZeZyaua,Dual Polarity Prompts with Stochastic Entropy Perturbation for Label Noise,"Pre-trained Vision-Language Models (VLMs) have been widely adopted across various applications through few-shot prompt tuning. However, their potential to effectively address label noise, a common challenge in real-world data, remains largely unexplored. While recent prompt tuning methods exhibit robustness under low to moderate noise, they struggle in handling higher noise ratios or more complex noise patterns. To bridge this gap, we introduce “DP-SEP”, a novel framework designed to enhance the adaptation of VLMs under challenging label noise. DP-SEP consists of two key components: (1) Dual Polarity (DP) Prompts, which dynamically adjust text-image alignments by bringing the correct description closer, while pushing away incorrect ones. DP includes a novel class reordering function to generate negative prompts and a Polarity Alignment Reweighting (PAR) factor to effectively balance the dual prompts. (2) Stochastic Entropy Perturbation (SEP), which injects controlled randomness to reduce the VLM’s overconfidence in noisy labels. Extensive experiments on 7 synthetic datasets and two real-world noisy benchmarks demonstrate that DP-SEP significantly outperforms existing methods. In particular, we achieve an impressive average gain of 11.17% and 16.98% under 65% and 80% sym. noise. Significant improvements of DP-SEP establishes as a new direction for handling label noise in adopting VLMs.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
999,https://openreview.net/forum?id=JaTWss4Mjs,"Lid-Lab-NeRF: Generating Temporally Consistent, Labelled LiDAR Point Clouds using Neural Radiance Fields","LiDAR point clouds are essential for autonomous driving and urban planning, but collecting and labelling high-quality data remains expensive and inflexible due to fixed vehicle trajectories. While synthetic data generation offers a potential solution, existing approaches struggle to bridge the reality gap and often produce temporally inconsistent or unlabelled results. We present Lid-Lab-NeRF, a novel framework that generates labelled, dynamic LiDAR point clouds by combining neural radiance fields with motion field prediction for better time-consistent outputs and semantic labelling through a semantic radiance field. Our model simultaneously produces intensity, depth, labels, and raydrop probability, while a feature alignment loss and raydrop optimisation pipeline ensure realistic scan patterns. Experiments on the KITTI-360 dataset demonstrate that Lid-Lab-NeRF effectively synthesizes labelled point clouds that closely match real LiDAR characteristics.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
1004,https://openreview.net/forum?id=H9XNtKkAZL,RP-SAM2: Refining Point Prompts for Stable Surgical Instrument Segmentation,"Accurate surgical instrument segmentation is essential in cataract surgery for tasks such as skill assessment and workflow optimization. However, limited annotated data makes it difficult to develop fully automatic models. Prompt-based methods like SAM2 offer flexibility yet remain highly sensitive to the point prompt placement, often leading to inconsistent segmentations. We address this issue by introducing RP-SAM2, which incorporates a novel shift block and a compound loss function to stabilize point prompts. Our approach reduces annotator reliance on precise point positioning while maintaining robust segmentation capabilities. Experiments on the Cataract1k dataset demonstrate that RP-SAM2 improves segmentation accuracy, with a 2% mDSC gain, a 21.36% reduction in mHD95, and decreased variance across random single-point prompt results compared to SAM2. Additionally, on the CaDIS dataset, pseudo masks generated by RP-SAM2 for fine-tuning SAM2’s mask decoder outperformed those generated by SAM2. These results highlight RP-SAM2 as a practical, stable and reliable solution for semi-automatic instrument segmentation in data-constrained medical settings. The code will be publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
1006,https://openreview.net/forum?id=oLSfl2YbNG,Verifier Matters: Enhancing Inference-Time Scaling for Video Diffusion Models,"Inference-time scaling has recently gained attention as an effective strategy for improving the performance of generative models without requiring additional training. Although this paradigm has been successfully applied in text and image generation tasks, its extension to video diffusion models remains relatively underexplored. Indeed, video generation presents unique challenges due to its spatiotemporal complexity, particularly in evaluating intermediate generated samples, a procedure that is required by inference-time scaling algorithms. In this work, we systematically investigate the role of the verifier: the scoring mechanism used to guide sampling. We show that current verifiers, when applied at early diffusion steps, face significant reliability challenges due to noisy samples. We further demonstrate that fine-tuning verifiers on partially denoised samples significantly improves early-stage evaluation and leads to gains in generation quality across multiple inference-time scaling algorithms, including Greedy Search, Beam Search, and a novel Successive Halving baseline.",6,4,~Mickael_Chen1|~Xiaohui_Xie2,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
1007,https://openreview.net/forum?id=5VZ9BgOKC7,Is Structural Awareness the Key to Event Camera Data Cleansing for Enhancing Veracity?,"Neuromorphic vision sensors, such as event cameras, offer significant advantages over conventional frame-based cameras, including high dynamic range, low latency, and low power consumption. However, their high sensitivity to illumination changes and asynchronous operation also introduce substantial noise—typically in the form of false or structure-irrelevant event data—posing challenges to the veracity of the acquired information in downstream visual tasks such as object recognition. Traditional cleansing methods for neuromorphic vision sensor event data typically rely on denoising techniques guided by reference-based metrics, which require auxiliary modalities such as Active Pixel Sensor (APS) frames or manual annotations. These references are often unavailable in real-world scenarios. Moreover, existing reference-free metrics generally overlook structural integrity, leading to deceptively high scores when aggressive noise removal results in the loss of meaningful structure. In this paper, we propose the Temporal Structural Event Index(TSEI), a novel, reference-free, structure-aware metric designed to assess the veracity of cleansed neuromorphic vision sensor event data. TSEI integrates temporal structural similarity and contrast normalization within an adaptive segmentation framework to jointly evaluate signal preservation and noise suppression. Experiments on both synthetic and real-world datasets demonstrate that TSEI strongly correlates with structural fidelity and recognition accuracy, outperforming existing metrics in detecting over-cleansing and structural degradation. These findings highlight that structural awareness is a critical factor in enhancing the veracity of neuromorphic event data and ensuring reliable performance in visual recognition tasks.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
1013,https://openreview.net/forum?id=UrcTcCZy4K,SALT: Parameter-Efficient Fine-Tuning via Singular Value Adaptation with Low-Rank Transformation,"The complex nature of medical image segmentation calls for models that are specifically designed to capture detailed, domain-specific
features. Large foundation models offer considerable flexibility, yet the cost of fine-tuning these models remains a significant barrier. Parameter-Efficient Fine-Tuning (PEFT) methods, such as Low-Rank Adaptation (LoRA), efficiently update model weights with low-rank matrices but may suffer from underfitting when the chosen rank is insufficient to capture domain-specific nuances. Conversely, full-rank Singular Value Decomposition (SVD) based methods provide comprehensive updates by modifying all singular values, yet they often lack flexibility and exhibit variable performance across datasets. We propose SALT (Singular Value Adaptation with Low-Rank Transformation), a method that selectively adapts the most influential singular values using trainable scale and shift parameters while complementing this with a low-rank update for the remaining subspace. This hybrid approach harnesses the advantages of both LoRA and SVD, enabling effective adaptation without relying on increasing model size or depth. Evaluated on 5 challenging medical datasets, ranging from as few as 20 samples to 1000, SALT outperforms state-of-the-art PEFT (LoRA and SVD) by 2% to 5% in Dice with only 3.9% trainable parameters, demonstrating robust adaptation even in low-resource settings. The code will be publicly available.",5,4,~Simon_Weber1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1015,https://openreview.net/forum?id=0hSpdmhILl,In-Model Merging for Enhancing the Robustness of Medical Imaging Classification Models,"Model merging is an effective strategy to merge multiple models for enhancing model performances, and more efficient than ensemble learning as it will not introduce extra computation into inference. However, limited research explores if the merging process can occur within one model and enhance the model's robustness, which is particularly critical in the medical image domain. In the paper, we are the first to propose in-model merging (InMerge), a novel approach that enhances the model's robustness by selectively merging similar convolutional kernels in the deep layers of a single convolutional neural network (CNN) during the training process for classification. We also analytically reveal important characteristics that affect how in-model merging should be performed, serving as an insightful reference for the community. We demonstrate the feasibility and effectiveness of this technique for different CNN architectures on 4 prevalent datasets. The proposed InMerge-trained model surpasses the typically-trained model by a substantial margin. The code will be made public.",6,3,~Amir_Nakib1|~Aman_Gupta1|~Qing_En2,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1024,https://openreview.net/forum?id=NLh4WajK6j,Quantifying Risk in Pedestrian Crowds Using Divergence Estimated from Flows of Head-Tracking Data,"The rapid pace of urbanization and the increasing frequency of large-scale events have made crowd management a critical concern in urban planning, facility design, and event operations. Effective crowd management must address congestion while ensuring pedestrian safety and comfort. Achieving this requires a reliable assessment of potential risks within crowds. While crowd density is a common indicator of congestion and can identify hazardous high-density situations, it is inadequate for evaluating risk in low- to medium-density crowds, where pedestrian movement dynamics play a more significant role. To overcome this limitation, we propose the crowd risk score (CRS), a novel quantitative metric for assessing crowd-related risk grounded in the mathematical concept of divergence from vector calculus. The proposed approach was validated using datasets derived from real-world event footage with human-annotated risk labels. 
Experimental evaluations on these datasets demonstrated the effectiveness of the proposed method. By computing the CRS from pedestrian data automatically obtained through deep learning-based detection and tracking, the proposed method enables end-to-end analysis of crowd-related risk directly from video input. This automation supports crowd monitoring and analysis in practical crowd management scenarios.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
1029,https://openreview.net/forum?id=etq5VMwzIL,PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI,"Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often degrades image quality. While Implicit Neural Representations (INRs) show promise for MRI reconstruction, they struggle at high acceleration factors due to weak prior constraints, leading to structural loss and aliasing artefacts. To address this, we propose PrIINeR, an INR-based MRI reconstruction method that integrates prior knowledge from pre-trained deep learning models into the INR framework. By combining population-level knowledge with instance-based optimization and enforcing dual data consistency, PrIINeR aligns both with the acquired k-space data and the prior-informed reconstruction. Evaluated on the NYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based approaches but also improves upon several learning-based state-of-the-art methods, significantly improving structural preservation and fidelity while effectively removing aliasing artefacts. PrIINeR bridges deep learning and INR-based techniques, offering a more reliable solution for high-quality, accelerated MRI reconstruction. The code is publicly available on https://anonymous.4open.science/r/PrIINeR-5CE2.",4,4,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
1030,https://openreview.net/forum?id=sMnYsfTIuU,Language-Guided Decision Override for Adaptive and Retraining-Free Video Anomaly Detection,"One-class video anomaly detection (OC-VAD) detects all events outside the learned distribution as anomalies and requires costly model retraining to redefine what is considered anomalous.
Moreover, most existing methods lack the flexibility to incorporate the prior knowledge that human surveillance operators often have about specific anomalies they wish to flag or ignore.
To address these challenges, we propose LinGuard (Language-guided anomaly guard), a method that lets users dynamically redefine anomaly/normality judgments at inference time by providing an Event Prompt—a language description of events that should be treated as normal or anomalous—without retraining.
LinGuard fuses the anomaly score from a pre-trained OC-VAD backbone with a similarity-based score computed between the Event Prompt and event captions generated by a large vision language model (LVLM).
We evaluate LinGuard on standard OC-VAD and general action recognition datasets under challenging conditions where normal/anomalous event set changes during test time.
Experiments demonstrate that LinGuard can flexibly redefine user-specified events as normal or anomalous, effectively handling both unknown and known anomalies while eliminating retraining costs.
Our code and model will be released upon acceptance.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
1035,https://openreview.net/forum?id=OVRLjMCdFd,Superpixel Anything: A general object-based framework for accurate yet regular superpixel segmentation,"Superpixels are widely used in computer vision to simplify image representation and reduce computational complexity. While traditional methods rely on low-level features, deep learning-based approaches leverage high-level features but also tend to sacrifice regularity of superpixels to capture complex objects, leading to accurate but less interpretable segmentations. In this work, we introduce SPAM (SuperPixel Anything Model), a versatile framework for segmenting images into accurate yet regular superpixels. We train a model to extract image features for superpixel generation, and at inference, we leverage a large-scale pre-trained model for semantic-agnostic segmentation to ensure that superpixels align with object masks. SPAM can handle any prior high-level segmentation, resolving uncertainty regions, and is able to interactively focus on specific objects. Comprehensive experiments demonstrate that SPAM qualitatively and quantitatively outperforms state-of-the-art methods on segmentation tasks, making it a valuable and robust tool for various applications. Code and pre-trained models will be made available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
1036,https://openreview.net/forum?id=nzvdIjqAR7,Benchmarking Vision Foundation Models for Input Monitoring in Autonomous Driving,"Deep neural networks (DNNs) remain challenged by distribution shifts in complex open-world domains like automated driving (AD): Robustness against yet unknown novel objects (semantic shift) or styles like lighting conditions (covariate shift) cannot be guaranteed. Hence, reliable operation-time monitors for identification of out-of-training-datadistribution (OOD) scenarios are imperative. Current approaches for OOD classification are untested for complex domains like AD, are limited in the kinds of shifts they detect, or even require supervision with OOD samples. We instead establish a framework around a principled, unsupervised and model-agnostic method that unifies detection of semantic and covariate shifts: Find a full model of the training data’s feature distribution, to then use its density at new points as in-distribution (ID) score. To implement this, we propose to combine Vision Foundation Models (VFMs) as feature extractors with density modeling techniques. Through a comprehensive benchmark of 4 VFMs with different backbone architectures and 5 density modeling techniques against established baselines, we provide the first systematic evaluation of OOD classification capabilities of VFMs across diverse conditions. VFMs embeddings with density estimation outperform existing approaches in identifying OOD inputs. Additionally, we show that our method detects high-risk inputs likely to cause errors in downstream tasks, thereby improving overall performance. Overall, VFMs, when coupled with robust density modeling techniques, are promising to realize model-agnostic, unsupervised, reliable safety monitors in complex vision tasks.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
1037,https://openreview.net/forum?id=Ov6I5fZ6nJ,Generative Data Augmentation for Object Point Cloud Segmentation,"Data augmentation is widely used to train deep learning models to address data scarcity. However, traditional data augmentation (TDA) typically relies on simple geometric transformation, such as random rotation and rescaling, resulting in minimal data diversity enrichment and limited model performance improvement. State-of-the-art generative models for 3D shape generation rely on the denoising diffusion probabilistic models and manage to generate realistic novel point clouds for 3D content creation and manipulation. Nevertheless, the generated 3D shapes lack associated point-wise semantic labels, restricting their usage in enlarging the training data for point cloud segmentation tasks. To bridge the gap between data augmentation techniques and the advanced diffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to a part-aware generative model that can generate high-quality point clouds conditioned on given segmentation masks. Leveraging the novel generative model, we introduce a 3-step generative data augmentation (GDA) pipeline for point cloud segmentation training. Our GDA approach requires only a small amount of labeled samples but enriches the training data with generated variants and pseudo-labeled samples, which are validated by a novel diffusion-based pseudo-label filtering method. Extensive experiments on two large-scale synthetic datasets and a real-world medical dataset demonstrate that our GDA method outperforms TDA approach and related semi-supervised and self-supervised methods.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
1051,https://openreview.net/forum?id=GGdZv9PheQ,Conditional Prototype Learning for Few-Shot Object Detection,"Few-Shot Object Detection (FSOD) aims to train models for detecting novel objects by leveraging abundant data from base classes and only a few examples from novel classes. In the meta-learning paradigm for FSOD, the class prototypes are learned to guide the feature learning. However, the particularity of limited samples may cause the semantic bias of the class prototypes, which compromises the generalization ability. To address this, we propose Conditional Prototype Learning (CPL), a novel framework that integrates robust conditional references with training data to mitigate bias and enhance prototype robustness. Specifically, we introduce a Masked Conditional Variational Autoencoder (MCVA) to generate more robust conditional prototypes by correcting semantic biases. Furthermore, since ideal prototypes should not retain sample-specific particularities, the conditional prototypes lack explicit spatial and scale information. We design the task-aware feature aggregation (TFA) module to enhance the feature presentation for classification and regression tasks. Extensive experiments on PASCAL VOC and MS COCO demonstrate that our approach achieves state-of-the-art performance, validating the effectiveness of CPL in improving generalization to novel classes.",6,4,~Amirreza_Shaban1|~Shounak_Datta1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
1058,https://openreview.net/forum?id=nAZZT8wOiy,Atomizer: Generalizing to unseen modalities by breaking images down to a set of scalars,"The growing number of Earth observation satellites has led to increasingly diverse remote sensing data, with varying spatial, spectral, and temporal configurations. Most existing models rely on fixed input formats and modality-specific encoders, which require retraining when new configurations are introduced, limiting their ability to generalize across modalities. We introduce Atomizer, a flexible architecture that represents remote sensing images as sets of scalars, each corresponding to a spectral band value of a pixel. Each scalar is enriched with contextual metadata (acquisition time, spatial resolution, wavelength, and bandwidth), producing an atomic representation that allows a single encoder to process arbitrary modalities without interpolation or resampling. Atomizer uses structured tokenization with Fourier features and non-uniform radial basis functions to encode content and context, and maps tokens into a latent space via cross-attention. Under modality-disjoint evaluations, Atomizer outperforms standard models and demonstrates robust performance across varying resolutions and spatial sizes.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
1060,https://openreview.net/forum?id=XBcOZH6c2O,Learning Correlation-aware Aleatoric Uncertainty for 3D Hand Pose Estimation,"3D hand pose estimation is a fundamental task in understanding human hands. However, accurately estimating 3D hand poses remains challenging due to the complex movement of hands, self-similarity, and frequent occlusions. In this work, we address two limitations: the inability of existing 3D hand pose estimation methods to estimate aleatoric (data) uncertainty, and the lack of uncertainty modeling that incorporates joint correlation knowledge, which has not been thoroughly investigated. To this end, we introduce aleatoric uncertainty modeling into the 3D hand pose estimation framework, aiming to achieve a better trade-off between modeling joint correlations and computational efficiency. We propose a novel parameterization that leverages a single linear layer to capture intrinsic correlations among hand joints. This is enabled by formulating the hand joint output space as a probabilistic distribution, allowing the linear layer to capture joint correlations. Our proposed parameterization is used as a task head layer, and can be applied as an add-on module on top of the existing models. Our experiments demonstrate that our parameterization for uncertainty modeling outperforms existing approaches. Furthermore, the 3D hand pose model equipped with our uncertainty head achieves favorable accuracy while introducing new uncertainty modeling capability to the model.",6,3,~Xiangyi_Yan1|~Qichen_Fu2|~Benjamin_Allaert1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1061,https://openreview.net/forum?id=LbcPsKAb5V,IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis,"Pathological images play an essential role in cancer prognosis, while survival analysis, which integrates computational techniques,
can predict critical clinical events such as patient mortality or disease recurrence from whole-slide images (WSIs). Recent advancements in multiple instance learning have significantly improved the efficiency of survival analysis. However, existing methods often struggle to balance the
modeling of long-range spatial relationships with local contextual dependencies and typically lack inherent interpretability, limiting their clinical
utility. To address these challenges, we propose the Interpretable Pathology Graph-Transformer (IPGhormer), a novel framework that captures
the characteristics of the tumor microenvironment and models their spatial dependencies across the tissue. IPGhormer uniquely provides interpretability at both tissue and cellular levels without requiring posthoc manual annotations, enabling detailed analyses of individual WSIs
and cross-cohort assessments. Comprehensive evaluations on four public benchmark datasets demonstrate that IPGhormer outperforms stateof-the-art methods in both predictive accuracy and interpretability. In
summary, our method, IPGhormer offers a promising tool for cancer
prognosis assessment, paving the way for more reliable and interpretable
decision-support systems in pathology. The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
1062,https://openreview.net/forum?id=vTkaJuzRUM,Calibration-Aware Prompt Learning for Medical Vision-Language Models,"Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability.
To address this, we introduce \texttt{CalibPrompt}, the first framework to calibrate Med-VLMs during prompt tuning. 
\texttt{CalibPrompt} optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime.
First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs.
Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that \texttt{CalibPrompt} consistently improves calibration without drastically affecting clean accuracy. We will make our code publicly available upon acceptance.",5,3,~Liang_Wang26|~Lakshmikar_Reddy_Polamreddy1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1064,https://openreview.net/forum?id=CDguDkbVXC,3D-WAG: Hierarchical Wavelet-Guided Autoregressive Generation for High-Fidelity 3D Shapes,"Autoregressive (AR) models excel in language and image generation, but their role in 3D generation faces high computational cost and resolution challenges. Existing 3D AR methods, using voxel grids or implicit representations, produce long, redundant token sequences, limiting high-fidelity 3D shape generation and incur high inference cost. To address these issues, we introduce 3D-WAG, a novel autoregressive framework employing compact wavelet-based hierarchical representations for efficient and expressive 3D shape generation. By representing the shapes in the wavelet domain, 3D-WAG captures coarse to fine geometric details as multi-scale discrete token maps, using a 3D vector-quantized variational autoencoder (VQVAE), enabling efficient AR modeling and detailed shape understanding. Unlike conventional next-token prediction, 3D-WAG formulates 3D shape generation as a next-scale token map prediction problem, achieving faster inference time of 1.15 seconds per sample on a single NVIDIA H100 GPU, which is 15 times faster than state-of-the-art diffusion based 3D generation model UDiFF. Furthermore, 3D-WAG supports unconditional, class-conditional, and text-conditional shape generation. Experimental results on standard 3D benchmarks, including ShapeNet and DeepFashion3D, show that 3D-WAG outperforms state-of-the-art methods on metrics such as Minimum Matching Distance (MMD) and Coverage (COV), generating high-quality 3D shapes that accurately represent real-world data distributions.",5,4,~Santosh_Kumar4,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
1071,https://openreview.net/forum?id=6gnfkiZHBd,Stabilizing Open-Set Test-Time Adaptation via Primary-Auxiliary Filtering and Knowledge-Integrated Prediction,"Deep neural networks demonstrate strong performance under aligned training-test distributions. However, real-world test data often exhibit domain shifts. Test-Time Adap- tation (TTA) addresses this challenge by adapting the model to test data during inference. While most TTA studies assume that the training and test data share the same class set (closed-set TTA), real-world scenarios frequently include open-set data (open-set TTA), which can degrade closed-set accuracy. A recent study has shown that identifying open- set data during adaptation and maximizing its entropy is an effective solution. However, the previous method relies on the source model for filtering, resulting in low filtering ac- curacy on domain-shifted test data. In contrast, we found that the adapting model, which learns domain knowledge from noisy test streams, tends to be unstable and leads to error accumulation when used for filtering. To address this problem, we propose Primary- Auxiliary Filtering (PAF), which employs an auxiliary filter to validate data filtered by the primary filter. Furthermore, we propose Knowledge-Integrated Prediction (KIP), which calibrates the outputs of the adapting model, EMA model, and source model to integrate their complementary knowledge for OSTTA. We validate our approach across diverse closed-set and open-set datasets. Our method enhances both closed-set accuracy and open-set discrimination over existing methods.",7,4,~Tae-Min_Choi1|~Amirreza_Shaban1|~Heechul_Jung3,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
1072,https://openreview.net/forum?id=NnTj32d2yX,Controllable Garment Generation with Multi-Modal Diffusion Guidance,"We address the task of controllable garment image generation, where the goal is to synthesize realistic clothing images from incomplete and interpretable inputs such as sketches, color cues, text prompts, and fabric textures. This task poses unique challenges in aligning high-level semantics with spatial structure while preserving visual fidelity. We propose \textbf{GenWear}, a unified conditional diffusion framework built atop a frozen Paint-by-Example (PBE) denoising UNet, augmented with learnable modules for modality-specific control. We introduce SketchCtrl, a ControlNet based module which injects multi-scale spatial features from sketches via zero-convolution, ensuring structural fidelity. ColorFiLM employs feature-wise linear modulation to steer global color tone without disrupting pretrained activations. A BLIP-2-based text adapter biases the sketch encoder with semantic priors to resolve ambiguities, while a cross-attentive texture adapter injects local fabric cues into the decoder for material realism. These modules operate synergistically, enabling disentangled control without modifying the core diffusion backbone. Unlike prior works, GenWear does not require a full reference image at test time, treating the sketch as both spatial and semantic guide. Experiments on VITON-HD dataset demonstrate that our approach achieves state-of-the-art quality and controllability across diverse garment types and modalities.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
1076,https://openreview.net/forum?id=XzTwopvIKf,CLFSeg: A Fuzzy-Logic based Solution for Boundary Clarity and Uncertainty Reduction in Medical Image Segmentation,"Accurate polyp and cardiac segmentation for early detection and treatment is essential for the diagnosis and treatment planning of cancer-like diseases. Traditional convolutional neural network (CNN) based models have represented limited generalizability, robustness, and inability to handle uncertainty, which affects the segmentation performance. To solve these problems, this paper introduces CLFSeg, an encoder-decoder based framework that aggregates the Fuzzy-Convolutional (FC) module leveraging convolutional layers and fuzzy logic. This module enhances the segmentation performance by identifying local and global features while minimizing the uncertainty, noise, and ambiguity in boundary regions, ensuring computing efficiency. In order to handle class imbalance problem while focusing on the areas of interest with tiny and boundary regions, binary cross-entropy (BCE) with dice loss is incorporated. Our proposed model exhibits exceptional performance on four publicly available datasets, including CVC-ColonDB, CVC-ClinicDB, EtisLaribPolypDB, and ACDC. Extensive experiments and visual studies show CLFSeg surpasses the existing SOTA performance and focuses on relevant regions of interest in anatomical structures. The proposed CLFSeg improves performance while ensuring computing efficiency, which makes it a potential solution for real-world medical diagnostic scenarios.",5,3,~Guangyu_Li3|~Miles_Everett1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1077,https://openreview.net/forum?id=Cwh7yeZQ2L,Towards Sharper Object Boundaries in Self-Supervised Depth Estimation,"Accurate monocular depth estimation is crucial for 3D scene understanding, but existing methods often blur depth at object boundaries, introducing spurious intermediate 3D points. While achieving sharp edges usually requires very fine-grained supervision, our method produces crisp depth discontinuities using only self-supervision. Specifically, we model per-pixel depth as a mixture distribution, capturing multiple plausible depths and shifting uncertainty from direct regression to the mixture weights. This formulation integrates seamlessly into existing pipelines via variance-aware loss functions and uncertainty propagation. Extensive evaluations on KITTI and VKITTIv2 show that our method achieves up to 35\% higher boundary sharpness and improves point cloud quality compared to state-of-the-art baselines.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
1081,https://openreview.net/forum?id=fdHhuQjzrk,ImProvShow: Multimodal Fusion for Image Provenance Summarization,"We present ImProvShow; a novel approach to summarizing the multi-stage edit history (or `provenance') of an image.   ImProvShow fuses visual and textual cues to succinctly summarize multiple manipulations applied to an image in a sequence; a novel extension of the classical image difference captioning  (IDC) problem. ImProvShow takes as input several intermediate thumbnails of the image editing sequence, as well as any coarse human or machine-generated annotations of the individual manipulations at each stage, if available. We demonstrate that the presence of intermediate images and/or auxiliary textual information improves the model's edit captioning  performance. To train ImProvShow, we introduce METS (Multiple Edits and Textual Summaries) - a new open dataset of image editing sequences, with textual machine annotations of each editorial step and human edit summarization captions after the 5th, 10th and 15th manipulation.",6,4,~Vivek_Menon1|~Fabio_Pizzati1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1091,https://openreview.net/forum?id=3DwmS7iABD,CLIMB-3D: Class-Incremental Imbalanced 3D Instance Segmentation,"While 3D instance segmentation (3DIS) has advanced significantly, existing methods typically assume that all object classes are known in advance and are uniformly distributed. However, this assumption is unrealistic in dynamic, real-world environments where new classes emerge gradually and exhibit natural imbalance. Although some approaches have addressed class emergence, they often overlook class imbalance, resulting in suboptimal performance -- particularly on rare categories. To tackle this challenge, we propose CLIMB-3D, a unified framework for CLass-incremental Imbalance-aware 3DIS. Building upon established exemplar replay (ER) strategies, we show that ER alone is insufficient to achieve robust performance under constrained memory conditions. To mitigate this, we introduce a novel pseudo-label generator (PLG) that extends supervision to previously learned categories by leveraging predictions from a frozen prior model. Despite its promise, PLG tends to bias towards frequent classes. Therefore, we propose a class-balanced re-weighting (CBR) scheme, that estimates object frequencies from pseudo-labels and dynamically adjusts training bias -- without requiring access to past data. We design and evaluate three incremental scenarios for 3DIS on the challenging ScanNet200 dataset, and additionally on semantic segmentation on ScanNetV2. Our approach achieves state-of-the-art results, surpassing prior work by up to 16.76\% mAP for instance segmentation and approximately 30\% mIoU for semantic segmentation, demonstrating strong generalization across both frequent and rare classes.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
1095,https://openreview.net/forum?id=bTBusBZFag,Supervised Segmentation Model for Improved Detection of OSSN using Slit Lamp Images,"Ocular surface squamous neoplasia (OSSN) is a rare eye disease that affects the conjunctiva and the cornea and can cause blindness if not diagnosed early. At an early stage, the diagnosis of OSSN is often confused with other ocular surface diseases (OOSD) such as pterygium, pinguecula, episcleritis, papilloma, etc. Recently, AI algorithms have been explored for OSSN classification using OOSD and healthy patients as a control population. However, these algorithms either use traditional or deep models pre-trained on natural images or mild fine-tuning from some limited examples with image-level annotations, and finally perform poorly, and the results are unreliable. In this paper, we present a self-explainable deep neural network-based segmentation architecture to predict the pixel-level probability of OSSN in slit lamp images. The proposed method is more accurate and reliable than the existing methods as it eliminates the noisy variations in the images and focuses on the causal parts. The experiments in the collected data set for 163 OSSN, 200 OOSD, and 269 healthy people show the efficacy of the proposed methods in screening OSSN.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jefersson A Dos Santos
1096,https://openreview.net/forum?id=sjM2USIVLg,LED: Light Enhanced Depth Estimation at Night,"Nighttime camera-based depth estimation is a highly challenging task, especially for autonomous driving applications, where accurate depth perception is essential for ensuring safe navigation.
Models trained on daytime data often fail in the absence of precise but costly LiDAR. Even vision foundation models trained on large amounts of data are unreliable in low-light conditions. 
In this work, we aim to improve the reliability of perception systems at night time.
To this end, we introduce Light Enhanced Depth (LED), a novel, cost-effective approach that significantly improves depth estimation in low-light environments by harnessing a pattern projected by high definition headlights available in modern vehicles. 
LED leads to significant performance boosts across multiple depth-estimation architectures (encoder-decoder, Adabins, DepthFormer, Depth Anything V2) both on synthetic and real datasets.
Furthermore, increased performances beyond illuminated areas reveal a holistic enhancement in scene understanding.
Finally, we release the Nighttime Synthetic Drive Dataset, a synthetic and photo-realistic nighttime dataset, which comprises 49,990 comprehensively annotated images. 
To facilitate further research, both synthetic dataset and code will be made publicly available at https://anonymous.4open.science/r/LED-135D.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Zhixiang Chen
1109,https://openreview.net/forum?id=D5GtynokuS,AKD-BNN: Adaptive Kernel Dynamic Bayesian Neural Networks for Enhanced Medical Image Segmentation with Uncertainty Estimation,"Medical image segmentation faces significant challenges due to spatial heterogeneity and complex feature variations. Traditional Bayesian Neural Networks (BNNs) combined with Gaussian Processes (GPs) rely on fixed kernel functions, limiting their adaptability to diverse image regions. This paper proposes an Adaptive Kernel Dynamic BNN (AKD-BNN) framework, which enhances segmentation performance and uncertainty estimation through a dynamic kernel learning module and multi-scale feature fusion. The approach includes a dynamic kernel module that adjusts GP kernel parameters based on local image semantics, Transformer-based multi-scale feature extraction, and an uncertainty-guided loss function. Experiments on multiple medical image datasets, including brain tumor (BraTS), cardiac MRI (ACDC), and lung CT (LIDC-IDRI), demonstrate that AKD-BNN outperforms state-of-the-art methods such as BNN-GP and DeepLabV3+. Specifically, AKDBNN improves the Dice score from 0.90 to 0.95 on BraTS and reduces the Expected Calibration Error (ECE) by approximately 20%. The results highlight its superior segmentation consistency in complex regions, such as tumor boundaries, providing more reliable uncertainty maps for clinical applications. This study offers a robust and interpretable solution for medical image analysis, with potential extensions to other high-dimensional tasks.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1110,https://openreview.net/forum?id=ORF8ejKjoM,Robust Human Registration with Body Part Segmentation on Noisy Point Clouds,"Registering human meshes to 3D point clouds is essential for applications such as augmented reality and human-robot interaction but often yields imprecise results due to noise and background clutter in real-world data. We introduce a hybrid approach that incorporates body-part segmentation into the mesh fitting process, enhancing both human pose estimation and segmentation accuracy. Our method first assigns body part labels to individual points, which then guide a two-step SMPL-X mesh fitting: initial pose and orientation estimation using body part centroids, followed by global refinement of the point cloud alignment. Additionally, we demonstrate that the fitted human mesh can refine body part labels, leading to improved segmentation. Evaluations on the cluttered and noisy real-world datasets InterCap, EgoBody, and BEHAVE show that our approach significantly outperforms prior methods in both pose estimation and segmentation accuracy. The code will be made open source upon publication.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
1118,https://openreview.net/forum?id=CT9OIe9H1e,Conformal Predictors for Efficient Video Text Spotting,"Video text spotting can be achieved by first detecting and recognizing text instances at the frame level, then tracking these texts across video frames. A major challenge with this approach is the inconsistency of text recognition across frames, where an image text spotter may misrecognize text in some frames while correctly recognizing it in others. We argue that text recognition accuracy is correlated with the model's prediction uncertainty. To improve prediction accuracy in videos, we use a statistically rigorous approach to estimate the uncertainty of predicted texts and leverage tracking information to produce more confident and accurate results. To this end, we employ a pretrained state-of-the-art text spotter and apply the conformal prediction framework to estimate its prediction uncertainties. A second challenge addressed in this work is the inconsistent quality of predicted bounding boxes, which impacts text tracking performance. As a solution, we use a second conformal prediction approach that applies corrections to the predicted bounding boxes, providing guarantees with a predefined probability of success. Extensive experiments on four public datasets demonstrate consistent improvements over state-of-the-art methods, achieving IDF1 gains of 1.6% on DSText, 1.0% on ArTVideo, and 2.3% on BOVText. While being computationally lightweight, the proposed method integrates seamlessly into any pretrained matching-based video text spotter, enhancing overall performance. The code will be released with the camera-ready version.",5,3,~Maurice_Fallon1|~Nupur_Thakur1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
1121,https://openreview.net/forum?id=vD8G57Mggw,MIAS-SAM: Medical Image Anomaly Segmentation without thresholding,"This paper presents MIAS-SAM, a novel approach for the segmentation of anomalous regions in medical images. MIAS-SAM uses a patch-based memory bank to store relevant image features, which are extracted from normal data using the SAM encoder. At inference time, the embedding patches extracted from the SAM encoder are compared with those in the memory bank to obtain the anomaly map. Finally, MIAS-SAM computes the center of gravity of the anomaly map to prompt the SAM decoder, obtaining an accurate segmentation from the previously extracted features. Differently from prior works, MIAS-SAM does not require to define a threshold value to obtain the segmentation from the anomaly map. Experimental results conducted on three publicly available datasets, each with a different imaging modality (Brain MRI, Liver CT, and Retina OCT) show accurate anomaly segmentation capabilities measured using DICE score.

The code is available at: https://anonymous.4open.science/r/MIAS-SAM-1078",5,3,~Segun_Adebayo1|~Hassen_Drira1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
1123,https://openreview.net/forum?id=NImbVMoy3G,"Evaluating Self-Supervised Learning in Medical Imaging:  A Systematic Investigation of Robustness, Generalizability, and Multi-Domain Impact","Self-supervised learning (SSL) has emerged as a promising paradigm in medical imaging, addressing the chronic challenge of limited labeled data in healthcare settings. While SSL has shown impressive results, existing studies in the medical domain are often limited in scope, focusing on specific datasets or modalities, or evaluating only isolated aspects of model performance. This fragmented evaluation approach poses a significant challenge, as models deployed in critical medical settings must not only achieve high accuracy but also demonstrate robust performance and generalizability across diverse datasets and varying conditions. 
To bridge this gap, we conduct a rigorous investigation into the design space of SSL for medical imaging, evaluating 8 major SSL methods across 11 real-world medical datasets. Our analysis spans three core dimensions: (1) in-domain performance under varying label proportions (1\%, 10\%, and 100\%), (2) cross-dataset generalization, and (3) robustness to out-of-distribution (OOD) samples. Beyond empirical evaluation, we further examine how initialization strategies, model architectures, and multi-domain pre-training contribute to SSL’s success in medical imaging.",6,4,~Armaghan_Moemeni1|~Yan_Hu1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
1127,https://openreview.net/forum?id=5T6YPmBblh,ALFred: An Active Learning Framework for Real-world Semi-supervised Anomaly Detection with Adaptive Thresholds,"Video Anomaly Detection (VAD) can play a key role in spotting unusual activities in video footage. VAD is difficult to use in real-world settings due to the dynamic nature of human actions, environmental variations, and domain shifts. Traditional evaluation metrics often prove inadequate for such scenarios, as they rely on static assumptions and fall short of identifying a threshold that distinguishes normal from anomalous behavior in dynamic settings. To address this, we introduce an active learning framework tailored for VAD, designed for adapting to the ever-changing real-world conditions. Our approach leverages active learning to continuously select the most informative data points for labeling, thereby enhancing model adaptability. A critical innovation is the incorporation of a human-in-the-loop mechanism, which enables the identification of actual normal and anomalous instances from pseudo-labeling results generated by AI. This collected data allows the framework to define an adaptive threshold tailored to different environments, ensuring the system remains effective as the definition of 'normal' shifts across various settings. Implemented within a lab-based framework that simulates real-world conditions, our approach allows for rigorous testing and refinement of VAD algorithms with a new metric. Experimental results show that our method achieves an EBI (Error Balance Index) of 27.16 for Q1 in real-world simulated scenarios, demonstrating its practical effectiveness and significantly enhancing the applicability of VAD in dynamic environments.",5,3,~Abdulhamid_Adebayo1|~Bahareh_Nikpour1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Edmond S. L. Ho
1132,https://openreview.net/forum?id=mSoXObeV0G,Isolated Channel Vision Transformers: From Single-Channel Pretraining to Multi-Channel Finetuning,"Vision Transformers (ViTs) have achieved remarkable success in standard RGB image analysis. However, applying ViTs to multi-channel imaging (MCI) data, e.g., for medical and remote sensing applications, remains a challenge. In particular, MCI data often consist of layers acquired from different modalities. Directly training ViTs on such data can obscure modality/channel-specific information and impair the performance. 
In this paper, we propose Isolated Channel ViT (IC-ViT), a simple yet effective training framework for large-scale MCI data. By randomly sampling one channel per image per iteration, IC-ViT learns channel-specific representations without requiring multi-channel fusion during pretraining. These representations are later integrated during finetuning to capture cross-channel dependencies in downstream tasks.
Experiments on various benchmarks, including JUMP-CP and CHAMMI for cell microscopy imaging, and So2Sat-LCZ42 for satellite imaging, show the proposed IC-ViT outperforms existing channel-adaptive approaches by 4–14%. Moreover, its efficient training makes it a suitable candidate for large-scale pretraining of foundation models on heterogeneous data.",5,4,~Haibin_Cai1,N/A,N/A,N/A,N/A,N/A,N/A,2,1,Accept (Oral),Accept,Jefersson A Dos Santos
1134,https://openreview.net/forum?id=cCRilcUXpy,TRUDI and TITUS: A Multi-Perspective Dataset and A Three-Stage Recognition System for Transportation Unit Identification,"Identifying transportation units (TUs) is essential for improving the efficiency of port logistics. However, progress in this field has been hindered by the lack of publicly available benchmark datasets that capture the diversity and dynamics of real-world port environments. To address this gap, we present the TRUDI dataset—a comprehensive collection comprising 35,034 annotated instances across five categories: container, tank container, trailer, ID text, and logo. The images were captured at operational ports using both ground-based and aerial cameras, under a wide variety of lighting and weather conditions. For the identification of TUs—which involves reading the 11-digit alphanumeric ID typically painted on each unit—we introduce TITUS, a dedicated pipeline that operates in three stages: (1) segmenting the TU instances, (2) detecting the location of the ID text, and (3) recognising and validating the extracted ID. Unlike alternative systems, which often require similar scenes, specific camera angles or gate setups, our evaluation demonstrates that TITUS reliably identifies TUs from a range of camera perspectives and in varying lighting and weather conditions. By making the TRUDI dataset publicly available, we provide a robust benchmark that enables the development and comparison of new approaches. This contribution supports digital transformation efforts in multipurpose ports and helps to increase the efficiency of entire logistics chains.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
1135,https://openreview.net/forum?id=JnVxyziEDB,The Trauma THOMPSON Dataset for Real-World Emergency AI,"We present the Trauma THOMPSON dataset and benchmarks designed to advance artificial intelligence research for real-time decision support in emergency and austere medical environments. The dataset contains 220 unscripted egocentric videos of five emergency procedures, including a diverse collection of ""just-in-time"" (JIT) life-saving interventions performed under resource-constrained conditions. These JIT scenarios more closely reflect the realities of humanitarian and field-based operational medicine, where standard protocols must often be adapted or creatively executed. To support deeper visual understanding, we introduce two new layers of fine-grained annotations: object detection labels for critical medical instruments and supplies and hand annotations to facilitate hand tracking and surgical skill assessment. These additions enable new research directions in spatiotemporal reasoning, interaction modeling, and AI copilots that interpret and guide complex procedures in real time. The Trauma THOMPSON dataset includes benchmark tasks in action recognition, action anticipation, visual question answering (VQA), object detection, and hand localization. We evaluate state-of-the-art models across these tasks, identifying current strengths and open challenges in developing robust AI for field-deployable decision-making. The dataset is available at \url{https://anonymous.4open.science/r/dataset-58F3}, and it can serve as a foundation for building intelligent systems that assist frontline caregivers.",5,5,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Oral),Accept,Charith Abhayaratne
1137,https://openreview.net/forum?id=gKW3TIBB8o,ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates,"Pretrained vision-language models (VLMs) like CLIP show strong zero-shot performance but struggle with generalization under distribution shifts. Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test data in new domains. While some TTA methods rely on prompt-tuning, training-free cache-based approaches are preferred for efficiency. However, current cache-based TTA models store only a limited set of high-confidence samples, restricting the decision boundary to these samples and ignoring the influence of other incoming test data. To address this, we propose Efficient Test-Time Adaptation (ETTA), introducing a Recursive Updating module that integrates all incoming test samples, progressively refining the decision boundary. This strategy mimics an unbounded cache, dynamically updating contextual embeddings for improved accuracy with minimal memory and computational overhead. ETTA also includes an Adaptive Ensemble module to reduce prompt dependency in image-to-text scores by dynamically selecting optimal prompts for each class. Furthermore, ETTA adaptively combines scores from both modules based on confidence levels, leveraging their complementary strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses the state-of-the-art TTA models in computational complexity and accuracy, setting a new standard for effective, efficient test-time adaptation.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
1139,https://openreview.net/forum?id=gEp1w99ewn,RETRO: REthinking Tactile Representation Learning with Material PriOrs,"Tactile perception is profoundly influenced by the surface properties of objects in contact—properties such as texture, hardness, and gloss. However, despite their crucial role in shaping tactile experiences, these material characteristics have been largely neglected in existing tactile representation learning methods. Most approaches primarily focus on aligning tactile data with visual or textual information, overlooking the richness of tactile feedback that comes from understanding the materials' inherent properties. In this work, we address this gap by revisiting the tactile representation learning framework and incorporating material-aware priors into the learning process. These priors, which represent pre-learned characteristics specific to different materials, allow tactile models to better capture and generalize the nuances of surface texture. Our method enables more accurate, contextually rich tactile feedback across diverse materials and textures, improving performance in real-world applications such as robotics, haptic feedback systems, and material editing. Codes will be made publicly available to facilitate further research in this area.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,2,1,Accept (Poster),Accept,Zhixiang Chen
1156,https://openreview.net/forum?id=0b2xmT59Hi,Hierarchical Image-Guided 3D Point Cloud Segmentation in Industrial Scenes via Multi-View Bayesian Fusion,"Reliable 3D segmentation is critical for understanding complex scenes with dense layouts and multi-scale objects, as commonly seen in industrial environments. In such scenarios, heavy occlusion weakens geometric boundaries between objects, and large differences in object scale will cause end-to-end models fail to capture both coarse and fine details accurately. Existing 3D point-based methods require costly annotations, while image-guided methods often suffer from semantic inconsistencies across views. To address these challenges, we propose a hierarchical image-guided 3D segmentation framework that progressively refines segmentation from instance-level to part-level.  Instance segmentation involves rendering a top-view image and projecting SAM-generated masks prompted by YOLO-World back onto the 3D point cloud. Part-level segmentation is subsequently performed by rendering multi-view images of each instance obtained from the previous stage and applying the same 2D segmentation and back-projection process at each view, followed by Bayesian updating fusion to ensure semantic consistency across views. Experiments on real-world factory data demonstrate that our method effectively handles occlusion and structural complexity, achieving consistently high per-class mIoU scores. Additional evaluations on public dataset confirm the generalization ability of our framework, highlighting its robustness, annotation efficiency, and adaptability to diverse 3D environments.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
1161,https://openreview.net/forum?id=51cPGHqJo9,Evaluating Perceptual Distance Models by Fitting Binomial Distributions to Two-Alternative Forced Choice Data,"The Two Alternative Forced Choice (2AFC) paradigm offers advantages over the Mean Opinion Score (MOS) paradigm in psychophysics (PF), such as simplicity and robustness. However, when evaluating perceptual distance models, MOS enables direct correlation between model predictions and PF data. In contrast, 2AFC only allows pairwise comparisons to be converted into a quality ranking similar to MOS when presentations include shared images. In large datasets, like BAPPS, where image patches and distortions are combined randomly, deriving rankings from 2AFC PF data becomes infeasible. To address this, instead of relying on MOS correlation, researchers have trained ad-hoc neural networks to reproduce 2AFC PF data based on pairs of model distances -— a black-box approach with conceptual and operational limitations. This paper introduces a more robust distance-model evaluation method using a pure probabilistic approach, applying maximum likelihood estimation to a binomial decision model. Our method demonstrates superior simplicity, interpretability, flexibility, and computational efficiency, as shown through evaluations of various visual distance models on two 2AFC PF datasets.",5,3,~Kiran_Raja1|~Muhammad_Abdullah_Adnan1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Charith Abhayaratne
1171,https://openreview.net/forum?id=3aBo6bcVeU,Conflict-Aware Adversarial Training,"Adversarial training is the most effective method to obtain adversarial robustness for deep neural networks by directly involving adversarial samples in the training procedure. To obtain an accurate and robust model, the weighted-average method is applied to optimize standard loss and adversarial loss simultaneously. In this paper, we argue that the weighted-average method does not provide the best tradeoff for standard performance and adversarial robustness. We argue that the failure of the weighted-average method is due to the conflict between gradients derived from standard and adversarial loss, and further demonstrate such a conflict increases with attack budget theoretically and practically. To alleviate this problem, we propose a new trade-off paradigm for adversarial training with a conflict-aware factor for the convex combination of standard and adversarial loss, named Conflict-Aware Adversarial Training (CA-AT). Comprehensive experimental results show that CA-AT consistently offers a superior trade-off between standard performance and adversarial robustness under the settings of adversarial training from scratch and parameter-efficient fine-tuning (PEFT).",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
1177,https://openreview.net/forum?id=xW8NvUDx8C,Binarizing Severely Degraded Ancient Bamboo Slips: Dataset and Baseline,"Bamboo and wooden slips were the primary  writing materials in China for more than 800 years, carrying valuable historical records. However, due to the lengthy corrosion and/or   weathering effects over a period of two millennia, texts on the ancient bamboo (and wooden) slips typically suffer from severe degradation problems, such as ink deterioration and text blur, which render the binarization of  severely degraded bamboo slip manuscripts a very challenging task. Due to the scarcity of benchmark dataset in this direction, in this work we aim to build HanBamboo, a specialized bamboo slips dataset with pixel-level annotations for research on binarization of severely degraded ancient manuscripts. HanBamboo comprises 1,000 infrared bamboo slip images displaying varying levels of ink  degradation and text blur. Among them, bamboo slips exhibiting low-contrast ink traces characterized by significant fading and diminished visibility requires remarkably greater time in pixel-level annotation, indicating the inherent difficulty in binarizing these bamboo slips. As a minor contribution, we also propose a baseline approach MRA-Net, which is an Mamba-based Encoder-Decoder framework for degraded bamboo slips binarization that devises additional multi-scale discrete wavelet transform and adaptive reweighting attention  mechanisms to capture and enhance the stroke details of the texts and suppress background noise during binarization. Comprehensive experiments on both HanBamboo and public document binarization benchmark datasets DIBCO 2017 and 2018 demonstrate the effectiveness of our baseline. Our dataset and code are available at: https://anonymous.4open.science/r/MRA-Net-7911.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
1180,https://openreview.net/forum?id=wMUAXsOJhk,MO-SHW: Hierarchy-Aware Multi-Objective Optimization for Open-World Segmentation,"The exploitation of hierarchical information by vision models has shown significant benefits in various segmentation tasks.
However, this remains unexplored in open-world scenarios, where models must cope with unknown, evolving, and underrepresented labeled class spaces. Most existing hierarchy-aware segmentation approaches are not readily applicable to open-world settings. This is primarily because they rely on architectural modifications that are incompatible with the design constraints of open-world models. Moreover, hierarchy-aware losses are challenging to integrate into such pipelines, as they often conflict with task-specific objectives and exacerbate optimization complexity in already multi-objective training environments.
In this work, we demonstrate that hierarchy-aware losses can be effectively leveraged in open-world models when optimized under a multi-objective learning framework. Specifically, we show that gradient-based multi-objective optimization methods, such as multi-objective gradient descent (MOGD), are well-suited for jointly optimizing hierarchical and task-specific objectives, leading to better overall performance. To support this, we propose SHW, a novel hierarchy-aware loss function based on the Wasserstein distance. SHW is lightweight, model-agnostic, and encourages intra-class compactness and inter-class separation across multiple semantic levels. The integration of SHW with MOGD yields a general, model-agnostic framework that enables the effective exploitation of semantic hierarchies in open-world segmentation tasks, improving the performance of several recent methods.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
1183,https://openreview.net/forum?id=MuRzu4GtLY,Contrastive Point Feature Matching for Open-world Object Counting,"Open-world object counting refers to estimating the number of target objects specified by a text description. Current state-of-the-art density-based methods rely on density map regression trained with counting-supervised losses, which often overlook the spatial and visual correspondence between image regions and object instances. Furthermore, the absence of spatial supervision leads to inconsistent performance in challenging scenarios such as crowding and occlusion. We propose CPMNet (Contrastive Point Matching Network), a novel framework that introduces spatial supervision through contrastive learning with point-level guidance. CPMNet regress object point locations from the predicted density map through Sinkhorn EM (expectation maximization), extracts their corresponding features, and applies contrastive learning to enhance discrimination between target objects and irrelevant regions. Experimental results on a benchmark dataset demonstrate that our method offers enhanced performance in open-world counting, providing a more precise and reliable solution for estimating object counts.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jun Liu
1187,https://openreview.net/forum?id=s7GicfmfMh,Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers,"Out-of-Distribution (OOD) detection is critical to AI reliability and safety, yet in many practical settings, only a limited amount of training data is available. Bayesian Neural Networks (BNNs) are a promising class of model on which to base OOD detection, because they explicitly represent epistemic (i.e. model) uncertainty. In the small training data regime, BNNs are especially valuable because they can incorporate prior model information. We introduce a new family of Bayesian posthoc OOD scores based on expected logit vectors, and compare 5 Bayesian and 4 deterministic posthoc OOD scores.   Experiments on MNIST and CIFAR-10 In-Distributions, with 5000 training samples or less, show that the Bayesian methods outperform corresponding deterministic methods.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Jun Liu
1196,https://openreview.net/forum?id=d0VYFQ3OHn,IoSR: End-to-End Intraoral Scans Repairing,"Despite being crucial, intraoral 3D scan repairing is often overlooked in research, as it remains a labor-intensive, behind-the-scenes task in digital dental workflows. Scanning limitations \textemdash stemming from scanning technology, intraoral anatomy, and device handling \textemdash can lead to gaps or artifacts, making high-precision reconstruction essential for clinical procedures as aligners, surgical guides, and crowns. Efficient scan repair can also reduce the back-and-forth between dentists and dental labs, or even the need to reschedule additional scanning sessions, thus streamlining the treatment process.
This paper presents the first comprehensive approach to address this challenge, introducing an end-to-end method for repairing intraoral 3D scans by leveraging both geometric and implicit representations. Our model employs a two-block architecture: a coarse generator with a geometric-aware transformer encoder and multilayer perceptron, followed a fine generator allows generating dense points and refining the generated point cloud based on implicit representation.
Experimental results show that our method outperforms state-of-the-art approaches offering enhanced accuracy and generating point clouds with improved fidelity. The code and dataset will be publicly available.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Edmond S. L. Ho
1199,https://openreview.net/forum?id=w3JPMTOOGa,AULUNet: An Adaptive Ultra-Lightweight U-Net Framework for Efficient Skin Lesion Segmentation in Resource-Constrained Environments,"High accuracy in skin lesion segmentation is crucial for mobile and embedded medical applications, where most existing models are too computationally demanding. We introduce AULUNet, an ultra-lightweight U-Net framework that addresses this limitation through three key innovations: Adaptive Kernel Fusion (AKF) for balanced extraction of local and global features, Residual Global Fusion (RGF) for refining bottleneck representations with global context, and Lightweight Skip Gate (LSG) for selective skip connection enhancement. On ISIC2017, AULUNet achieves 85.32 % mIoU and 92.08 % DSC, surpassing standard U-Net by 5.77 % and 3.47 %, respectively. On ISIC2018, it achieves 82.44 % mIoU and 90.37 % DSC, delivering gains of 7.80 % and 4.89 % over the baseline. Notably, our model requires only 0.029 M parameters and 0.069 GFLOPs, reducing parameter count by 99.63 % (from 7.773 M to 0.029 M) and compute by 99.50 % (from 13.758 GFLOPs to 0.069 GFLOPs), while maintaining consistent performance across diverse lesion structures. We validate the robustness of our design through extensive experiments and ablation studies. Our code is publicly available at https://github.com/defabc987412365zubos100/AULUNet.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Fani Deligianni
1209,https://openreview.net/forum?id=lTcu1U6Ikb,Language-Guided Reinforcement Learning for Hard Attention in Few-Shot Learning,"Attention mechanisms have demonstrated significant potential in enhancing learning models by identifying key portions of input data, particularly in scenarios with limited training samples. Inspired by human perception, we propose that focusing on essential data segments, rather than the entire dataset, can improve the accuracy and reliability of the learning models. However, identifying these critical data segments, or ""hard attention finding,"" is challenging, especially in few-shot learning, due to the scarcity of training data and the complexity of model parameters. To address this, we introduce LaHA, a novel framework that leverages language-guided deep reinforcement learning to identify and utilize informative data regions, thereby improving both interpretability and performance. Extensive experiments on benchmark datasets validate the effectiveness of LaHA.",6,3,~Yang_He_Xu1|~Shuo_Lei1|~Konstantinos_Bacharidis1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Jefersson A Dos Santos
1220,https://openreview.net/forum?id=SZiuOaNGMk,Hair Strand Reconstruction based on 3D Gaussian Splatting,"Human hair reconstruction is a challenging problem in computer vision, with growing importance for applications in virtual reality and digital human modeling. Recent advances in 3D Gaussians Splatting (3DGS) provide efficient and explicit scene representations that naturally align with the structure of hair strands. In this work, we extend the 3DGS framework to enable strand-level hair geometry reconstruction from multi-view images. Our multi-stage pipeline first reconstructs detailed hair geometry using a differentiable Gaussian rasterizer, then merges individual Gaussian segments into coherent strands through a novel merging scheme, and finally refines and grows the strands under photometric supervision.

While existing methods typically evaluate reconstruction quality at the geometric level, they often neglect the connectivity and topology of hair strands. To address this, we propose a new evaluation metric that serves as a proxy for assessing topological accuracy in strand reconstruction. Extensive experiments on both synthetic and real-world datasets demonstrate that our method robustly handles a wide range of hairstyles and achieves efficient reconstruction, typically completing within one hour.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Charith Abhayaratne
1224,https://openreview.net/forum?id=A2Kylh60ai,RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans,"Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important challenge useful to obtain quick and accurate detection of brain anomalies, when precise pixel level anomaly annotations are unavailable and only weak labels (e.g. slice-level) are available. In this work, we propose RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings, a novel two-stage WSAD framework. In the first stage, we introduce a Discriminative Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak masks based on slice-level labels, serving as coarse localization cues. In the second stage, we propose a segmentation network with a region aware spatial attention mechanism that relies on fixed location-based random embeddings. This design enables the model to effectively focus on anomalous regions. Our approach achieves state-of-the-art anomaly detection  performance, significantly outperforming existing WSAD methods while utilizing less than 8 million parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23 and MSD datasets demonstrate a substantial performance improvement coupled with a significant reduction in computational complexity.",7,3,~Ivan_Oseledets1|~Dongheng_Lin1|~Rainer_Stiefelhagen1|~Yihao_Ding1,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Weak Accept (Poster),Accept,Fani Deligianni
1257,https://openreview.net/forum?id=uAn5obR5EG,End-to-End LiDAR optimization for point cloud registration,"LiDAR sensors are a key modality for 3D perception, yet they are typically designed
independently of downstream tasks such as point cloud registration. Conventional registration operates on pre-acquired datasets with fixed LiDAR configurations, leading to suboptimal data collection and significant computational overhead for sampling, noise
filtering, and parameter tuning. In this work, we propose an adaptive LiDAR sensing
framework that dynamically adjusts sensor parameters, jointly optimizing LiDAR acquisition and registration hyperparameters. By integrating registration feedback into the sensing loop, our approach optimally balances point density, noise, and sparsity, improving registration accuracy and efficiency. Evaluations in the CARLA simulation demonstrate that our method outperforms fixed-parameter baselines while retaining generalization abilities, highlighting the potential of adaptive LiDAR for autonomous perception
and robotic applications.",3,3,,N/A,N/A,N/A,N/A,N/A,N/A,1,1,Accept (Poster),Accept,Zhixiang Chen
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,
,,,,,,,,,,,,,,,,,