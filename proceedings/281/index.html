<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="800" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center"><style>body { background-color: white !important; color: black !important; }</style>CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections</h2><br><h5 style="font-weight:normal; font-size: 1.25em;" align="center"><autocolor><h5 style="font-weight:normal; color: black;" align="center">Mohamed Fazli Mohamed Imam (), Rufael Fekadu Marew (Mohamed bin Zayed University of Artificial Intelligence), Jameel Hassan Abdul Samadh (Johns Hopkins University), Mustansar Fiaz (International Business Machines), Alham Fikri Aji (Mohamed bin Zayed University of Artificial Intelligence), Hisham Cholakkal (MBZUAI)</h5></autocolor></h5><h5 style="font-weight:normal; color: black;" align="center"><a href="https://bmvc2025.bmva.org" target="_blank" style="color: black;"><i>The 35<sup>th</sup> British Machine Vision Conference</i></a></h5><div class="cta"><a href="https://bmva-archive.org.uk/bmvc/2025/papers/Paper_281/paper.pdf" role="button">PDF</a><a href="https://bmva-archive.org.uk/bmvc/2025/papers/Paper_281/281_supplementary.zip" role="button">Supplementary</a><br></div><h2 id="abstract">Abstract</h2>In the era of foundation models, CLIP has emerged as a powerful tool for aligning text and visual modalities into a common embedding space. However, the alignment objective used to train CLIP often results in subpar visual features for fine-grained tasks. In contrast, models pretrained in a self-supervised manner, such as DINO, excel at extracting rich visual features due to their specialized training paradigm. Yet, these self-supervised learning (SSL) models require an additional supervised linear probing step, which relies on fully labeled data, often expensive and difficult to obtain at scale. In this paper, we propose a label-free prompt-tuning method that leverages the rich visual features extracted using DINO SSL model and the broad contextual knowledge of LLMs to enhance CLIP’s image classification performance using purely unlabeled images. Our approach unfolds in three key steps: (i) We generate robust textual feature embeddings that more accurately represent object classes by leveraging class-specific descriptions from large language models (LLMs). (ii) The textual embeddings are then used to produce pseudo-labels to train an alignment module that integrates the complementary strengths of LLM description-based textual embeddings and visual features extracted from DINO. (iii) Finally, we prompt-tune CLIP’s vision encoder using the trained alignment module. This three-step process allows us to harness the best of visual and textual foundation models, resulting in a powerful and efficient approach that surpasses state-of-the-art (SOTA) label-free classification methods. Notably, our framework, NoLA (No Labels Attached), achieves an average absolute gain of 3.6% over the state-of-the-art LaFTer across 11 diverse image classification datasets. Our code and models will be made publicly available.<br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Imam_2025_BMVC,
author    = {Mohamed Fazli Mohamed Imam and Rufael Fekadu Marew and Jameel Hassan Abdul Samadh and Mustansar Fiaz and Alham Fikri Aji and Hisham Cholakkal},
title     = {CLIP meets DINO for Tuning Zero-Shot Classifier using Unlabeled Image Collections},
booktitle = {36th British Machine Vision Conference 2025, {BMVC} 2025, Sheffield, UK, November 24-27, 2025},
publisher = {BMVA},
year      = {2025},
url       = {https://bmva-archive.org.uk/bmvc/2025/papers/Paper_281/paper.pdf}
}
</code></pre></div></div><br><br><p><small style="color: black;">Copyright &copy 2025 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><black>Imprint<black></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><black>Data Protection</autocolor></a></small></p></section></div></body></html>