<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>Intra-Modal Divergence-Weighted Distillation for Vision-Language Models</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="800" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center"><style>body { background-color: white !important; color: black !important; }</style>Intra-Modal Divergence-Weighted Distillation for Vision-Language Models</h2><br><h5 style="font-weight:normal; font-size: 1.25em;" align="center"><autocolor><h5 style="font-weight:normal; color: black;" align="center">Youva Addad (Université de Caen Basse Normandie), Alexis Lechervy (Université de Caen Basse Normandie), Frédéric Jurie (Université de Caen Normandie)</h5></autocolor></h5><h5 style="font-weight:normal; color: black;" align="center"><a href="https://bmvc2025.bmva.org" target="_blank" style="color: black;"><i>The 35<sup>th</sup> British Machine Vision Conference</i></a></h5><div class="cta"><a href="https://bmva-archive.org.uk/bmvc/2025/papers/Paper_458/paper.pdf" role="button">PDF</a><a href="https://bmva-archive.org.uk/bmvc/2025/papers/Paper_458/458_supplementary.zip" role="button">Supplementary</a><br></div><h2 id="abstract">Abstract</h2>Large vision-language models like CLIP offer strong zero-shot capabilities but are computationally demanding. Knowledge distillation is crucial for creating efficient student models; however, effectively transferring the teacher's nuanced understanding of within-modality relationships, especially among negative examples, remains challenging. We introduce a novel distillation method focused on capturing the teacher's intra-modal relational knowledge. Our approach employs Kullback-Leibler divergence to measure the disagreement between student and teacher pairwise similarity distributions within each modality. This disagreement score then dynamically weights the distillation loss, compelling the student to prioritize learning from samples exhibiting the most significant relational discrepancies. This strategy encourages closer alignment of the student's internal representation space with the teacher's. Experiments demonstrate our method produces performant and efficient student models by effectively transferring this vital relational information. The source code will be made publicly available.<br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Addad_2025_BMVC,
author    = {Youva Addad and Alexis Lechervy and Frédéric Jurie},
title     = {Intra-Modal Divergence-Weighted Distillation for Vision-Language Models},
booktitle = {36th British Machine Vision Conference 2025, {BMVC} 2025, Sheffield, UK, November 24-27, 2025},
publisher = {BMVA},
year      = {2025},
url       = {https://bmva-archive.org.uk/bmvc/2025/papers/Paper_458/paper.pdf}
}
</code></pre></div></div><br><br><p><small style="color: black;">Copyright &copy 2025 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><black>Imprint<black></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><black>Data Protection</autocolor></a></small></p></section></div></body></html>