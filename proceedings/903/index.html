<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>Toward Robust Audio-Visual Synchronization Detection in Egocentric Video with Sparse Synchronization Events</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="800" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center"><style>body { background-color: white !important; color: black !important; }</style>Toward Robust Audio-Visual Synchronization Detection in Egocentric Video with Sparse Synchronization Events</h2><br><h5 style="font-weight:normal; font-size: 1.25em;" align="center"><autocolor><h5 style="font-weight:normal; color: black;" align="center">Jordan Voas (University of Texas at Austin), Wei-Cheng Tseng (University of Texas at Austin), Benoit Vallade (Amazon), Alex Mackin (Amazon), David Higham (Amazon), David Harwath (University of Texas)</h5></autocolor></h5><h5 style="font-weight:normal; color: black;" align="center"><a href="https://bmvc2025.bmva.org" target="_blank" style="color: black;"><i>The 35<sup>th</sup> British Machine Vision Conference</i></a></h5><div class="cta"><a href="https://bmva-archive.org.uk/bmvc/2025/papers/Paper_903/paper.pdf" role="button">PDF</a><a href="https://bmva-archive.org.uk/bmvc/2025/papers/Paper_903/903_supplementary.zip" role="button">Supplementary</a><br></div><h2 id="abstract">Abstract</h2>Audio-Visual Synchronization Detection (AVS) is a core task in multimodal video quality analysis, yet most existing methods are developed and evaluated on domains with limited diversity of sparse events or predominantly dense, repetitive cues, such as talking heads or scripted broadcasts, restricting their generalization to many real-world scenarios. We present the first comprehensive study of AVS in the challenging domain of egocentric video, using the Ego4D dataset as a benchmark. Motivated by the growing use of head-mounted and body-worn cameras in live streaming, augmented reality, law enforcement, and sports, this domain presents unique challenges: sparse, heterogeneous synchronization events, unstable viewpoints, and minimal access to dense anchors like visible faces. Our findings reveal sharp performance drops in existing AVS models on egocentric content. In response, we introduce $\textit{AS-Synchformer}$, a novel streaming AVS model tailored for sparse, unconstrained video. $\textit{AS-Synchformer}$ incorporates three key innovations: (1) a history-aware streaming token selection strategy, (2) a contrastive alignment loss to enforce temporal correspondence for selected streaming tokens, and (3) an Earth Moverâ€™s Distance (EMD) loss to capture ordinal offset structure for the AVS task. These yield substantial gains, including a 3.55% boost in ACC@1 and a 22.3% EMD reduction over strong streaming baselines like APA Synchformer, and a 2.41% ACC@1 gain with a 21.6% EMD reduction over Synchformer in snapshot AVS, setting a new state of the art in both paradigms. Moreover, we investigate the individual impact of full encoder fine-tuning on our model through an ablation study. Our analysis highlights the critical role of encoder fine-tuning in achieving robust AVS under real-world egocentric conditions, representing the first large-scale AVS systems with end-to-end training released.<br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Voas_2025_BMVC,
author    = {Jordan Voas and Wei-Cheng Tseng and Benoit Vallade and Alex Mackin and David Higham and David Harwath},
title     = {Toward Robust Audio-Visual Synchronization Detection in Egocentric Video with Sparse Synchronization Events},
booktitle = {36th British Machine Vision Conference 2025, {BMVC} 2025, Sheffield, UK, November 24-27, 2025},
publisher = {BMVA},
year      = {2025},
url       = {https://bmva-archive.org.uk/bmvc/2025/papers/Paper_903/paper.pdf}
}
</code></pre></div></div><br><br><p><small style="color: black;">Copyright &copy 2025 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><black>Imprint<black></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><black>Data Protection</autocolor></a></small></p></section></div></body></html>