<!DOCTYPE html><html lang="en-US"><head><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" /><title>UMM: A Unified Multi-Modal Model for Low-Level Vision Tasks with Dual-Driven Prompting</title><link rel="stylesheet" href="../assets/css/style.css"><link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png"></head><body><div class="wrapper"><section><center><a href="../"><img src="../images/bmvc-logo.png" width="800" class="figure-img img-responsive center-block"></a><br /><br /></center><h2 class="project-name" style="font-weight:normal; font-size: 167%;" align="center"><style>body { background-color: white !important; color: black !important; }</style>UMM: A Unified Multi-Modal Model for Low-Level Vision Tasks with Dual-Driven Prompting</h2><br><h5 style="font-weight:normal; font-size: 1.25em;" align="center"><autocolor><h5 style="font-weight:normal; color: black;" align="center">Ziqi Luo ( South University of Science and Technology), Jinxiang Lai (The Hong Kong University of Science and Technology), Ruitao Chen (Southern University of Science and Technology), Jinyu Yang (tapall.ai), Bin-Bin Gao (Tencent), Qiang Nie (The Hong Kong University of Science and Technology), Jun Liu (Tencent YouTu Lab), Jinfan Wang (Southern University of Science and Technology), Feng Zheng (Southern University of Science and Technology)</h5></autocolor></h5><h5 style="font-weight:normal; color: black;" align="center"><a href="https://bmvc2025.bmva.org" target="_blank" style="color: black;"><i>The 35<sup>th</sup> British Machine Vision Conference</i></a></h5><div class="cta"><a href="https://bmva-archive.org.uk/bmvc/2025/papers/Paper_392/paper.pdf" role="button">PDF</a><br></div><h2 id="abstract">Abstract</h2>Current research lacks exploration of the unified multi-modal model  for low-level vision tasks. Although there are some RGB-only unified models, they usually require multiple independent decoders or additional parameters for different tasks, failing to exploit their potential connection and shared knowledge. In this paper, we propose a unified multi-modal model (UMM) that can cope with various low-level vision tasks with all parameters shared and one common decoder. The core of our model is the proposed innovative dual-driven prompting paradigm, which aims to employ multi-modal prompting to enhance the robustness of the model and utilize task prompts to guide the model to extract features related to the specific tasks. Furthermore, we propose a task-aware fusion module (TFM). It guides multi-modal fusion through task prompts, enabling the model to focus on key features of the specific tasks during the fusion process. The experimental results show that our unified model UMM achieves competitive performance on various multi-modal low-level vision tasks, including RGB-T glass detection, RGB-T low-light enhancement, RGB-D salient object detection and RGB-N drivable area detection.<br><br><h2>Citation</h2><div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{Luo_2025_BMVC,
author    = {Ziqi Luo and Jinxiang Lai and Ruitao Chen and Jinyu Yang and Bin-Bin Gao and Qiang Nie and Jun Liu and Jinfan Wang and Feng Zheng},
title     = {UMM: A Unified Multi-Modal Model for Low-Level Vision Tasks with Dual-Driven Prompting},
booktitle = {36th British Machine Vision Conference 2025, {BMVC} 2025, Sheffield, UK, November 24-27, 2025},
publisher = {BMVA},
year      = {2025},
url       = {https://bmva-archive.org.uk/bmvc/2025/papers/Paper_392/paper.pdf}
}
</code></pre></div></div><br><br><p><small style="color: black;">Copyright &copy 2025 <a href="https://britishmachinevisionassociation.github.io/" rel="noopener"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a><br>The British Machine Vision Conference is organised by <a href="https://britishmachinevisionassociation.github.io/"><black>The British Machine Vision Association and Society for Pattern Recognition</autocolor></a>. The Association is a Company limited by guarantee, No.2543446, and a non-profit-making body, registered in England and Wales as Charity No.1002307 (Registered Office: Dept. of Computer Science, Durham University, South Road, Durham, DH1 3LE, UK).</small></p><p><small><a href="https://imprint.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de" rel="noopener"><black>Imprint<black></a> | <a href="https://data-protection.mpi-klsb.mpg.de/inf/bmvc2022.mpi-inf.mpg.de?lang=en" rel="noopener"><black>Data Protection</autocolor></a></small></p></section></div></body></html>